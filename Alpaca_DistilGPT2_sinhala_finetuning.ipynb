{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6ed8cc207701483da90ff05d4c47043a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_44ef62cf4b604b0988013eb95c53db76",
              "IPY_MODEL_432510b142ea46bc935649b574f32e56",
              "IPY_MODEL_ef629d8ddffc4e1f83467edff222ba2c"
            ],
            "layout": "IPY_MODEL_5a55f06d40c3414e84844c27399bda86"
          }
        },
        "44ef62cf4b604b0988013eb95c53db76": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0576a06be4ff4e5ab0ece2547e2805ad",
            "placeholder": "​",
            "style": "IPY_MODEL_a5c9836e9d0d4fdd86302d6807c2ddfd",
            "value": "tokenizer_config.json: 100%"
          }
        },
        "432510b142ea46bc935649b574f32e56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f569a1927cb846e586b7c99495d07f25",
            "max": 25,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f6439c043fcf4ae4a6e3990f1ea25a43",
            "value": 25
          }
        },
        "ef629d8ddffc4e1f83467edff222ba2c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4fba246fce2e42e2b2c22570625ee4c7",
            "placeholder": "​",
            "style": "IPY_MODEL_c6a3324546a14e2cb3c2e028cc8595a3",
            "value": " 25.0/25.0 [00:00&lt;00:00, 654B/s]"
          }
        },
        "5a55f06d40c3414e84844c27399bda86": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0576a06be4ff4e5ab0ece2547e2805ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a5c9836e9d0d4fdd86302d6807c2ddfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f569a1927cb846e586b7c99495d07f25": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6439c043fcf4ae4a6e3990f1ea25a43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4fba246fce2e42e2b2c22570625ee4c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c6a3324546a14e2cb3c2e028cc8595a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b0f9a6ed59f94df2994d652316184d98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2514408fb73d4d2986b5ce8dd22ae882",
              "IPY_MODEL_7868f692e20c4263bb05b75512c399b5",
              "IPY_MODEL_6fb5ba22497c4d3ba08494394f13fd01"
            ],
            "layout": "IPY_MODEL_c5ff9a64f1b4437e84b2a38bf2e9ef38"
          }
        },
        "2514408fb73d4d2986b5ce8dd22ae882": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8e51acfba6ef4bfbb6844e06babd7a3b",
            "placeholder": "​",
            "style": "IPY_MODEL_3ebbb265499d41b8b0fcf9c50aa6df2f",
            "value": "sentencepiece.bpe.model: 100%"
          }
        },
        "7868f692e20c4263bb05b75512c399b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_df6f56a8003a4bceb025d8cc87331848",
            "max": 5069051,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_22bf885c93fb4fcd86e55ae7bef17293",
            "value": 5069051
          }
        },
        "6fb5ba22497c4d3ba08494394f13fd01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21b4d56125004eee88c1c82a618a86db",
            "placeholder": "​",
            "style": "IPY_MODEL_88280e4448174fd6930a7042704299e9",
            "value": " 5.07M/5.07M [00:00&lt;00:00, 11.3MB/s]"
          }
        },
        "c5ff9a64f1b4437e84b2a38bf2e9ef38": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8e51acfba6ef4bfbb6844e06babd7a3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ebbb265499d41b8b0fcf9c50aa6df2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "df6f56a8003a4bceb025d8cc87331848": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22bf885c93fb4fcd86e55ae7bef17293": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21b4d56125004eee88c1c82a618a86db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "88280e4448174fd6930a7042704299e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f7ac37ce86fb4e0e92b92fa538d455fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_46296a8ddcf647398c780acb119b459a",
              "IPY_MODEL_6e91d92f936848c0bc9b8c4a701bcd7a",
              "IPY_MODEL_7cb95c4381894417b51b52466dcbf788"
            ],
            "layout": "IPY_MODEL_bdc8ce1c0ade40b1a816b3c4d7065633"
          }
        },
        "46296a8ddcf647398c780acb119b459a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_829739012e34468bb12c121da9470c65",
            "placeholder": "​",
            "style": "IPY_MODEL_221e8a5dcaf54353ac3286d282c73691",
            "value": "tokenizer.json: 100%"
          }
        },
        "6e91d92f936848c0bc9b8c4a701bcd7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad9642f7705f404c8b422d04eda8bf78",
            "max": 9096718,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_074c21a49aa14549b461ec6dc6162713",
            "value": 9096718
          }
        },
        "7cb95c4381894417b51b52466dcbf788": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_357bf73655384b76a8e430de23fbca64",
            "placeholder": "​",
            "style": "IPY_MODEL_cf17b7779e1b4f64995eea0b6a92ea98",
            "value": " 9.10M/9.10M [00:00&lt;00:00, 17.8MB/s]"
          }
        },
        "bdc8ce1c0ade40b1a816b3c4d7065633": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "829739012e34468bb12c121da9470c65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "221e8a5dcaf54353ac3286d282c73691": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ad9642f7705f404c8b422d04eda8bf78": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "074c21a49aa14549b461ec6dc6162713": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "357bf73655384b76a8e430de23fbca64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cf17b7779e1b4f64995eea0b6a92ea98": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "783e6449d3834b308fc5a13a5a3a1258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_453da33df7b94290878f3c420c06c5e1",
              "IPY_MODEL_88dd291664344378b79896ad7adc5c2b",
              "IPY_MODEL_80b39cd27d0d413fbdc1fe58510a3f0c"
            ],
            "layout": "IPY_MODEL_2c231e82c6ce462a890463616b3fa8aa"
          }
        },
        "453da33df7b94290878f3c420c06c5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_35065f738dde420bb84d2cbc40e336bd",
            "placeholder": "​",
            "style": "IPY_MODEL_73d063d137bc4e2a94e9dba39d079749",
            "value": "config.json: 100%"
          }
        },
        "88dd291664344378b79896ad7adc5c2b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2111cd6d1ba84cc9b9256581f3b1cc60",
            "max": 615,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_980a0607d18043a2a39a057d1f3a8098",
            "value": 615
          }
        },
        "80b39cd27d0d413fbdc1fe58510a3f0c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_23b38162117b4db183763c1e77b28b6c",
            "placeholder": "​",
            "style": "IPY_MODEL_0ac48cac92ee4dee82077dbb3a22e594",
            "value": " 615/615 [00:00&lt;00:00, 13.1kB/s]"
          }
        },
        "2c231e82c6ce462a890463616b3fa8aa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "35065f738dde420bb84d2cbc40e336bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "73d063d137bc4e2a94e9dba39d079749": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "2111cd6d1ba84cc9b9256581f3b1cc60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "980a0607d18043a2a39a057d1f3a8098": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "23b38162117b4db183763c1e77b28b6c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ac48cac92ee4dee82077dbb3a22e594": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a20fe4a6e3a94ea080212946463d90fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_229f43cc301d4e73b55da8bc93554d80",
              "IPY_MODEL_7f5d455e924f4e6c8a601822ec65a131",
              "IPY_MODEL_33899c3996fe420983990bd932146471"
            ],
            "layout": "IPY_MODEL_b5fa1033d1ba47e4886ebb1a57ac660d"
          }
        },
        "229f43cc301d4e73b55da8bc93554d80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6816b76aca954ddc8f1c856039d3c576",
            "placeholder": "​",
            "style": "IPY_MODEL_c37c0cb69fd24404a97a2dce15fd2f92",
            "value": "Map: 100%"
          }
        },
        "7f5d455e924f4e6c8a601822ec65a131": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_76e36638363a4cdfa21e79cbe2f1cacf",
            "max": 49741,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_2e06ab944f1c48aaa64fa7eb18619b49",
            "value": 49741
          }
        },
        "33899c3996fe420983990bd932146471": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee6ef0685a1b4c68a0f6770439536e1b",
            "placeholder": "​",
            "style": "IPY_MODEL_ff610a0530304cc29722255a6e194052",
            "value": " 49741/49741 [00:57&lt;00:00, 851.05 examples/s]"
          }
        },
        "b5fa1033d1ba47e4886ebb1a57ac660d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6816b76aca954ddc8f1c856039d3c576": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c37c0cb69fd24404a97a2dce15fd2f92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "76e36638363a4cdfa21e79cbe2f1cacf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e06ab944f1c48aaa64fa7eb18619b49": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee6ef0685a1b4c68a0f6770439536e1b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff610a0530304cc29722255a6e194052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "335045bdc0bb4a0ba0ff1f023e92ee64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cd96853831134644939740d922ef3fa6",
              "IPY_MODEL_b82e091820e04446a153716c466e5572",
              "IPY_MODEL_d9610416ab1a4fa8bb7042460a7fcead"
            ],
            "layout": "IPY_MODEL_386a45ba7fca45faba7e6979ee98cfc1"
          }
        },
        "cd96853831134644939740d922ef3fa6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdb3fac7566f4eb4b35bcfe9e61f30c0",
            "placeholder": "​",
            "style": "IPY_MODEL_3f7f12f8493642318388b83c3abbeb53",
            "value": "config.json: 100%"
          }
        },
        "b82e091820e04446a153716c466e5572": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5b200a0e80504da59fe666c96e62d241",
            "max": 762,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e80f4b6b6aed4bccbe6b28fab138fedb",
            "value": 762
          }
        },
        "d9610416ab1a4fa8bb7042460a7fcead": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_21e1cb13553d4bda9985a3fd16ec8181",
            "placeholder": "​",
            "style": "IPY_MODEL_6526e281b20f49119797c6cdbb00084e",
            "value": " 762/762 [00:00&lt;00:00, 34.0kB/s]"
          }
        },
        "386a45ba7fca45faba7e6979ee98cfc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdb3fac7566f4eb4b35bcfe9e61f30c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3f7f12f8493642318388b83c3abbeb53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5b200a0e80504da59fe666c96e62d241": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e80f4b6b6aed4bccbe6b28fab138fedb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "21e1cb13553d4bda9985a3fd16ec8181": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6526e281b20f49119797c6cdbb00084e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "410979cbeebb4e03ba39c10140deab1d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e16f766d80334f0bbcd43a417ab4166f",
              "IPY_MODEL_3c3fc92235c34f5b8030d0ef97cf6c61",
              "IPY_MODEL_8dc5aa7170204df595552494c0da9b04"
            ],
            "layout": "IPY_MODEL_31923f7151ab4025a74b1c47c6cd7dce"
          }
        },
        "e16f766d80334f0bbcd43a417ab4166f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6aa778468694aad8c9b669e43986e51",
            "placeholder": "​",
            "style": "IPY_MODEL_9b1bd6b889984392bb93461268d29b30",
            "value": "model.safetensors: 100%"
          }
        },
        "3c3fc92235c34f5b8030d0ef97cf6c61": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb4000045de148fbb6c95b144fe622c7",
            "max": 352824413,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_40414cf08ec94085b21f4c7c1c1d1807",
            "value": 352824413
          }
        },
        "8dc5aa7170204df595552494c0da9b04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e1fd1d88be404ffe9f2122530552d003",
            "placeholder": "​",
            "style": "IPY_MODEL_addc8c908e384e9e955303383d2a1edb",
            "value": " 353M/353M [00:03&lt;00:00, 172MB/s]"
          }
        },
        "31923f7151ab4025a74b1c47c6cd7dce": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f6aa778468694aad8c9b669e43986e51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b1bd6b889984392bb93461268d29b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bb4000045de148fbb6c95b144fe622c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "40414cf08ec94085b21f4c7c1c1d1807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e1fd1d88be404ffe9f2122530552d003": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "addc8c908e384e9e955303383d2a1edb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "71ff5a23177c4b06b6a34f1dfcec7a57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7a58ff427e1d4b00b13195d2aac8edd1",
              "IPY_MODEL_780768c16af74b94af4db8594a2f770a",
              "IPY_MODEL_51b2d1ce972b45bbbd2657153efcc3af"
            ],
            "layout": "IPY_MODEL_48f64a44b3db48e8b1fec2fc05e4e751"
          }
        },
        "7a58ff427e1d4b00b13195d2aac8edd1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_97b40c754713446086ab8055d0b2aecb",
            "placeholder": "​",
            "style": "IPY_MODEL_5b71e82ed99f41b99577194cc0cdc83d",
            "value": "generation_config.json: 100%"
          }
        },
        "780768c16af74b94af4db8594a2f770a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_59c1129a5ded419d82d2190987a4ac88",
            "max": 124,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c93d211cc23849c8a05e76a91b464169",
            "value": 124
          }
        },
        "51b2d1ce972b45bbbd2657153efcc3af": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ee1e6f8df1de45539a94d44ed20f6c6f",
            "placeholder": "​",
            "style": "IPY_MODEL_b31f4aae7a064f3a82d1155a5c5babe3",
            "value": " 124/124 [00:00&lt;00:00, 7.27kB/s]"
          }
        },
        "48f64a44b3db48e8b1fec2fc05e4e751": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "97b40c754713446086ab8055d0b2aecb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5b71e82ed99f41b99577194cc0cdc83d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "59c1129a5ded419d82d2190987a4ac88": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c93d211cc23849c8a05e76a91b464169": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ee1e6f8df1de45539a94d44ed20f6c6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b31f4aae7a064f3a82d1155a5c5babe3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVzhEe3yMoW3",
        "outputId": "d1713644-21a8-45b8-c09b-b7cde1234fd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow==14.0.2 in /usr/local/lib/python3.10/dist-packages (14.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==14.0.2) (1.25.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install pyarrow==14.0.2"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests==2.31.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDWn_z8hM0lz",
        "outputId": "934e76db-09a2-4f23-d76f-6fae89af4080"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests==2.31.0 in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests==2.31.0) (2024.7.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers torch pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cn0inpmcM5KG",
        "outputId": "8d9f9573-6953-4bae-bc93-e6cdecefaa42"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.41.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.3.0+cu121)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.15.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.7.4)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets==2.18.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aHYCzseNzL9",
        "outputId": "cf322abf-1973-416f-9c8c-ae28521d96cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets==2.18.0\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/510.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m163.8/510.5 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m501.8/510.5 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets==2.18.0) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==2.18.0) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets==2.18.0) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.18.0) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets==2.18.0) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.18.0) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.18.0) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==2.18.0) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==2.18.0) (1.16.0)\n",
            "Installing collected packages: datasets\n",
            "  Attempting uninstall: datasets\n",
            "    Found existing installation: datasets 2.20.0\n",
            "    Uninstalling datasets-2.20.0:\n",
            "      Successfully uninstalled datasets-2.20.0\n",
            "Successfully installed datasets-2.18.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IQTrpQTjOkzH",
        "outputId": "7999ee6d-5281-461c-d622-5be029d694a8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.18.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.15.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (16.1.0)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.0.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.4)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.23.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.7.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyarrow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bANLMxQiOzQD",
        "outputId": "23330323-ab24-4049-deba-faa44abde13c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (16.1.0)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.25.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "from datasets import Dataset\n",
        "\n",
        "# Load the dataset from the JSON file\n",
        "with open('/content/alpaca-sinhala.json', 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "\n",
        "# Convert to a DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Combine \"instruction\" and \"input\" for the prompt\n",
        "df['prompt'] = df['instruction'] + \" \" + df['input']\n",
        "df = df[['prompt', 'output']]\n",
        "\n",
        "# Create a Hugging Face dataset\n",
        "dataset = Dataset.from_pandas(df)\n"
      ],
      "metadata": {
        "id": "YeJR3u84NRMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import XLMRobertaTokenizer\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained('xlm-roberta-base')\n",
        "special_tokens_dict = {'pad_token': '[PAD]'}\n",
        "num_added_tokens = tokenizer.add_special_tokens(special_tokens_dict)\n",
        "print(f\"Added {num_added_tokens} special tokens.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326,
          "referenced_widgets": [
            "6ed8cc207701483da90ff05d4c47043a",
            "44ef62cf4b604b0988013eb95c53db76",
            "432510b142ea46bc935649b574f32e56",
            "ef629d8ddffc4e1f83467edff222ba2c",
            "5a55f06d40c3414e84844c27399bda86",
            "0576a06be4ff4e5ab0ece2547e2805ad",
            "a5c9836e9d0d4fdd86302d6807c2ddfd",
            "f569a1927cb846e586b7c99495d07f25",
            "f6439c043fcf4ae4a6e3990f1ea25a43",
            "4fba246fce2e42e2b2c22570625ee4c7",
            "c6a3324546a14e2cb3c2e028cc8595a3",
            "b0f9a6ed59f94df2994d652316184d98",
            "2514408fb73d4d2986b5ce8dd22ae882",
            "7868f692e20c4263bb05b75512c399b5",
            "6fb5ba22497c4d3ba08494394f13fd01",
            "c5ff9a64f1b4437e84b2a38bf2e9ef38",
            "8e51acfba6ef4bfbb6844e06babd7a3b",
            "3ebbb265499d41b8b0fcf9c50aa6df2f",
            "df6f56a8003a4bceb025d8cc87331848",
            "22bf885c93fb4fcd86e55ae7bef17293",
            "21b4d56125004eee88c1c82a618a86db",
            "88280e4448174fd6930a7042704299e9",
            "f7ac37ce86fb4e0e92b92fa538d455fe",
            "46296a8ddcf647398c780acb119b459a",
            "6e91d92f936848c0bc9b8c4a701bcd7a",
            "7cb95c4381894417b51b52466dcbf788",
            "bdc8ce1c0ade40b1a816b3c4d7065633",
            "829739012e34468bb12c121da9470c65",
            "221e8a5dcaf54353ac3286d282c73691",
            "ad9642f7705f404c8b422d04eda8bf78",
            "074c21a49aa14549b461ec6dc6162713",
            "357bf73655384b76a8e430de23fbca64",
            "cf17b7779e1b4f64995eea0b6a92ea98",
            "783e6449d3834b308fc5a13a5a3a1258",
            "453da33df7b94290878f3c420c06c5e1",
            "88dd291664344378b79896ad7adc5c2b",
            "80b39cd27d0d413fbdc1fe58510a3f0c",
            "2c231e82c6ce462a890463616b3fa8aa",
            "35065f738dde420bb84d2cbc40e336bd",
            "73d063d137bc4e2a94e9dba39d079749",
            "2111cd6d1ba84cc9b9256581f3b1cc60",
            "980a0607d18043a2a39a057d1f3a8098",
            "23b38162117b4db183763c1e77b28b6c",
            "0ac48cac92ee4dee82077dbb3a22e594"
          ]
        },
        "id": "3FcAlRPUNRzi",
        "outputId": "ca9b4fc1-cb28-4b7e-863f-809700ada364"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6ed8cc207701483da90ff05d4c47043a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b0f9a6ed59f94df2994d652316184d98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f7ac37ce86fb4e0e92b92fa538d455fe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/615 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "783e6449d3834b308fc5a13a5a3a1258"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Added 1 special tokens.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize_function(examples):\n",
        "    inputs = tokenizer(examples[\"prompt\"], truncation=True, padding=\"max_length\", max_length=64)\n",
        "    targets = tokenizer(examples[\"output\"], truncation=True, padding=\"max_length\", max_length=64)\n",
        "\n",
        "    inputs[\"input_ids\"] = [input_id + target_id for input_id, target_id in zip(inputs[\"input_ids\"], targets[\"input_ids\"])]\n",
        "    inputs[\"attention_mask\"] = [input_mask + target_mask for input_mask, target_mask in zip(inputs[\"attention_mask\"], targets[\"attention_mask\"])]\n",
        "    inputs[\"labels\"] = inputs[\"input_ids\"].copy()\n",
        "\n",
        "    max_token_id = max(max(input_id) for input_id in inputs[\"input_ids\"])\n",
        "    if max_token_id >= len(tokenizer):\n",
        "        raise ValueError(f\"Token ID {max_token_id} is out of range for tokenizer with vocab size {len(tokenizer)}.\")\n",
        "\n",
        "    return inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "a20fe4a6e3a94ea080212946463d90fe",
            "229f43cc301d4e73b55da8bc93554d80",
            "7f5d455e924f4e6c8a601822ec65a131",
            "33899c3996fe420983990bd932146471",
            "b5fa1033d1ba47e4886ebb1a57ac660d",
            "6816b76aca954ddc8f1c856039d3c576",
            "c37c0cb69fd24404a97a2dce15fd2f92",
            "76e36638363a4cdfa21e79cbe2f1cacf",
            "2e06ab944f1c48aaa64fa7eb18619b49",
            "ee6ef0685a1b4c68a0f6770439536e1b",
            "ff610a0530304cc29722255a6e194052"
          ]
        },
        "id": "sARvAHAaNTtX",
        "outputId": "290a0345-7b73-4702-f0d9-15a7d3eb8065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/49741 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a20fe4a6e3a94ea080212946463d90fe"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "def collate_fn(batch):\n",
        "    input_ids = torch.tensor([item['input_ids'] for item in batch])\n",
        "    attention_mask = torch.tensor([item['attention_mask'] for item in batch])\n",
        "    labels = torch.tensor([item['labels'] for item in batch])\n",
        "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
        "\n",
        "train_loader = DataLoader(tokenized_datasets, batch_size=2, collate_fn=collate_fn)\n",
        "\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "scaler = GradScaler()"
      ],
      "metadata": {
        "id": "1BfhNbX6NV7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, AdamW, get_linear_schedule_with_warmup\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"distilgpt2\")\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
        "num_training_steps = len(train_loader) * 3  # Assuming 3 epochs\n",
        "lr_scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 169,
          "referenced_widgets": [
            "335045bdc0bb4a0ba0ff1f023e92ee64",
            "cd96853831134644939740d922ef3fa6",
            "b82e091820e04446a153716c466e5572",
            "d9610416ab1a4fa8bb7042460a7fcead",
            "386a45ba7fca45faba7e6979ee98cfc1",
            "cdb3fac7566f4eb4b35bcfe9e61f30c0",
            "3f7f12f8493642318388b83c3abbeb53",
            "5b200a0e80504da59fe666c96e62d241",
            "e80f4b6b6aed4bccbe6b28fab138fedb",
            "21e1cb13553d4bda9985a3fd16ec8181",
            "6526e281b20f49119797c6cdbb00084e",
            "410979cbeebb4e03ba39c10140deab1d",
            "e16f766d80334f0bbcd43a417ab4166f",
            "3c3fc92235c34f5b8030d0ef97cf6c61",
            "8dc5aa7170204df595552494c0da9b04",
            "31923f7151ab4025a74b1c47c6cd7dce",
            "f6aa778468694aad8c9b669e43986e51",
            "9b1bd6b889984392bb93461268d29b30",
            "bb4000045de148fbb6c95b144fe622c7",
            "40414cf08ec94085b21f4c7c1c1d1807",
            "e1fd1d88be404ffe9f2122530552d003",
            "addc8c908e384e9e955303383d2a1edb",
            "71ff5a23177c4b06b6a34f1dfcec7a57",
            "7a58ff427e1d4b00b13195d2aac8edd1",
            "780768c16af74b94af4db8594a2f770a",
            "51b2d1ce972b45bbbd2657153efcc3af",
            "48f64a44b3db48e8b1fec2fc05e4e751",
            "97b40c754713446086ab8055d0b2aecb",
            "5b71e82ed99f41b99577194cc0cdc83d",
            "59c1129a5ded419d82d2190987a4ac88",
            "c93d211cc23849c8a05e76a91b464169",
            "ee1e6f8df1de45539a94d44ed20f6c6f",
            "b31f4aae7a064f3a82d1155a5c5babe3"
          ]
        },
        "id": "2p-i4SibNYQq",
        "outputId": "b72a7d0f-d628-4ebe-e96b-6df390c8fe45"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "335045bdc0bb4a0ba0ff1f023e92ee64"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "410979cbeebb4e03ba39c10140deab1d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "71ff5a23177c4b06b6a34f1dfcec7a57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
        "\n",
        "model.train()\n",
        "accumulation_steps = 4\n",
        "\n",
        "for epoch in range(3):\n",
        "    optimizer.zero_grad()\n",
        "    for step, batch in enumerate(train_loader):\n",
        "        inputs = batch['input_ids'].to(device)\n",
        "        attention_mask = batch['attention_mask'].to(device)\n",
        "        labels = batch['labels'].to(device)\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(input_ids=inputs, attention_mask=attention_mask, labels=labels)\n",
        "            loss = outputs.loss\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        if (step + 1) % accumulation_steps == 0:\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        print(f\"Epoch {epoch + 1}, Step {step + 1}, Loss: {loss.item()}\")\n",
        "\n",
        "# Save the model\n",
        "save_directory = \"./sinhala_fine_tuned_distilgpt2_2\"\n",
        "if not os.path.exists(save_directory):\n",
        "    os.makedirs(save_directory)\n",
        "\n",
        "model.save_pretrained(save_directory)\n",
        "tokenizer.save_pretrained(save_directory)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FdyDp8qsNaXZ",
        "outputId": "bf1d2196-b63a-4f28-f45b-02e6cbd18a05"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Step 1, Loss: 28.631305694580078\n",
            "Epoch 1, Step 2, Loss: 33.99980163574219\n",
            "Epoch 1, Step 3, Loss: 24.668676376342773\n",
            "Epoch 1, Step 4, Loss: 40.620079040527344\n",
            "Epoch 1, Step 5, Loss: 35.81111526489258\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/optim/lr_scheduler.py:143: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
            "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 3, Step 19872, Loss: 1.4083448648452759\n",
            "Epoch 3, Step 19873, Loss: 1.324100136756897\n",
            "Epoch 3, Step 19874, Loss: 1.0356146097183228\n",
            "Epoch 3, Step 19875, Loss: 2.096290111541748\n",
            "Epoch 3, Step 19876, Loss: 0.38733699917793274\n",
            "Epoch 3, Step 19877, Loss: 1.909152626991272\n",
            "Epoch 3, Step 19878, Loss: 2.4554953575134277\n",
            "Epoch 3, Step 19879, Loss: 1.6421046257019043\n",
            "Epoch 3, Step 19880, Loss: 2.6898889541625977\n",
            "Epoch 3, Step 19881, Loss: 0.8987845182418823\n",
            "Epoch 3, Step 19882, Loss: 2.0192883014678955\n",
            "Epoch 3, Step 19883, Loss: 0.9462364912033081\n",
            "Epoch 3, Step 19884, Loss: 0.891612708568573\n",
            "Epoch 3, Step 19885, Loss: 2.1051816940307617\n",
            "Epoch 3, Step 19886, Loss: 0.6224548816680908\n",
            "Epoch 3, Step 19887, Loss: 1.7662112712860107\n",
            "Epoch 3, Step 19888, Loss: 0.604706346988678\n",
            "Epoch 3, Step 19889, Loss: 1.5219805240631104\n",
            "Epoch 3, Step 19890, Loss: 1.6945208311080933\n",
            "Epoch 3, Step 19891, Loss: 1.6366322040557861\n",
            "Epoch 3, Step 19892, Loss: 1.484209418296814\n",
            "Epoch 3, Step 19893, Loss: 2.1629652976989746\n",
            "Epoch 3, Step 19894, Loss: 1.460059404373169\n",
            "Epoch 3, Step 19895, Loss: 1.4098016023635864\n",
            "Epoch 3, Step 19896, Loss: 1.5881950855255127\n",
            "Epoch 3, Step 19897, Loss: 1.079280138015747\n",
            "Epoch 3, Step 19898, Loss: 1.792432188987732\n",
            "Epoch 3, Step 19899, Loss: 1.6992965936660767\n",
            "Epoch 3, Step 19900, Loss: 2.6580259799957275\n",
            "Epoch 3, Step 19901, Loss: 1.2141386270523071\n",
            "Epoch 3, Step 19902, Loss: 1.4394160509109497\n",
            "Epoch 3, Step 19903, Loss: 2.5375940799713135\n",
            "Epoch 3, Step 19904, Loss: 2.2582662105560303\n",
            "Epoch 3, Step 19905, Loss: 1.9776161909103394\n",
            "Epoch 3, Step 19906, Loss: 2.6905531883239746\n",
            "Epoch 3, Step 19907, Loss: 2.178767204284668\n",
            "Epoch 3, Step 19908, Loss: 1.8439347743988037\n",
            "Epoch 3, Step 19909, Loss: 2.019395589828491\n",
            "Epoch 3, Step 19910, Loss: 1.9111273288726807\n",
            "Epoch 3, Step 19911, Loss: 1.8533978462219238\n",
            "Epoch 3, Step 19912, Loss: 1.513545274734497\n",
            "Epoch 3, Step 19913, Loss: 1.6408742666244507\n",
            "Epoch 3, Step 19914, Loss: 1.1570265293121338\n",
            "Epoch 3, Step 19915, Loss: 0.8665802478790283\n",
            "Epoch 3, Step 19916, Loss: 1.2921252250671387\n",
            "Epoch 3, Step 19917, Loss: 0.794575035572052\n",
            "Epoch 3, Step 19918, Loss: 1.2676116228103638\n",
            "Epoch 3, Step 19919, Loss: 2.02542781829834\n",
            "Epoch 3, Step 19920, Loss: 0.5229565501213074\n",
            "Epoch 3, Step 19921, Loss: 1.409149408340454\n",
            "Epoch 3, Step 19922, Loss: 1.6422611474990845\n",
            "Epoch 3, Step 19923, Loss: 1.1885086297988892\n",
            "Epoch 3, Step 19924, Loss: 1.5230165719985962\n",
            "Epoch 3, Step 19925, Loss: 1.755603313446045\n",
            "Epoch 3, Step 19926, Loss: 1.9144351482391357\n",
            "Epoch 3, Step 19927, Loss: 2.041445016860962\n",
            "Epoch 3, Step 19928, Loss: 2.64290189743042\n",
            "Epoch 3, Step 19929, Loss: 0.5931494235992432\n",
            "Epoch 3, Step 19930, Loss: 2.1559629440307617\n",
            "Epoch 3, Step 19931, Loss: 0.8250574469566345\n",
            "Epoch 3, Step 19932, Loss: 2.066178321838379\n",
            "Epoch 3, Step 19933, Loss: 1.7551116943359375\n",
            "Epoch 3, Step 19934, Loss: 1.5577428340911865\n",
            "Epoch 3, Step 19935, Loss: 0.7469585537910461\n",
            "Epoch 3, Step 19936, Loss: 0.7935820817947388\n",
            "Epoch 3, Step 19937, Loss: 1.8366338014602661\n",
            "Epoch 3, Step 19938, Loss: 2.26926589012146\n",
            "Epoch 3, Step 19939, Loss: 1.8547186851501465\n",
            "Epoch 3, Step 19940, Loss: 2.2534215450286865\n",
            "Epoch 3, Step 19941, Loss: 1.5402014255523682\n",
            "Epoch 3, Step 19942, Loss: 2.269948720932007\n",
            "Epoch 3, Step 19943, Loss: 1.682111144065857\n",
            "Epoch 3, Step 19944, Loss: 1.8278169631958008\n",
            "Epoch 3, Step 19945, Loss: 1.7931489944458008\n",
            "Epoch 3, Step 19946, Loss: 1.462043285369873\n",
            "Epoch 3, Step 19947, Loss: 0.9426727890968323\n",
            "Epoch 3, Step 19948, Loss: 1.463220238685608\n",
            "Epoch 3, Step 19949, Loss: 2.6392107009887695\n",
            "Epoch 3, Step 19950, Loss: 2.448249340057373\n",
            "Epoch 3, Step 19951, Loss: 1.9479854106903076\n",
            "Epoch 3, Step 19952, Loss: 0.6342872977256775\n",
            "Epoch 3, Step 19953, Loss: 0.5656540989875793\n",
            "Epoch 3, Step 19954, Loss: 0.8303675651550293\n",
            "Epoch 3, Step 19955, Loss: 1.64851975440979\n",
            "Epoch 3, Step 19956, Loss: 2.2432103157043457\n",
            "Epoch 3, Step 19957, Loss: 1.3776836395263672\n",
            "Epoch 3, Step 19958, Loss: 1.7965205907821655\n",
            "Epoch 3, Step 19959, Loss: 2.823671340942383\n",
            "Epoch 3, Step 19960, Loss: 1.9933240413665771\n",
            "Epoch 3, Step 19961, Loss: 1.826664686203003\n",
            "Epoch 3, Step 19962, Loss: 1.872223138809204\n",
            "Epoch 3, Step 19963, Loss: 1.2635869979858398\n",
            "Epoch 3, Step 19964, Loss: 1.5517830848693848\n",
            "Epoch 3, Step 19965, Loss: 1.5962064266204834\n",
            "Epoch 3, Step 19966, Loss: 1.6460546255111694\n",
            "Epoch 3, Step 19967, Loss: 1.2832541465759277\n",
            "Epoch 3, Step 19968, Loss: 2.1547226905822754\n",
            "Epoch 3, Step 19969, Loss: 1.6474512815475464\n",
            "Epoch 3, Step 19970, Loss: 0.5189643502235413\n",
            "Epoch 3, Step 19971, Loss: 0.7417778372764587\n",
            "Epoch 3, Step 19972, Loss: 0.5472562909126282\n",
            "Epoch 3, Step 19973, Loss: 1.7955660820007324\n",
            "Epoch 3, Step 19974, Loss: 1.8290375471115112\n",
            "Epoch 3, Step 19975, Loss: 0.4798922836780548\n",
            "Epoch 3, Step 19976, Loss: 2.1501898765563965\n",
            "Epoch 3, Step 19977, Loss: 1.3924705982208252\n",
            "Epoch 3, Step 19978, Loss: 1.7762768268585205\n",
            "Epoch 3, Step 19979, Loss: 0.5134822726249695\n",
            "Epoch 3, Step 19980, Loss: 2.3186538219451904\n",
            "Epoch 3, Step 19981, Loss: 0.9310543537139893\n",
            "Epoch 3, Step 19982, Loss: 1.1206490993499756\n",
            "Epoch 3, Step 19983, Loss: 1.0839265584945679\n",
            "Epoch 3, Step 19984, Loss: 2.187556028366089\n",
            "Epoch 3, Step 19985, Loss: 1.714482069015503\n",
            "Epoch 3, Step 19986, Loss: 1.2608016729354858\n",
            "Epoch 3, Step 19987, Loss: 1.6204756498336792\n",
            "Epoch 3, Step 19988, Loss: 2.339890718460083\n",
            "Epoch 3, Step 19989, Loss: 0.7091723084449768\n",
            "Epoch 3, Step 19990, Loss: 1.0957553386688232\n",
            "Epoch 3, Step 19991, Loss: 1.268444538116455\n",
            "Epoch 3, Step 19992, Loss: 2.558897018432617\n",
            "Epoch 3, Step 19993, Loss: 1.3919332027435303\n",
            "Epoch 3, Step 19994, Loss: 1.5219733715057373\n",
            "Epoch 3, Step 19995, Loss: 1.5710543394088745\n",
            "Epoch 3, Step 19996, Loss: 0.4029657542705536\n",
            "Epoch 3, Step 19997, Loss: 1.3499805927276611\n",
            "Epoch 3, Step 19998, Loss: 0.56842041015625\n",
            "Epoch 3, Step 19999, Loss: 1.6443812847137451\n",
            "Epoch 3, Step 20000, Loss: 1.4473921060562134\n",
            "Epoch 3, Step 20001, Loss: 1.60641348361969\n",
            "Epoch 3, Step 20002, Loss: 1.7322871685028076\n",
            "Epoch 3, Step 20003, Loss: 2.014889717102051\n",
            "Epoch 3, Step 20004, Loss: 1.9362218379974365\n",
            "Epoch 3, Step 20005, Loss: 1.8626652956008911\n",
            "Epoch 3, Step 20006, Loss: 1.3602077960968018\n",
            "Epoch 3, Step 20007, Loss: 1.4299567937850952\n",
            "Epoch 3, Step 20008, Loss: 0.7438591122627258\n",
            "Epoch 3, Step 20009, Loss: 1.1250280141830444\n",
            "Epoch 3, Step 20010, Loss: 1.115047574043274\n",
            "Epoch 3, Step 20011, Loss: 0.6836467981338501\n",
            "Epoch 3, Step 20012, Loss: 1.6683557033538818\n",
            "Epoch 3, Step 20013, Loss: 1.271157145500183\n",
            "Epoch 3, Step 20014, Loss: 0.8283467888832092\n",
            "Epoch 3, Step 20015, Loss: 0.9541506767272949\n",
            "Epoch 3, Step 20016, Loss: 2.1791787147521973\n",
            "Epoch 3, Step 20017, Loss: 0.9503424167633057\n",
            "Epoch 3, Step 20018, Loss: 1.0639175176620483\n",
            "Epoch 3, Step 20019, Loss: 2.2205123901367188\n",
            "Epoch 3, Step 20020, Loss: 1.3605409860610962\n",
            "Epoch 3, Step 20021, Loss: 1.1007044315338135\n",
            "Epoch 3, Step 20022, Loss: 1.720970869064331\n",
            "Epoch 3, Step 20023, Loss: 2.4167838096618652\n",
            "Epoch 3, Step 20024, Loss: 1.7456984519958496\n",
            "Epoch 3, Step 20025, Loss: 1.4364911317825317\n",
            "Epoch 3, Step 20026, Loss: 2.884822368621826\n",
            "Epoch 3, Step 20027, Loss: 1.78279709815979\n",
            "Epoch 3, Step 20028, Loss: 1.521543264389038\n",
            "Epoch 3, Step 20029, Loss: 1.8151133060455322\n",
            "Epoch 3, Step 20030, Loss: 1.8124010562896729\n",
            "Epoch 3, Step 20031, Loss: 0.977373480796814\n",
            "Epoch 3, Step 20032, Loss: 0.6363030672073364\n",
            "Epoch 3, Step 20033, Loss: 0.8939725160598755\n",
            "Epoch 3, Step 20034, Loss: 0.703260600566864\n",
            "Epoch 3, Step 20035, Loss: 1.2429618835449219\n",
            "Epoch 3, Step 20036, Loss: 1.5683695077896118\n",
            "Epoch 3, Step 20037, Loss: 0.6434016227722168\n",
            "Epoch 3, Step 20038, Loss: 1.9314281940460205\n",
            "Epoch 3, Step 20039, Loss: 2.021592140197754\n",
            "Epoch 3, Step 20040, Loss: 2.2104783058166504\n",
            "Epoch 3, Step 20041, Loss: 1.2712692022323608\n",
            "Epoch 3, Step 20042, Loss: 1.6855509281158447\n",
            "Epoch 3, Step 20043, Loss: 2.0869996547698975\n",
            "Epoch 3, Step 20044, Loss: 2.378293037414551\n",
            "Epoch 3, Step 20045, Loss: 1.2953301668167114\n",
            "Epoch 3, Step 20046, Loss: 1.4826595783233643\n",
            "Epoch 3, Step 20047, Loss: 1.161275029182434\n",
            "Epoch 3, Step 20048, Loss: 1.1917086839675903\n",
            "Epoch 3, Step 20049, Loss: 1.5953710079193115\n",
            "Epoch 3, Step 20050, Loss: 1.4198014736175537\n",
            "Epoch 3, Step 20051, Loss: 1.4413169622421265\n",
            "Epoch 3, Step 20052, Loss: 1.4513471126556396\n",
            "Epoch 3, Step 20053, Loss: 0.85601407289505\n",
            "Epoch 3, Step 20054, Loss: 1.0569006204605103\n",
            "Epoch 3, Step 20055, Loss: 2.24965500831604\n",
            "Epoch 3, Step 20056, Loss: 2.569546937942505\n",
            "Epoch 3, Step 20057, Loss: 1.5110077857971191\n",
            "Epoch 3, Step 20058, Loss: 2.2287471294403076\n",
            "Epoch 3, Step 20059, Loss: 1.5065749883651733\n",
            "Epoch 3, Step 20060, Loss: 2.0312697887420654\n",
            "Epoch 3, Step 20061, Loss: 0.9914695024490356\n",
            "Epoch 3, Step 20062, Loss: 1.220600962638855\n",
            "Epoch 3, Step 20063, Loss: 1.4597665071487427\n",
            "Epoch 3, Step 20064, Loss: 1.347019910812378\n",
            "Epoch 3, Step 20065, Loss: 1.6490274667739868\n",
            "Epoch 3, Step 20066, Loss: 0.5177987217903137\n",
            "Epoch 3, Step 20067, Loss: 0.7041342258453369\n",
            "Epoch 3, Step 20068, Loss: 1.2572747468948364\n",
            "Epoch 3, Step 20069, Loss: 0.5093109607696533\n",
            "Epoch 3, Step 20070, Loss: 2.3407628536224365\n",
            "Epoch 3, Step 20071, Loss: 2.0529513359069824\n",
            "Epoch 3, Step 20072, Loss: 1.6685439348220825\n",
            "Epoch 3, Step 20073, Loss: 1.2199416160583496\n",
            "Epoch 3, Step 20074, Loss: 2.1508333683013916\n",
            "Epoch 3, Step 20075, Loss: 1.2804254293441772\n",
            "Epoch 3, Step 20076, Loss: 0.5987637639045715\n",
            "Epoch 3, Step 20077, Loss: 2.025754451751709\n",
            "Epoch 3, Step 20078, Loss: 1.7543363571166992\n",
            "Epoch 3, Step 20079, Loss: 1.927034854888916\n",
            "Epoch 3, Step 20080, Loss: 0.9464848041534424\n",
            "Epoch 3, Step 20081, Loss: 0.9333174824714661\n",
            "Epoch 3, Step 20082, Loss: 2.079181671142578\n",
            "Epoch 3, Step 20083, Loss: 2.083707571029663\n",
            "Epoch 3, Step 20084, Loss: 1.4823131561279297\n",
            "Epoch 3, Step 20085, Loss: 1.488353967666626\n",
            "Epoch 3, Step 20086, Loss: 1.738302230834961\n",
            "Epoch 3, Step 20087, Loss: 1.851987361907959\n",
            "Epoch 3, Step 20088, Loss: 1.855676293373108\n",
            "Epoch 3, Step 20089, Loss: 1.974994421005249\n",
            "Epoch 3, Step 20090, Loss: 2.286836862564087\n",
            "Epoch 3, Step 20091, Loss: 0.8642812967300415\n",
            "Epoch 3, Step 20092, Loss: 1.837876319885254\n",
            "Epoch 3, Step 20093, Loss: 2.002338171005249\n",
            "Epoch 3, Step 20094, Loss: 1.443725824356079\n",
            "Epoch 3, Step 20095, Loss: 1.0898141860961914\n",
            "Epoch 3, Step 20096, Loss: 1.5355041027069092\n",
            "Epoch 3, Step 20097, Loss: 1.6676403284072876\n",
            "Epoch 3, Step 20098, Loss: 2.1862411499023438\n",
            "Epoch 3, Step 20099, Loss: 1.461836338043213\n",
            "Epoch 3, Step 20100, Loss: 0.9458256363868713\n",
            "Epoch 3, Step 20101, Loss: 2.4735093116760254\n",
            "Epoch 3, Step 20102, Loss: 1.7268705368041992\n",
            "Epoch 3, Step 20103, Loss: 1.1774632930755615\n",
            "Epoch 3, Step 20104, Loss: 1.5319786071777344\n",
            "Epoch 3, Step 20105, Loss: 1.8607969284057617\n",
            "Epoch 3, Step 20106, Loss: 2.0516915321350098\n",
            "Epoch 3, Step 20107, Loss: 1.5608092546463013\n",
            "Epoch 3, Step 20108, Loss: 1.6649373769760132\n",
            "Epoch 3, Step 20109, Loss: 2.0687954425811768\n",
            "Epoch 3, Step 20110, Loss: 2.2509233951568604\n",
            "Epoch 3, Step 20111, Loss: 1.834000587463379\n",
            "Epoch 3, Step 20112, Loss: 0.8946051597595215\n",
            "Epoch 3, Step 20113, Loss: 1.9040641784667969\n",
            "Epoch 3, Step 20114, Loss: 2.217629909515381\n",
            "Epoch 3, Step 20115, Loss: 1.740951657295227\n",
            "Epoch 3, Step 20116, Loss: 1.9527277946472168\n",
            "Epoch 3, Step 20117, Loss: 0.8899585008621216\n",
            "Epoch 3, Step 20118, Loss: 1.591829538345337\n",
            "Epoch 3, Step 20119, Loss: 0.590481162071228\n",
            "Epoch 3, Step 20120, Loss: 0.5620648264884949\n",
            "Epoch 3, Step 20121, Loss: 1.9076005220413208\n",
            "Epoch 3, Step 20122, Loss: 0.623481273651123\n",
            "Epoch 3, Step 20123, Loss: 0.9847879409790039\n",
            "Epoch 3, Step 20124, Loss: 1.4751648902893066\n",
            "Epoch 3, Step 20125, Loss: 2.5093977451324463\n",
            "Epoch 3, Step 20126, Loss: 1.8223134279251099\n",
            "Epoch 3, Step 20127, Loss: 1.1575453281402588\n",
            "Epoch 3, Step 20128, Loss: 2.726846218109131\n",
            "Epoch 3, Step 20129, Loss: 2.304790496826172\n",
            "Epoch 3, Step 20130, Loss: 0.5664666891098022\n",
            "Epoch 3, Step 20131, Loss: 1.8667203187942505\n",
            "Epoch 3, Step 20132, Loss: 2.05305552482605\n",
            "Epoch 3, Step 20133, Loss: 0.5895306468009949\n",
            "Epoch 3, Step 20134, Loss: 0.9120656251907349\n",
            "Epoch 3, Step 20135, Loss: 2.0382776260375977\n",
            "Epoch 3, Step 20136, Loss: 0.8499453663825989\n",
            "Epoch 3, Step 20137, Loss: 1.2551876306533813\n",
            "Epoch 3, Step 20138, Loss: 1.238512635231018\n",
            "Epoch 3, Step 20139, Loss: 1.1257481575012207\n",
            "Epoch 3, Step 20140, Loss: 0.8847246170043945\n",
            "Epoch 3, Step 20141, Loss: 2.215423822402954\n",
            "Epoch 3, Step 20142, Loss: 2.6359927654266357\n",
            "Epoch 3, Step 20143, Loss: 0.773125946521759\n",
            "Epoch 3, Step 20144, Loss: 2.445021152496338\n",
            "Epoch 3, Step 20145, Loss: 1.401875376701355\n",
            "Epoch 3, Step 20146, Loss: 1.8341832160949707\n",
            "Epoch 3, Step 20147, Loss: 2.1211981773376465\n",
            "Epoch 3, Step 20148, Loss: 0.5443272590637207\n",
            "Epoch 3, Step 20149, Loss: 2.051311731338501\n",
            "Epoch 3, Step 20150, Loss: 1.4676390886306763\n",
            "Epoch 3, Step 20151, Loss: 1.3544738292694092\n",
            "Epoch 3, Step 20152, Loss: 1.5115268230438232\n",
            "Epoch 3, Step 20153, Loss: 1.088462233543396\n",
            "Epoch 3, Step 20154, Loss: 1.4101872444152832\n",
            "Epoch 3, Step 20155, Loss: 1.775810956954956\n",
            "Epoch 3, Step 20156, Loss: 2.2961373329162598\n",
            "Epoch 3, Step 20157, Loss: 1.3593013286590576\n",
            "Epoch 3, Step 20158, Loss: 1.4858245849609375\n",
            "Epoch 3, Step 20159, Loss: 0.9773688316345215\n",
            "Epoch 3, Step 20160, Loss: 1.064862847328186\n",
            "Epoch 3, Step 20161, Loss: 1.7445497512817383\n",
            "Epoch 3, Step 20162, Loss: 1.0763100385665894\n",
            "Epoch 3, Step 20163, Loss: 1.6490442752838135\n",
            "Epoch 3, Step 20164, Loss: 1.1563720703125\n",
            "Epoch 3, Step 20165, Loss: 1.8181785345077515\n",
            "Epoch 3, Step 20166, Loss: 1.7855823040008545\n",
            "Epoch 3, Step 20167, Loss: 1.440362811088562\n",
            "Epoch 3, Step 20168, Loss: 2.414208173751831\n",
            "Epoch 3, Step 20169, Loss: 3.1233885288238525\n",
            "Epoch 3, Step 20170, Loss: 2.041632890701294\n",
            "Epoch 3, Step 20171, Loss: 2.132779836654663\n",
            "Epoch 3, Step 20172, Loss: 2.3472211360931396\n",
            "Epoch 3, Step 20173, Loss: 1.9018669128417969\n",
            "Epoch 3, Step 20174, Loss: 1.0715757608413696\n",
            "Epoch 3, Step 20175, Loss: 1.546918511390686\n",
            "Epoch 3, Step 20176, Loss: 0.871218740940094\n",
            "Epoch 3, Step 20177, Loss: 1.5603045225143433\n",
            "Epoch 3, Step 20178, Loss: 1.2879739999771118\n",
            "Epoch 3, Step 20179, Loss: 0.9998564720153809\n",
            "Epoch 3, Step 20180, Loss: 1.8888639211654663\n",
            "Epoch 3, Step 20181, Loss: 2.2019944190979004\n",
            "Epoch 3, Step 20182, Loss: 0.6386544704437256\n",
            "Epoch 3, Step 20183, Loss: 1.5522572994232178\n",
            "Epoch 3, Step 20184, Loss: 1.137453556060791\n",
            "Epoch 3, Step 20185, Loss: 2.141542434692383\n",
            "Epoch 3, Step 20186, Loss: 1.2627512216567993\n",
            "Epoch 3, Step 20187, Loss: 0.8702335953712463\n",
            "Epoch 3, Step 20188, Loss: 0.577781617641449\n",
            "Epoch 3, Step 20189, Loss: 2.033900499343872\n",
            "Epoch 3, Step 20190, Loss: 1.3962035179138184\n",
            "Epoch 3, Step 20191, Loss: 2.0621681213378906\n",
            "Epoch 3, Step 20192, Loss: 1.8892533779144287\n",
            "Epoch 3, Step 20193, Loss: 1.7009005546569824\n",
            "Epoch 3, Step 20194, Loss: 1.0306379795074463\n",
            "Epoch 3, Step 20195, Loss: 1.9029699563980103\n",
            "Epoch 3, Step 20196, Loss: 1.2085554599761963\n",
            "Epoch 3, Step 20197, Loss: 1.6456555128097534\n",
            "Epoch 3, Step 20198, Loss: 1.268896460533142\n",
            "Epoch 3, Step 20199, Loss: 1.643210768699646\n",
            "Epoch 3, Step 20200, Loss: 2.637331485748291\n",
            "Epoch 3, Step 20201, Loss: 1.4326844215393066\n",
            "Epoch 3, Step 20202, Loss: 1.0480875968933105\n",
            "Epoch 3, Step 20203, Loss: 1.7826639413833618\n",
            "Epoch 3, Step 20204, Loss: 1.2113416194915771\n",
            "Epoch 3, Step 20205, Loss: 1.3383351564407349\n",
            "Epoch 3, Step 20206, Loss: 0.40012332797050476\n",
            "Epoch 3, Step 20207, Loss: 1.6861566305160522\n",
            "Epoch 3, Step 20208, Loss: 1.6091272830963135\n",
            "Epoch 3, Step 20209, Loss: 2.5194478034973145\n",
            "Epoch 3, Step 20210, Loss: 2.7106428146362305\n",
            "Epoch 3, Step 20211, Loss: 1.8866621255874634\n",
            "Epoch 3, Step 20212, Loss: 0.665063738822937\n",
            "Epoch 3, Step 20213, Loss: 2.089204788208008\n",
            "Epoch 3, Step 20214, Loss: 0.7663979530334473\n",
            "Epoch 3, Step 20215, Loss: 1.8405251502990723\n",
            "Epoch 3, Step 20216, Loss: 2.2653472423553467\n",
            "Epoch 3, Step 20217, Loss: 1.796826958656311\n",
            "Epoch 3, Step 20218, Loss: 2.1978611946105957\n",
            "Epoch 3, Step 20219, Loss: 2.100123882293701\n",
            "Epoch 3, Step 20220, Loss: 1.4106128215789795\n",
            "Epoch 3, Step 20221, Loss: 2.5527803897857666\n",
            "Epoch 3, Step 20222, Loss: 1.049767017364502\n",
            "Epoch 3, Step 20223, Loss: 1.1369165182113647\n",
            "Epoch 3, Step 20224, Loss: 1.9174208641052246\n",
            "Epoch 3, Step 20225, Loss: 1.5128904581069946\n",
            "Epoch 3, Step 20226, Loss: 1.4511300325393677\n",
            "Epoch 3, Step 20227, Loss: 1.2631425857543945\n",
            "Epoch 3, Step 20228, Loss: 2.1712589263916016\n",
            "Epoch 3, Step 20229, Loss: 0.964701235294342\n",
            "Epoch 3, Step 20230, Loss: 1.0979504585266113\n",
            "Epoch 3, Step 20231, Loss: 1.4744930267333984\n",
            "Epoch 3, Step 20232, Loss: 0.5018564462661743\n",
            "Epoch 3, Step 20233, Loss: 2.1082324981689453\n",
            "Epoch 3, Step 20234, Loss: 2.3548238277435303\n",
            "Epoch 3, Step 20235, Loss: 0.6817827820777893\n",
            "Epoch 3, Step 20236, Loss: 0.9577165246009827\n",
            "Epoch 3, Step 20237, Loss: 2.6031124591827393\n",
            "Epoch 3, Step 20238, Loss: 0.9014265537261963\n",
            "Epoch 3, Step 20239, Loss: 2.3315532207489014\n",
            "Epoch 3, Step 20240, Loss: 1.661808967590332\n",
            "Epoch 3, Step 20241, Loss: 1.4352515935897827\n",
            "Epoch 3, Step 20242, Loss: 1.5280548334121704\n",
            "Epoch 3, Step 20243, Loss: 1.5346037149429321\n",
            "Epoch 3, Step 20244, Loss: 2.116997480392456\n",
            "Epoch 3, Step 20245, Loss: 1.3767908811569214\n",
            "Epoch 3, Step 20246, Loss: 1.193915605545044\n",
            "Epoch 3, Step 20247, Loss: 1.2349836826324463\n",
            "Epoch 3, Step 20248, Loss: 1.3745824098587036\n",
            "Epoch 3, Step 20249, Loss: 1.2195520401000977\n",
            "Epoch 3, Step 20250, Loss: 2.0544209480285645\n",
            "Epoch 3, Step 20251, Loss: 0.6048151850700378\n",
            "Epoch 3, Step 20252, Loss: 2.466099500656128\n",
            "Epoch 3, Step 20253, Loss: 2.644150972366333\n",
            "Epoch 3, Step 20254, Loss: 0.7480098009109497\n",
            "Epoch 3, Step 20255, Loss: 1.599415898323059\n",
            "Epoch 3, Step 20256, Loss: 1.3782727718353271\n",
            "Epoch 3, Step 20257, Loss: 0.8234692811965942\n",
            "Epoch 3, Step 20258, Loss: 2.302574634552002\n",
            "Epoch 3, Step 20259, Loss: 2.1580095291137695\n",
            "Epoch 3, Step 20260, Loss: 1.6346269845962524\n",
            "Epoch 3, Step 20261, Loss: 1.196772813796997\n",
            "Epoch 3, Step 20262, Loss: 0.6391696333885193\n",
            "Epoch 3, Step 20263, Loss: 1.1849937438964844\n",
            "Epoch 3, Step 20264, Loss: 0.5828339457511902\n",
            "Epoch 3, Step 20265, Loss: 1.6115825176239014\n",
            "Epoch 3, Step 20266, Loss: 0.9970825910568237\n",
            "Epoch 3, Step 20267, Loss: 2.0325071811676025\n",
            "Epoch 3, Step 20268, Loss: 1.0867583751678467\n",
            "Epoch 3, Step 20269, Loss: 1.8558332920074463\n",
            "Epoch 3, Step 20270, Loss: 2.023045301437378\n",
            "Epoch 3, Step 20271, Loss: 1.356018304824829\n",
            "Epoch 3, Step 20272, Loss: 1.3591445684432983\n",
            "Epoch 3, Step 20273, Loss: 1.7206361293792725\n",
            "Epoch 3, Step 20274, Loss: 0.5497159957885742\n",
            "Epoch 3, Step 20275, Loss: 0.428120881319046\n",
            "Epoch 3, Step 20276, Loss: 2.129457712173462\n",
            "Epoch 3, Step 20277, Loss: 1.8988028764724731\n",
            "Epoch 3, Step 20278, Loss: 0.7214193940162659\n",
            "Epoch 3, Step 20279, Loss: 1.0022780895233154\n",
            "Epoch 3, Step 20280, Loss: 2.281646728515625\n",
            "Epoch 3, Step 20281, Loss: 1.516372799873352\n",
            "Epoch 3, Step 20282, Loss: 1.2180616855621338\n",
            "Epoch 3, Step 20283, Loss: 0.7258461117744446\n",
            "Epoch 3, Step 20284, Loss: 0.6404260993003845\n",
            "Epoch 3, Step 20285, Loss: 0.841296374797821\n",
            "Epoch 3, Step 20286, Loss: 0.40515702962875366\n",
            "Epoch 3, Step 20287, Loss: 1.8189748525619507\n",
            "Epoch 3, Step 20288, Loss: 2.347784996032715\n",
            "Epoch 3, Step 20289, Loss: 2.2090587615966797\n",
            "Epoch 3, Step 20290, Loss: 2.0462582111358643\n",
            "Epoch 3, Step 20291, Loss: 1.2496140003204346\n",
            "Epoch 3, Step 20292, Loss: 1.3075679540634155\n",
            "Epoch 3, Step 20293, Loss: 0.9369997382164001\n",
            "Epoch 3, Step 20294, Loss: 1.371815800666809\n",
            "Epoch 3, Step 20295, Loss: 1.9992938041687012\n",
            "Epoch 3, Step 20296, Loss: 2.3884358406066895\n",
            "Epoch 3, Step 20297, Loss: 2.075124740600586\n",
            "Epoch 3, Step 20298, Loss: 2.199380874633789\n",
            "Epoch 3, Step 20299, Loss: 1.1648972034454346\n",
            "Epoch 3, Step 20300, Loss: 1.8362822532653809\n",
            "Epoch 3, Step 20301, Loss: 1.7935678958892822\n",
            "Epoch 3, Step 20302, Loss: 1.1728315353393555\n",
            "Epoch 3, Step 20303, Loss: 0.7232836484909058\n",
            "Epoch 3, Step 20304, Loss: 1.5338481664657593\n",
            "Epoch 3, Step 20305, Loss: 2.388720750808716\n",
            "Epoch 3, Step 20306, Loss: 1.5322904586791992\n",
            "Epoch 3, Step 20307, Loss: 2.1123905181884766\n",
            "Epoch 3, Step 20308, Loss: 0.7927209734916687\n",
            "Epoch 3, Step 20309, Loss: 1.6970841884613037\n",
            "Epoch 3, Step 20310, Loss: 1.5113937854766846\n",
            "Epoch 3, Step 20311, Loss: 2.2236762046813965\n",
            "Epoch 3, Step 20312, Loss: 2.272141456604004\n",
            "Epoch 3, Step 20313, Loss: 0.7098423838615417\n",
            "Epoch 3, Step 20314, Loss: 1.6976882219314575\n",
            "Epoch 3, Step 20315, Loss: 2.1955807209014893\n",
            "Epoch 3, Step 20316, Loss: 0.7869241237640381\n",
            "Epoch 3, Step 20317, Loss: 1.503535509109497\n",
            "Epoch 3, Step 20318, Loss: 2.1024129390716553\n",
            "Epoch 3, Step 20319, Loss: 1.707702875137329\n",
            "Epoch 3, Step 20320, Loss: 1.2379122972488403\n",
            "Epoch 3, Step 20321, Loss: 0.8866782188415527\n",
            "Epoch 3, Step 20322, Loss: 1.2526096105575562\n",
            "Epoch 3, Step 20323, Loss: 1.266987919807434\n",
            "Epoch 3, Step 20324, Loss: 1.0707693099975586\n",
            "Epoch 3, Step 20325, Loss: 1.4391005039215088\n",
            "Epoch 3, Step 20326, Loss: 1.808147668838501\n",
            "Epoch 3, Step 20327, Loss: 1.140688419342041\n",
            "Epoch 3, Step 20328, Loss: 0.5142256617546082\n",
            "Epoch 3, Step 20329, Loss: 1.753798007965088\n",
            "Epoch 3, Step 20330, Loss: 1.3934966325759888\n",
            "Epoch 3, Step 20331, Loss: 1.6795614957809448\n",
            "Epoch 3, Step 20332, Loss: 2.2470974922180176\n",
            "Epoch 3, Step 20333, Loss: 1.5338348150253296\n",
            "Epoch 3, Step 20334, Loss: 1.4394135475158691\n",
            "Epoch 3, Step 20335, Loss: 1.3112294673919678\n",
            "Epoch 3, Step 20336, Loss: 1.7267465591430664\n",
            "Epoch 3, Step 20337, Loss: 2.6656196117401123\n",
            "Epoch 3, Step 20338, Loss: 1.3848682641983032\n",
            "Epoch 3, Step 20339, Loss: 0.465419203042984\n",
            "Epoch 3, Step 20340, Loss: 1.723440170288086\n",
            "Epoch 3, Step 20341, Loss: 1.2014914751052856\n",
            "Epoch 3, Step 20342, Loss: 0.9335817098617554\n",
            "Epoch 3, Step 20343, Loss: 1.4600212574005127\n",
            "Epoch 3, Step 20344, Loss: 2.3878965377807617\n",
            "Epoch 3, Step 20345, Loss: 1.3286434412002563\n",
            "Epoch 3, Step 20346, Loss: 0.743844211101532\n",
            "Epoch 3, Step 20347, Loss: 1.3087105751037598\n",
            "Epoch 3, Step 20348, Loss: 3.228970766067505\n",
            "Epoch 3, Step 20349, Loss: 1.7428183555603027\n",
            "Epoch 3, Step 20350, Loss: 1.5225489139556885\n",
            "Epoch 3, Step 20351, Loss: 2.1080472469329834\n",
            "Epoch 3, Step 20352, Loss: 1.5215855836868286\n",
            "Epoch 3, Step 20353, Loss: 2.0325984954833984\n",
            "Epoch 3, Step 20354, Loss: 1.376744270324707\n",
            "Epoch 3, Step 20355, Loss: 1.708692193031311\n",
            "Epoch 3, Step 20356, Loss: 2.584578514099121\n",
            "Epoch 3, Step 20357, Loss: 1.372688889503479\n",
            "Epoch 3, Step 20358, Loss: 0.5746679902076721\n",
            "Epoch 3, Step 20359, Loss: 1.2670698165893555\n",
            "Epoch 3, Step 20360, Loss: 1.5488485097885132\n",
            "Epoch 3, Step 20361, Loss: 1.4808235168457031\n",
            "Epoch 3, Step 20362, Loss: 1.4184868335723877\n",
            "Epoch 3, Step 20363, Loss: 1.5856178998947144\n",
            "Epoch 3, Step 20364, Loss: 1.5407426357269287\n",
            "Epoch 3, Step 20365, Loss: 2.0942575931549072\n",
            "Epoch 3, Step 20366, Loss: 1.2518166303634644\n",
            "Epoch 3, Step 20367, Loss: 1.9151270389556885\n",
            "Epoch 3, Step 20368, Loss: 1.456803798675537\n",
            "Epoch 3, Step 20369, Loss: 0.9483800530433655\n",
            "Epoch 3, Step 20370, Loss: 1.5390360355377197\n",
            "Epoch 3, Step 20371, Loss: 1.361907720565796\n",
            "Epoch 3, Step 20372, Loss: 1.563846468925476\n",
            "Epoch 3, Step 20373, Loss: 1.7817871570587158\n",
            "Epoch 3, Step 20374, Loss: 1.1702773571014404\n",
            "Epoch 3, Step 20375, Loss: 1.814083218574524\n",
            "Epoch 3, Step 20376, Loss: 1.5881720781326294\n",
            "Epoch 3, Step 20377, Loss: 1.57858407497406\n",
            "Epoch 3, Step 20378, Loss: 1.6554980278015137\n",
            "Epoch 3, Step 20379, Loss: 1.187434434890747\n",
            "Epoch 3, Step 20380, Loss: 1.4779609441757202\n",
            "Epoch 3, Step 20381, Loss: 1.4647141695022583\n",
            "Epoch 3, Step 20382, Loss: 2.2921721935272217\n",
            "Epoch 3, Step 20383, Loss: 1.6649738550186157\n",
            "Epoch 3, Step 20384, Loss: 1.2875709533691406\n",
            "Epoch 3, Step 20385, Loss: 1.1651504039764404\n",
            "Epoch 3, Step 20386, Loss: 1.5562878847122192\n",
            "Epoch 3, Step 20387, Loss: 1.630204677581787\n",
            "Epoch 3, Step 20388, Loss: 1.997067928314209\n",
            "Epoch 3, Step 20389, Loss: 1.2065186500549316\n",
            "Epoch 3, Step 20390, Loss: 1.4302055835723877\n",
            "Epoch 3, Step 20391, Loss: 0.5814735293388367\n",
            "Epoch 3, Step 20392, Loss: 0.5375373959541321\n",
            "Epoch 3, Step 20393, Loss: 1.4852550029754639\n",
            "Epoch 3, Step 20394, Loss: 1.9330776929855347\n",
            "Epoch 3, Step 20395, Loss: 0.8877024054527283\n",
            "Epoch 3, Step 20396, Loss: 1.5713450908660889\n",
            "Epoch 3, Step 20397, Loss: 2.0747010707855225\n",
            "Epoch 3, Step 20398, Loss: 0.5098750591278076\n",
            "Epoch 3, Step 20399, Loss: 2.0081095695495605\n",
            "Epoch 3, Step 20400, Loss: 2.146497964859009\n",
            "Epoch 3, Step 20401, Loss: 0.42603981494903564\n",
            "Epoch 3, Step 20402, Loss: 1.2455480098724365\n",
            "Epoch 3, Step 20403, Loss: 1.9968922138214111\n",
            "Epoch 3, Step 20404, Loss: 2.552027463912964\n",
            "Epoch 3, Step 20405, Loss: 1.0504894256591797\n",
            "Epoch 3, Step 20406, Loss: 1.7063065767288208\n",
            "Epoch 3, Step 20407, Loss: 1.0779699087142944\n",
            "Epoch 3, Step 20408, Loss: 0.6329445242881775\n",
            "Epoch 3, Step 20409, Loss: 1.692155122756958\n",
            "Epoch 3, Step 20410, Loss: 1.8012268543243408\n",
            "Epoch 3, Step 20411, Loss: 2.0349998474121094\n",
            "Epoch 3, Step 20412, Loss: 1.4022915363311768\n",
            "Epoch 3, Step 20413, Loss: 1.3375352621078491\n",
            "Epoch 3, Step 20414, Loss: 1.5942704677581787\n",
            "Epoch 3, Step 20415, Loss: 1.094739556312561\n",
            "Epoch 3, Step 20416, Loss: 2.4407827854156494\n",
            "Epoch 3, Step 20417, Loss: 1.6703650951385498\n",
            "Epoch 3, Step 20418, Loss: 1.4984126091003418\n",
            "Epoch 3, Step 20419, Loss: 2.988767623901367\n",
            "Epoch 3, Step 20420, Loss: 1.3733842372894287\n",
            "Epoch 3, Step 20421, Loss: 1.2197778224945068\n",
            "Epoch 3, Step 20422, Loss: 1.7279212474822998\n",
            "Epoch 3, Step 20423, Loss: 0.49561789631843567\n",
            "Epoch 3, Step 20424, Loss: 2.3201146125793457\n",
            "Epoch 3, Step 20425, Loss: 0.7785258293151855\n",
            "Epoch 3, Step 20426, Loss: 0.5976993441581726\n",
            "Epoch 3, Step 20427, Loss: 2.2988617420196533\n",
            "Epoch 3, Step 20428, Loss: 2.2860896587371826\n",
            "Epoch 3, Step 20429, Loss: 1.157301902770996\n",
            "Epoch 3, Step 20430, Loss: 1.9615373611450195\n",
            "Epoch 3, Step 20431, Loss: 1.769044041633606\n",
            "Epoch 3, Step 20432, Loss: 1.5139647722244263\n",
            "Epoch 3, Step 20433, Loss: 0.6902744770050049\n",
            "Epoch 3, Step 20434, Loss: 2.2004446983337402\n",
            "Epoch 3, Step 20435, Loss: 1.009304165840149\n",
            "Epoch 3, Step 20436, Loss: 2.2978999614715576\n",
            "Epoch 3, Step 20437, Loss: 2.1764917373657227\n",
            "Epoch 3, Step 20438, Loss: 1.5665587186813354\n",
            "Epoch 3, Step 20439, Loss: 1.8815230131149292\n",
            "Epoch 3, Step 20440, Loss: 1.9076811075210571\n",
            "Epoch 3, Step 20441, Loss: 2.0946195125579834\n",
            "Epoch 3, Step 20442, Loss: 1.1468573808670044\n",
            "Epoch 3, Step 20443, Loss: 2.0394365787506104\n",
            "Epoch 3, Step 20444, Loss: 1.6560052633285522\n",
            "Epoch 3, Step 20445, Loss: 0.8998761177062988\n",
            "Epoch 3, Step 20446, Loss: 1.1021785736083984\n",
            "Epoch 3, Step 20447, Loss: 1.8551056385040283\n",
            "Epoch 3, Step 20448, Loss: 1.4782273769378662\n",
            "Epoch 3, Step 20449, Loss: 0.6207825541496277\n",
            "Epoch 3, Step 20450, Loss: 0.7275152802467346\n",
            "Epoch 3, Step 20451, Loss: 1.7296820878982544\n",
            "Epoch 3, Step 20452, Loss: 2.0374274253845215\n",
            "Epoch 3, Step 20453, Loss: 1.9839707612991333\n",
            "Epoch 3, Step 20454, Loss: 0.7284719944000244\n",
            "Epoch 3, Step 20455, Loss: 0.8076869249343872\n",
            "Epoch 3, Step 20456, Loss: 1.2527720928192139\n",
            "Epoch 3, Step 20457, Loss: 1.314013123512268\n",
            "Epoch 3, Step 20458, Loss: 1.7726927995681763\n",
            "Epoch 3, Step 20459, Loss: 0.35607248544692993\n",
            "Epoch 3, Step 20460, Loss: 2.579352617263794\n",
            "Epoch 3, Step 20461, Loss: 2.230893135070801\n",
            "Epoch 3, Step 20462, Loss: 1.6467673778533936\n",
            "Epoch 3, Step 20463, Loss: 1.0610908269882202\n",
            "Epoch 3, Step 20464, Loss: 1.450653314590454\n",
            "Epoch 3, Step 20465, Loss: 1.6878050565719604\n",
            "Epoch 3, Step 20466, Loss: 1.3333669900894165\n",
            "Epoch 3, Step 20467, Loss: 1.1277474164962769\n",
            "Epoch 3, Step 20468, Loss: 1.9579594135284424\n",
            "Epoch 3, Step 20469, Loss: 0.591770589351654\n",
            "Epoch 3, Step 20470, Loss: 1.3674213886260986\n",
            "Epoch 3, Step 20471, Loss: 1.8607300519943237\n",
            "Epoch 3, Step 20472, Loss: 1.1709067821502686\n",
            "Epoch 3, Step 20473, Loss: 1.588472604751587\n",
            "Epoch 3, Step 20474, Loss: 1.5175906419754028\n",
            "Epoch 3, Step 20475, Loss: 2.4351232051849365\n",
            "Epoch 3, Step 20476, Loss: 2.37585186958313\n",
            "Epoch 3, Step 20477, Loss: 1.2665247917175293\n",
            "Epoch 3, Step 20478, Loss: 1.5356091260910034\n",
            "Epoch 3, Step 20479, Loss: 1.4642860889434814\n",
            "Epoch 3, Step 20480, Loss: 1.7717013359069824\n",
            "Epoch 3, Step 20481, Loss: 1.2723662853240967\n",
            "Epoch 3, Step 20482, Loss: 2.0608999729156494\n",
            "Epoch 3, Step 20483, Loss: 2.5513627529144287\n",
            "Epoch 3, Step 20484, Loss: 2.2004220485687256\n",
            "Epoch 3, Step 20485, Loss: 0.8275071382522583\n",
            "Epoch 3, Step 20486, Loss: 1.5761854648590088\n",
            "Epoch 3, Step 20487, Loss: 0.9874661564826965\n",
            "Epoch 3, Step 20488, Loss: 2.320561170578003\n",
            "Epoch 3, Step 20489, Loss: 1.697484016418457\n",
            "Epoch 3, Step 20490, Loss: 2.3555757999420166\n",
            "Epoch 3, Step 20491, Loss: 1.2194437980651855\n",
            "Epoch 3, Step 20492, Loss: 1.2739728689193726\n",
            "Epoch 3, Step 20493, Loss: 0.6601313948631287\n",
            "Epoch 3, Step 20494, Loss: 1.7624199390411377\n",
            "Epoch 3, Step 20495, Loss: 2.403538942337036\n",
            "Epoch 3, Step 20496, Loss: 2.5713460445404053\n",
            "Epoch 3, Step 20497, Loss: 1.7833846807479858\n",
            "Epoch 3, Step 20498, Loss: 1.6933529376983643\n",
            "Epoch 3, Step 20499, Loss: 1.10757577419281\n",
            "Epoch 3, Step 20500, Loss: 1.7673426866531372\n",
            "Epoch 3, Step 20501, Loss: 2.3399479389190674\n",
            "Epoch 3, Step 20502, Loss: 1.2906655073165894\n",
            "Epoch 3, Step 20503, Loss: 1.7331098318099976\n",
            "Epoch 3, Step 20504, Loss: 1.4679325819015503\n",
            "Epoch 3, Step 20505, Loss: 2.404999256134033\n",
            "Epoch 3, Step 20506, Loss: 1.666783094406128\n",
            "Epoch 3, Step 20507, Loss: 2.2236342430114746\n",
            "Epoch 3, Step 20508, Loss: 1.47332763671875\n",
            "Epoch 3, Step 20509, Loss: 1.8683221340179443\n",
            "Epoch 3, Step 20510, Loss: 1.5577211380004883\n",
            "Epoch 3, Step 20511, Loss: 1.790231466293335\n",
            "Epoch 3, Step 20512, Loss: 1.1581838130950928\n",
            "Epoch 3, Step 20513, Loss: 0.477150559425354\n",
            "Epoch 3, Step 20514, Loss: 1.2373007535934448\n",
            "Epoch 3, Step 20515, Loss: 2.1802308559417725\n",
            "Epoch 3, Step 20516, Loss: 1.7209511995315552\n",
            "Epoch 3, Step 20517, Loss: 1.7995023727416992\n",
            "Epoch 3, Step 20518, Loss: 1.5466892719268799\n",
            "Epoch 3, Step 20519, Loss: 2.5481998920440674\n",
            "Epoch 3, Step 20520, Loss: 1.8776278495788574\n",
            "Epoch 3, Step 20521, Loss: 2.3434839248657227\n",
            "Epoch 3, Step 20522, Loss: 2.7362406253814697\n",
            "Epoch 3, Step 20523, Loss: 1.9686156511306763\n",
            "Epoch 3, Step 20524, Loss: 0.6153237819671631\n",
            "Epoch 3, Step 20525, Loss: 1.6841986179351807\n",
            "Epoch 3, Step 20526, Loss: 1.4183932542800903\n",
            "Epoch 3, Step 20527, Loss: 1.2654889822006226\n",
            "Epoch 3, Step 20528, Loss: 1.3447096347808838\n",
            "Epoch 3, Step 20529, Loss: 1.2848079204559326\n",
            "Epoch 3, Step 20530, Loss: 1.8254895210266113\n",
            "Epoch 3, Step 20531, Loss: 1.3001930713653564\n",
            "Epoch 3, Step 20532, Loss: 0.8993430137634277\n",
            "Epoch 3, Step 20533, Loss: 2.260061502456665\n",
            "Epoch 3, Step 20534, Loss: 0.6887650489807129\n",
            "Epoch 3, Step 20535, Loss: 1.4294151067733765\n",
            "Epoch 3, Step 20536, Loss: 1.8149408102035522\n",
            "Epoch 3, Step 20537, Loss: 0.6275375485420227\n",
            "Epoch 3, Step 20538, Loss: 2.7230517864227295\n",
            "Epoch 3, Step 20539, Loss: 1.7091206312179565\n",
            "Epoch 3, Step 20540, Loss: 1.9542231559753418\n",
            "Epoch 3, Step 20541, Loss: 1.2379165887832642\n",
            "Epoch 3, Step 20542, Loss: 1.2385863065719604\n",
            "Epoch 3, Step 20543, Loss: 1.4526679515838623\n",
            "Epoch 3, Step 20544, Loss: 1.3349361419677734\n",
            "Epoch 3, Step 20545, Loss: 0.969590961933136\n",
            "Epoch 3, Step 20546, Loss: 1.1052277088165283\n",
            "Epoch 3, Step 20547, Loss: 1.2381463050842285\n",
            "Epoch 3, Step 20548, Loss: 1.235878825187683\n",
            "Epoch 3, Step 20549, Loss: 0.40537476539611816\n",
            "Epoch 3, Step 20550, Loss: 1.2762656211853027\n",
            "Epoch 3, Step 20551, Loss: 2.0889930725097656\n",
            "Epoch 3, Step 20552, Loss: 1.3367981910705566\n",
            "Epoch 3, Step 20553, Loss: 1.4863736629486084\n",
            "Epoch 3, Step 20554, Loss: 2.7639715671539307\n",
            "Epoch 3, Step 20555, Loss: 1.7808160781860352\n",
            "Epoch 3, Step 20556, Loss: 2.4900143146514893\n",
            "Epoch 3, Step 20557, Loss: 0.9158295392990112\n",
            "Epoch 3, Step 20558, Loss: 1.9571961164474487\n",
            "Epoch 3, Step 20559, Loss: 0.9249357581138611\n",
            "Epoch 3, Step 20560, Loss: 1.5558847188949585\n",
            "Epoch 3, Step 20561, Loss: 0.6978803277015686\n",
            "Epoch 3, Step 20562, Loss: 1.0955172777175903\n",
            "Epoch 3, Step 20563, Loss: 2.0309367179870605\n",
            "Epoch 3, Step 20564, Loss: 2.020033359527588\n",
            "Epoch 3, Step 20565, Loss: 1.7849656343460083\n",
            "Epoch 3, Step 20566, Loss: 2.925446033477783\n",
            "Epoch 3, Step 20567, Loss: 0.4895575940608978\n",
            "Epoch 3, Step 20568, Loss: 1.1060264110565186\n",
            "Epoch 3, Step 20569, Loss: 1.8480499982833862\n",
            "Epoch 3, Step 20570, Loss: 0.6296583414077759\n",
            "Epoch 3, Step 20571, Loss: 1.3857976198196411\n",
            "Epoch 3, Step 20572, Loss: 0.7675780653953552\n",
            "Epoch 3, Step 20573, Loss: 2.190376043319702\n",
            "Epoch 3, Step 20574, Loss: 0.564705491065979\n",
            "Epoch 3, Step 20575, Loss: 1.3370187282562256\n",
            "Epoch 3, Step 20576, Loss: 0.8281523585319519\n",
            "Epoch 3, Step 20577, Loss: 0.8236105442047119\n",
            "Epoch 3, Step 20578, Loss: 2.067582845687866\n",
            "Epoch 3, Step 20579, Loss: 2.384711503982544\n",
            "Epoch 3, Step 20580, Loss: 1.6207318305969238\n",
            "Epoch 3, Step 20581, Loss: 2.0354411602020264\n",
            "Epoch 3, Step 20582, Loss: 1.3535550832748413\n",
            "Epoch 3, Step 20583, Loss: 2.561586618423462\n",
            "Epoch 3, Step 20584, Loss: 1.8827892541885376\n",
            "Epoch 3, Step 20585, Loss: 1.3550599813461304\n",
            "Epoch 3, Step 20586, Loss: 1.7984884977340698\n",
            "Epoch 3, Step 20587, Loss: 1.6513088941574097\n",
            "Epoch 3, Step 20588, Loss: 2.2592244148254395\n",
            "Epoch 3, Step 20589, Loss: 1.1376004219055176\n",
            "Epoch 3, Step 20590, Loss: 1.328176736831665\n",
            "Epoch 3, Step 20591, Loss: 1.182953953742981\n",
            "Epoch 3, Step 20592, Loss: 0.9950950145721436\n",
            "Epoch 3, Step 20593, Loss: 1.3529592752456665\n",
            "Epoch 3, Step 20594, Loss: 2.32891583442688\n",
            "Epoch 3, Step 20595, Loss: 1.2999964952468872\n",
            "Epoch 3, Step 20596, Loss: 0.7857603430747986\n",
            "Epoch 3, Step 20597, Loss: 0.7658537030220032\n",
            "Epoch 3, Step 20598, Loss: 0.5853936076164246\n",
            "Epoch 3, Step 20599, Loss: 1.4058030843734741\n",
            "Epoch 3, Step 20600, Loss: 1.2403167486190796\n",
            "Epoch 3, Step 20601, Loss: 1.6318331956863403\n",
            "Epoch 3, Step 20602, Loss: 1.1822125911712646\n",
            "Epoch 3, Step 20603, Loss: 1.5331990718841553\n",
            "Epoch 3, Step 20604, Loss: 1.2867556810379028\n",
            "Epoch 3, Step 20605, Loss: 1.180577278137207\n",
            "Epoch 3, Step 20606, Loss: 1.567033052444458\n",
            "Epoch 3, Step 20607, Loss: 0.4783181846141815\n",
            "Epoch 3, Step 20608, Loss: 1.099448800086975\n",
            "Epoch 3, Step 20609, Loss: 1.7955677509307861\n",
            "Epoch 3, Step 20610, Loss: 0.9974104166030884\n",
            "Epoch 3, Step 20611, Loss: 1.9514654874801636\n",
            "Epoch 3, Step 20612, Loss: 2.821326494216919\n",
            "Epoch 3, Step 20613, Loss: 1.452255129814148\n",
            "Epoch 3, Step 20614, Loss: 1.8315722942352295\n",
            "Epoch 3, Step 20615, Loss: 2.9748637676239014\n",
            "Epoch 3, Step 20616, Loss: 1.1449030637741089\n",
            "Epoch 3, Step 20617, Loss: 0.5117758512496948\n",
            "Epoch 3, Step 20618, Loss: 1.8670772314071655\n",
            "Epoch 3, Step 20619, Loss: 1.9424124956130981\n",
            "Epoch 3, Step 20620, Loss: 1.2355657815933228\n",
            "Epoch 3, Step 20621, Loss: 1.6115916967391968\n",
            "Epoch 3, Step 20622, Loss: 2.28031325340271\n",
            "Epoch 3, Step 20623, Loss: 2.537407636642456\n",
            "Epoch 3, Step 20624, Loss: 2.150653123855591\n",
            "Epoch 3, Step 20625, Loss: 1.7162717580795288\n",
            "Epoch 3, Step 20626, Loss: 1.3338528871536255\n",
            "Epoch 3, Step 20627, Loss: 1.4137012958526611\n",
            "Epoch 3, Step 20628, Loss: 1.2924818992614746\n",
            "Epoch 3, Step 20629, Loss: 1.2921644449234009\n",
            "Epoch 3, Step 20630, Loss: 1.4909237623214722\n",
            "Epoch 3, Step 20631, Loss: 1.2854294776916504\n",
            "Epoch 3, Step 20632, Loss: 1.7481414079666138\n",
            "Epoch 3, Step 20633, Loss: 2.286933660507202\n",
            "Epoch 3, Step 20634, Loss: 1.6052260398864746\n",
            "Epoch 3, Step 20635, Loss: 0.5188959240913391\n",
            "Epoch 3, Step 20636, Loss: 1.4406951665878296\n",
            "Epoch 3, Step 20637, Loss: 1.9052553176879883\n",
            "Epoch 3, Step 20638, Loss: 1.9680442810058594\n",
            "Epoch 3, Step 20639, Loss: 1.6137512922286987\n",
            "Epoch 3, Step 20640, Loss: 2.167847156524658\n",
            "Epoch 3, Step 20641, Loss: 2.183762788772583\n",
            "Epoch 3, Step 20642, Loss: 2.4329662322998047\n",
            "Epoch 3, Step 20643, Loss: 1.9074827432632446\n",
            "Epoch 3, Step 20644, Loss: 1.2440494298934937\n",
            "Epoch 3, Step 20645, Loss: 2.684527635574341\n",
            "Epoch 3, Step 20646, Loss: 1.0056830644607544\n",
            "Epoch 3, Step 20647, Loss: 1.5100817680358887\n",
            "Epoch 3, Step 20648, Loss: 1.950073003768921\n",
            "Epoch 3, Step 20649, Loss: 1.161971092224121\n",
            "Epoch 3, Step 20650, Loss: 1.0286662578582764\n",
            "Epoch 3, Step 20651, Loss: 0.9525207877159119\n",
            "Epoch 3, Step 20652, Loss: 1.4289110898971558\n",
            "Epoch 3, Step 20653, Loss: 1.3640204668045044\n",
            "Epoch 3, Step 20654, Loss: 1.8544882535934448\n",
            "Epoch 3, Step 20655, Loss: 0.4675840139389038\n",
            "Epoch 3, Step 20656, Loss: 1.3343918323516846\n",
            "Epoch 3, Step 20657, Loss: 2.0397775173187256\n",
            "Epoch 3, Step 20658, Loss: 1.5467755794525146\n",
            "Epoch 3, Step 20659, Loss: 2.1101720333099365\n",
            "Epoch 3, Step 20660, Loss: 1.2511723041534424\n",
            "Epoch 3, Step 20661, Loss: 0.5642287731170654\n",
            "Epoch 3, Step 20662, Loss: 0.46143510937690735\n",
            "Epoch 3, Step 20663, Loss: 0.9997512698173523\n",
            "Epoch 3, Step 20664, Loss: 1.503304123878479\n",
            "Epoch 3, Step 20665, Loss: 0.6401543021202087\n",
            "Epoch 3, Step 20666, Loss: 0.985596239566803\n",
            "Epoch 3, Step 20667, Loss: 1.1970090866088867\n",
            "Epoch 3, Step 20668, Loss: 1.0328476428985596\n",
            "Epoch 3, Step 20669, Loss: 1.4782545566558838\n",
            "Epoch 3, Step 20670, Loss: 1.7419546842575073\n",
            "Epoch 3, Step 20671, Loss: 1.4880515336990356\n",
            "Epoch 3, Step 20672, Loss: 1.7388720512390137\n",
            "Epoch 3, Step 20673, Loss: 2.669138193130493\n",
            "Epoch 3, Step 20674, Loss: 2.0656521320343018\n",
            "Epoch 3, Step 20675, Loss: 1.7716151475906372\n",
            "Epoch 3, Step 20676, Loss: 0.7164847254753113\n",
            "Epoch 3, Step 20677, Loss: 2.310833692550659\n",
            "Epoch 3, Step 20678, Loss: 1.6090130805969238\n",
            "Epoch 3, Step 20679, Loss: 1.1181614398956299\n",
            "Epoch 3, Step 20680, Loss: 1.4669369459152222\n",
            "Epoch 3, Step 20681, Loss: 1.5141091346740723\n",
            "Epoch 3, Step 20682, Loss: 0.9476016759872437\n",
            "Epoch 3, Step 20683, Loss: 1.0199975967407227\n",
            "Epoch 3, Step 20684, Loss: 2.4678800106048584\n",
            "Epoch 3, Step 20685, Loss: 1.552736520767212\n",
            "Epoch 3, Step 20686, Loss: 0.8205270767211914\n",
            "Epoch 3, Step 20687, Loss: 1.374081015586853\n",
            "Epoch 3, Step 20688, Loss: 2.0471227169036865\n",
            "Epoch 3, Step 20689, Loss: 1.7509673833847046\n",
            "Epoch 3, Step 20690, Loss: 2.431474208831787\n",
            "Epoch 3, Step 20691, Loss: 2.1983389854431152\n",
            "Epoch 3, Step 20692, Loss: 1.5219454765319824\n",
            "Epoch 3, Step 20693, Loss: 1.4032185077667236\n",
            "Epoch 3, Step 20694, Loss: 1.3155022859573364\n",
            "Epoch 3, Step 20695, Loss: 2.0477209091186523\n",
            "Epoch 3, Step 20696, Loss: 2.275174379348755\n",
            "Epoch 3, Step 20697, Loss: 2.0624489784240723\n",
            "Epoch 3, Step 20698, Loss: 1.5031918287277222\n",
            "Epoch 3, Step 20699, Loss: 1.8367371559143066\n",
            "Epoch 3, Step 20700, Loss: 2.315967559814453\n",
            "Epoch 3, Step 20701, Loss: 1.7123926877975464\n",
            "Epoch 3, Step 20702, Loss: 2.5169942378997803\n",
            "Epoch 3, Step 20703, Loss: 1.1259232759475708\n",
            "Epoch 3, Step 20704, Loss: 1.572535753250122\n",
            "Epoch 3, Step 20705, Loss: 2.065250873565674\n",
            "Epoch 3, Step 20706, Loss: 1.4900298118591309\n",
            "Epoch 3, Step 20707, Loss: 1.4763123989105225\n",
            "Epoch 3, Step 20708, Loss: 3.019213914871216\n",
            "Epoch 3, Step 20709, Loss: 2.6419548988342285\n",
            "Epoch 3, Step 20710, Loss: 2.029588222503662\n",
            "Epoch 3, Step 20711, Loss: 0.9340118169784546\n",
            "Epoch 3, Step 20712, Loss: 2.0435805320739746\n",
            "Epoch 3, Step 20713, Loss: 1.9573806524276733\n",
            "Epoch 3, Step 20714, Loss: 2.3011906147003174\n",
            "Epoch 3, Step 20715, Loss: 1.7145878076553345\n",
            "Epoch 3, Step 20716, Loss: 0.8729074597358704\n",
            "Epoch 3, Step 20717, Loss: 1.8640754222869873\n",
            "Epoch 3, Step 20718, Loss: 1.6173290014266968\n",
            "Epoch 3, Step 20719, Loss: 1.158276915550232\n",
            "Epoch 3, Step 20720, Loss: 0.7387422919273376\n",
            "Epoch 3, Step 20721, Loss: 1.4804972410202026\n",
            "Epoch 3, Step 20722, Loss: 1.4965434074401855\n",
            "Epoch 3, Step 20723, Loss: 2.035385847091675\n",
            "Epoch 3, Step 20724, Loss: 1.7638535499572754\n",
            "Epoch 3, Step 20725, Loss: 1.4386157989501953\n",
            "Epoch 3, Step 20726, Loss: 0.8306151628494263\n",
            "Epoch 3, Step 20727, Loss: 1.3775877952575684\n",
            "Epoch 3, Step 20728, Loss: 1.2033072710037231\n",
            "Epoch 3, Step 20729, Loss: 0.7804661989212036\n",
            "Epoch 3, Step 20730, Loss: 1.469947099685669\n",
            "Epoch 3, Step 20731, Loss: 1.9099152088165283\n",
            "Epoch 3, Step 20732, Loss: 1.2996675968170166\n",
            "Epoch 3, Step 20733, Loss: 1.3367547988891602\n",
            "Epoch 3, Step 20734, Loss: 1.5978785753250122\n",
            "Epoch 3, Step 20735, Loss: 1.8588696718215942\n",
            "Epoch 3, Step 20736, Loss: 1.0629205703735352\n",
            "Epoch 3, Step 20737, Loss: 0.9950511455535889\n",
            "Epoch 3, Step 20738, Loss: 1.547379493713379\n",
            "Epoch 3, Step 20739, Loss: 1.1202442646026611\n",
            "Epoch 3, Step 20740, Loss: 1.601588249206543\n",
            "Epoch 3, Step 20741, Loss: 1.3742389678955078\n",
            "Epoch 3, Step 20742, Loss: 1.4680335521697998\n",
            "Epoch 3, Step 20743, Loss: 2.0251612663269043\n",
            "Epoch 3, Step 20744, Loss: 1.5743999481201172\n",
            "Epoch 3, Step 20745, Loss: 1.3368128538131714\n",
            "Epoch 3, Step 20746, Loss: 2.5959837436676025\n",
            "Epoch 3, Step 20747, Loss: 2.31379771232605\n",
            "Epoch 3, Step 20748, Loss: 1.776820182800293\n",
            "Epoch 3, Step 20749, Loss: 1.3937519788742065\n",
            "Epoch 3, Step 20750, Loss: 2.3864989280700684\n",
            "Epoch 3, Step 20751, Loss: 1.3921829462051392\n",
            "Epoch 3, Step 20752, Loss: 1.1829540729522705\n",
            "Epoch 3, Step 20753, Loss: 1.1179876327514648\n",
            "Epoch 3, Step 20754, Loss: 1.3543858528137207\n",
            "Epoch 3, Step 20755, Loss: 2.591813325881958\n",
            "Epoch 3, Step 20756, Loss: 2.4576666355133057\n",
            "Epoch 3, Step 20757, Loss: 2.0193591117858887\n",
            "Epoch 3, Step 20758, Loss: 2.679426908493042\n",
            "Epoch 3, Step 20759, Loss: 1.2597342729568481\n",
            "Epoch 3, Step 20760, Loss: 1.3888407945632935\n",
            "Epoch 3, Step 20761, Loss: 2.4160666465759277\n",
            "Epoch 3, Step 20762, Loss: 1.1916954517364502\n",
            "Epoch 3, Step 20763, Loss: 1.418993592262268\n",
            "Epoch 3, Step 20764, Loss: 0.7920259237289429\n",
            "Epoch 3, Step 20765, Loss: 2.3299405574798584\n",
            "Epoch 3, Step 20766, Loss: 1.6755975484848022\n",
            "Epoch 3, Step 20767, Loss: 1.9430568218231201\n",
            "Epoch 3, Step 20768, Loss: 1.335079550743103\n",
            "Epoch 3, Step 20769, Loss: 1.244153380393982\n",
            "Epoch 3, Step 20770, Loss: 2.528963565826416\n",
            "Epoch 3, Step 20771, Loss: 1.9294072389602661\n",
            "Epoch 3, Step 20772, Loss: 2.077432870864868\n",
            "Epoch 3, Step 20773, Loss: 2.2305731773376465\n",
            "Epoch 3, Step 20774, Loss: 1.050551414489746\n",
            "Epoch 3, Step 20775, Loss: 1.1746370792388916\n",
            "Epoch 3, Step 20776, Loss: 2.296196460723877\n",
            "Epoch 3, Step 20777, Loss: 1.313199520111084\n",
            "Epoch 3, Step 20778, Loss: 3.1854448318481445\n",
            "Epoch 3, Step 20779, Loss: 1.8828312158584595\n",
            "Epoch 3, Step 20780, Loss: 1.4789985418319702\n",
            "Epoch 3, Step 20781, Loss: 1.0348691940307617\n",
            "Epoch 3, Step 20782, Loss: 1.6011351346969604\n",
            "Epoch 3, Step 20783, Loss: 1.4108434915542603\n",
            "Epoch 3, Step 20784, Loss: 1.3407386541366577\n",
            "Epoch 3, Step 20785, Loss: 1.5929419994354248\n",
            "Epoch 3, Step 20786, Loss: 1.5971925258636475\n",
            "Epoch 3, Step 20787, Loss: 1.9356632232666016\n",
            "Epoch 3, Step 20788, Loss: 1.3006610870361328\n",
            "Epoch 3, Step 20789, Loss: 1.310547113418579\n",
            "Epoch 3, Step 20790, Loss: 1.54643976688385\n",
            "Epoch 3, Step 20791, Loss: 2.155550003051758\n",
            "Epoch 3, Step 20792, Loss: 1.9232982397079468\n",
            "Epoch 3, Step 20793, Loss: 1.3364226818084717\n",
            "Epoch 3, Step 20794, Loss: 2.0536048412323\n",
            "Epoch 3, Step 20795, Loss: 1.762760877609253\n",
            "Epoch 3, Step 20796, Loss: 1.7591915130615234\n",
            "Epoch 3, Step 20797, Loss: 1.1011111736297607\n",
            "Epoch 3, Step 20798, Loss: 2.3622000217437744\n",
            "Epoch 3, Step 20799, Loss: 1.9618185758590698\n",
            "Epoch 3, Step 20800, Loss: 1.534325361251831\n",
            "Epoch 3, Step 20801, Loss: 1.087277889251709\n",
            "Epoch 3, Step 20802, Loss: 1.8015830516815186\n",
            "Epoch 3, Step 20803, Loss: 2.8776814937591553\n",
            "Epoch 3, Step 20804, Loss: 2.518824815750122\n",
            "Epoch 3, Step 20805, Loss: 2.7154324054718018\n",
            "Epoch 3, Step 20806, Loss: 1.5152673721313477\n",
            "Epoch 3, Step 20807, Loss: 0.7962872982025146\n",
            "Epoch 3, Step 20808, Loss: 1.4740957021713257\n",
            "Epoch 3, Step 20809, Loss: 1.8048021793365479\n",
            "Epoch 3, Step 20810, Loss: 2.3618690967559814\n",
            "Epoch 3, Step 20811, Loss: 1.1224720478057861\n",
            "Epoch 3, Step 20812, Loss: 0.684287965297699\n",
            "Epoch 3, Step 20813, Loss: 2.0913302898406982\n",
            "Epoch 3, Step 20814, Loss: 1.9167568683624268\n",
            "Epoch 3, Step 20815, Loss: 0.8002762198448181\n",
            "Epoch 3, Step 20816, Loss: 0.46029871702194214\n",
            "Epoch 3, Step 20817, Loss: 1.578429102897644\n",
            "Epoch 3, Step 20818, Loss: 1.8621221780776978\n",
            "Epoch 3, Step 20819, Loss: 1.7537119388580322\n",
            "Epoch 3, Step 20820, Loss: 1.3988174200057983\n",
            "Epoch 3, Step 20821, Loss: 2.313513994216919\n",
            "Epoch 3, Step 20822, Loss: 1.6516364812850952\n",
            "Epoch 3, Step 20823, Loss: 1.6694831848144531\n",
            "Epoch 3, Step 20824, Loss: 1.2533787488937378\n",
            "Epoch 3, Step 20825, Loss: 2.0984079837799072\n",
            "Epoch 3, Step 20826, Loss: 1.4197953939437866\n",
            "Epoch 3, Step 20827, Loss: 2.2161316871643066\n",
            "Epoch 3, Step 20828, Loss: 2.5147900581359863\n",
            "Epoch 3, Step 20829, Loss: 1.924723744392395\n",
            "Epoch 3, Step 20830, Loss: 1.9111647605895996\n",
            "Epoch 3, Step 20831, Loss: 2.1939966678619385\n",
            "Epoch 3, Step 20832, Loss: 1.771880030632019\n",
            "Epoch 3, Step 20833, Loss: 1.599339246749878\n",
            "Epoch 3, Step 20834, Loss: 1.800451636314392\n",
            "Epoch 3, Step 20835, Loss: 2.339515447616577\n",
            "Epoch 3, Step 20836, Loss: 1.558992862701416\n",
            "Epoch 3, Step 20837, Loss: 1.1040775775909424\n",
            "Epoch 3, Step 20838, Loss: 1.7578697204589844\n",
            "Epoch 3, Step 20839, Loss: 1.373453140258789\n",
            "Epoch 3, Step 20840, Loss: 1.2913336753845215\n",
            "Epoch 3, Step 20841, Loss: 0.8323644995689392\n",
            "Epoch 3, Step 20842, Loss: 1.1293185949325562\n",
            "Epoch 3, Step 20843, Loss: 1.7193529605865479\n",
            "Epoch 3, Step 20844, Loss: 1.4879616498947144\n",
            "Epoch 3, Step 20845, Loss: 2.4614369869232178\n",
            "Epoch 3, Step 20846, Loss: 1.433326244354248\n",
            "Epoch 3, Step 20847, Loss: 2.236037492752075\n",
            "Epoch 3, Step 20848, Loss: 1.575613260269165\n",
            "Epoch 3, Step 20849, Loss: 0.9184597730636597\n",
            "Epoch 3, Step 20850, Loss: 1.2108397483825684\n",
            "Epoch 3, Step 20851, Loss: 1.061551570892334\n",
            "Epoch 3, Step 20852, Loss: 0.6441284418106079\n",
            "Epoch 3, Step 20853, Loss: 1.5602269172668457\n",
            "Epoch 3, Step 20854, Loss: 2.086944341659546\n",
            "Epoch 3, Step 20855, Loss: 1.1740062236785889\n",
            "Epoch 3, Step 20856, Loss: 1.1235662698745728\n",
            "Epoch 3, Step 20857, Loss: 1.8472731113433838\n",
            "Epoch 3, Step 20858, Loss: 1.7045921087265015\n",
            "Epoch 3, Step 20859, Loss: 1.1677093505859375\n",
            "Epoch 3, Step 20860, Loss: 1.0637507438659668\n",
            "Epoch 3, Step 20861, Loss: 0.9474738836288452\n",
            "Epoch 3, Step 20862, Loss: 1.3571263551712036\n",
            "Epoch 3, Step 20863, Loss: 1.444521188735962\n",
            "Epoch 3, Step 20864, Loss: 1.200632929801941\n",
            "Epoch 3, Step 20865, Loss: 1.0808957815170288\n",
            "Epoch 3, Step 20866, Loss: 1.561537265777588\n",
            "Epoch 3, Step 20867, Loss: 0.39978060126304626\n",
            "Epoch 3, Step 20868, Loss: 2.022857666015625\n",
            "Epoch 3, Step 20869, Loss: 2.700284242630005\n",
            "Epoch 3, Step 20870, Loss: 0.5250681638717651\n",
            "Epoch 3, Step 20871, Loss: 1.1192798614501953\n",
            "Epoch 3, Step 20872, Loss: 1.6458778381347656\n",
            "Epoch 3, Step 20873, Loss: 2.4060206413269043\n",
            "Epoch 3, Step 20874, Loss: 2.2702934741973877\n",
            "Epoch 3, Step 20875, Loss: 2.0537898540496826\n",
            "Epoch 3, Step 20876, Loss: 1.2434604167938232\n",
            "Epoch 3, Step 20877, Loss: 1.5007189512252808\n",
            "Epoch 3, Step 20878, Loss: 1.756524682044983\n",
            "Epoch 3, Step 20879, Loss: 2.022608757019043\n",
            "Epoch 3, Step 20880, Loss: 1.6116681098937988\n",
            "Epoch 3, Step 20881, Loss: 0.5889439582824707\n",
            "Epoch 3, Step 20882, Loss: 1.567008376121521\n",
            "Epoch 3, Step 20883, Loss: 1.7932475805282593\n",
            "Epoch 3, Step 20884, Loss: 1.39296555519104\n",
            "Epoch 3, Step 20885, Loss: 1.755135178565979\n",
            "Epoch 3, Step 20886, Loss: 1.7928662300109863\n",
            "Epoch 3, Step 20887, Loss: 0.9398016333580017\n",
            "Epoch 3, Step 20888, Loss: 2.129041910171509\n",
            "Epoch 3, Step 20889, Loss: 0.4710463881492615\n",
            "Epoch 3, Step 20890, Loss: 1.5227464437484741\n",
            "Epoch 3, Step 20891, Loss: 1.1379613876342773\n",
            "Epoch 3, Step 20892, Loss: 0.8497788310050964\n",
            "Epoch 3, Step 20893, Loss: 1.1064516305923462\n",
            "Epoch 3, Step 20894, Loss: 1.3527302742004395\n",
            "Epoch 3, Step 20895, Loss: 0.6972446441650391\n",
            "Epoch 3, Step 20896, Loss: 1.7476634979248047\n",
            "Epoch 3, Step 20897, Loss: 2.0834336280822754\n",
            "Epoch 3, Step 20898, Loss: 1.9332584142684937\n",
            "Epoch 3, Step 20899, Loss: 1.823826551437378\n",
            "Epoch 3, Step 20900, Loss: 1.7347151041030884\n",
            "Epoch 3, Step 20901, Loss: 1.7960612773895264\n",
            "Epoch 3, Step 20902, Loss: 2.0626091957092285\n",
            "Epoch 3, Step 20903, Loss: 1.8106129169464111\n",
            "Epoch 3, Step 20904, Loss: 1.5508767366409302\n",
            "Epoch 3, Step 20905, Loss: 1.3253271579742432\n",
            "Epoch 3, Step 20906, Loss: 1.2127702236175537\n",
            "Epoch 3, Step 20907, Loss: 1.9982274770736694\n",
            "Epoch 3, Step 20908, Loss: 1.0192856788635254\n",
            "Epoch 3, Step 20909, Loss: 1.5778621435165405\n",
            "Epoch 3, Step 20910, Loss: 1.8178561925888062\n",
            "Epoch 3, Step 20911, Loss: 1.053770899772644\n",
            "Epoch 3, Step 20912, Loss: 2.421034812927246\n",
            "Epoch 3, Step 20913, Loss: 1.5822631120681763\n",
            "Epoch 3, Step 20914, Loss: 1.8716486692428589\n",
            "Epoch 3, Step 20915, Loss: 2.380955934524536\n",
            "Epoch 3, Step 20916, Loss: 1.7411302328109741\n",
            "Epoch 3, Step 20917, Loss: 2.0881574153900146\n",
            "Epoch 3, Step 20918, Loss: 1.900526523590088\n",
            "Epoch 3, Step 20919, Loss: 1.2566685676574707\n",
            "Epoch 3, Step 20920, Loss: 0.7601489424705505\n",
            "Epoch 3, Step 20921, Loss: 1.373408555984497\n",
            "Epoch 3, Step 20922, Loss: 1.2104202508926392\n",
            "Epoch 3, Step 20923, Loss: 1.0834827423095703\n",
            "Epoch 3, Step 20924, Loss: 1.5053478479385376\n",
            "Epoch 3, Step 20925, Loss: 0.7754592895507812\n",
            "Epoch 3, Step 20926, Loss: 1.0346161127090454\n",
            "Epoch 3, Step 20927, Loss: 1.7671679258346558\n",
            "Epoch 3, Step 20928, Loss: 2.0966947078704834\n",
            "Epoch 3, Step 20929, Loss: 2.3568363189697266\n",
            "Epoch 3, Step 20930, Loss: 1.6537948846817017\n",
            "Epoch 3, Step 20931, Loss: 0.9908398985862732\n",
            "Epoch 3, Step 20932, Loss: 2.5700302124023438\n",
            "Epoch 3, Step 20933, Loss: 1.0939598083496094\n",
            "Epoch 3, Step 20934, Loss: 1.8032565116882324\n",
            "Epoch 3, Step 20935, Loss: 1.298705816268921\n",
            "Epoch 3, Step 20936, Loss: 1.17045259475708\n",
            "Epoch 3, Step 20937, Loss: 1.1966983079910278\n",
            "Epoch 3, Step 20938, Loss: 1.2711186408996582\n",
            "Epoch 3, Step 20939, Loss: 0.8309899568557739\n",
            "Epoch 3, Step 20940, Loss: 0.5271010994911194\n",
            "Epoch 3, Step 20941, Loss: 2.055936098098755\n",
            "Epoch 3, Step 20942, Loss: 1.3412296772003174\n",
            "Epoch 3, Step 20943, Loss: 2.2322115898132324\n",
            "Epoch 3, Step 20944, Loss: 2.040043592453003\n",
            "Epoch 3, Step 20945, Loss: 1.7653917074203491\n",
            "Epoch 3, Step 20946, Loss: 1.4196221828460693\n",
            "Epoch 3, Step 20947, Loss: 1.6588153839111328\n",
            "Epoch 3, Step 20948, Loss: 1.7729136943817139\n",
            "Epoch 3, Step 20949, Loss: 1.1625206470489502\n",
            "Epoch 3, Step 20950, Loss: 0.8380303978919983\n",
            "Epoch 3, Step 20951, Loss: 1.5240962505340576\n",
            "Epoch 3, Step 20952, Loss: 0.40507975220680237\n",
            "Epoch 3, Step 20953, Loss: 0.47713151574134827\n",
            "Epoch 3, Step 20954, Loss: 1.547071933746338\n",
            "Epoch 3, Step 20955, Loss: 2.2094056606292725\n",
            "Epoch 3, Step 20956, Loss: 1.6071512699127197\n",
            "Epoch 3, Step 20957, Loss: 1.5477656126022339\n",
            "Epoch 3, Step 20958, Loss: 0.6686705350875854\n",
            "Epoch 3, Step 20959, Loss: 1.1209276914596558\n",
            "Epoch 3, Step 20960, Loss: 1.0194826126098633\n",
            "Epoch 3, Step 20961, Loss: 1.9214177131652832\n",
            "Epoch 3, Step 20962, Loss: 2.406737804412842\n",
            "Epoch 3, Step 20963, Loss: 1.0778285264968872\n",
            "Epoch 3, Step 20964, Loss: 1.2132608890533447\n",
            "Epoch 3, Step 20965, Loss: 2.381950855255127\n",
            "Epoch 3, Step 20966, Loss: 1.4143059253692627\n",
            "Epoch 3, Step 20967, Loss: 1.4677631855010986\n",
            "Epoch 3, Step 20968, Loss: 1.4408241510391235\n",
            "Epoch 3, Step 20969, Loss: 1.8482035398483276\n",
            "Epoch 3, Step 20970, Loss: 1.7709927558898926\n",
            "Epoch 3, Step 20971, Loss: 0.9484506249427795\n",
            "Epoch 3, Step 20972, Loss: 0.8534322381019592\n",
            "Epoch 3, Step 20973, Loss: 2.3879823684692383\n",
            "Epoch 3, Step 20974, Loss: 2.897223472595215\n",
            "Epoch 3, Step 20975, Loss: 2.1264662742614746\n",
            "Epoch 3, Step 20976, Loss: 1.987526774406433\n",
            "Epoch 3, Step 20977, Loss: 2.6307950019836426\n",
            "Epoch 3, Step 20978, Loss: 1.9474960565567017\n",
            "Epoch 3, Step 20979, Loss: 0.7180982232093811\n",
            "Epoch 3, Step 20980, Loss: 0.8104444146156311\n",
            "Epoch 3, Step 20981, Loss: 1.90911865234375\n",
            "Epoch 3, Step 20982, Loss: 1.026301383972168\n",
            "Epoch 3, Step 20983, Loss: 1.99886953830719\n",
            "Epoch 3, Step 20984, Loss: 1.2289960384368896\n",
            "Epoch 3, Step 20985, Loss: 0.5630959272384644\n",
            "Epoch 3, Step 20986, Loss: 1.41866934299469\n",
            "Epoch 3, Step 20987, Loss: 2.2715470790863037\n",
            "Epoch 3, Step 20988, Loss: 1.9523144960403442\n",
            "Epoch 3, Step 20989, Loss: 2.3082985877990723\n",
            "Epoch 3, Step 20990, Loss: 0.5750572681427002\n",
            "Epoch 3, Step 20991, Loss: 1.0262075662612915\n",
            "Epoch 3, Step 20992, Loss: 1.0931458473205566\n",
            "Epoch 3, Step 20993, Loss: 1.1364065408706665\n",
            "Epoch 3, Step 20994, Loss: 1.0943256616592407\n",
            "Epoch 3, Step 20995, Loss: 1.1454232931137085\n",
            "Epoch 3, Step 20996, Loss: 1.0742875337600708\n",
            "Epoch 3, Step 20997, Loss: 1.150523066520691\n",
            "Epoch 3, Step 20998, Loss: 2.023405075073242\n",
            "Epoch 3, Step 20999, Loss: 0.7576221823692322\n",
            "Epoch 3, Step 21000, Loss: 2.292771816253662\n",
            "Epoch 3, Step 21001, Loss: 1.8634281158447266\n",
            "Epoch 3, Step 21002, Loss: 2.012051582336426\n",
            "Epoch 3, Step 21003, Loss: 1.6899840831756592\n",
            "Epoch 3, Step 21004, Loss: 1.6233201026916504\n",
            "Epoch 3, Step 21005, Loss: 0.6847234964370728\n",
            "Epoch 3, Step 21006, Loss: 1.3440051078796387\n",
            "Epoch 3, Step 21007, Loss: 1.5994541645050049\n",
            "Epoch 3, Step 21008, Loss: 1.2100564241409302\n",
            "Epoch 3, Step 21009, Loss: 1.5328904390335083\n",
            "Epoch 3, Step 21010, Loss: 1.459661841392517\n",
            "Epoch 3, Step 21011, Loss: 1.2834094762802124\n",
            "Epoch 3, Step 21012, Loss: 1.4344689846038818\n",
            "Epoch 3, Step 21013, Loss: 1.7104036808013916\n",
            "Epoch 3, Step 21014, Loss: 1.0049400329589844\n",
            "Epoch 3, Step 21015, Loss: 0.7999128699302673\n",
            "Epoch 3, Step 21016, Loss: 2.0849146842956543\n",
            "Epoch 3, Step 21017, Loss: 1.4645378589630127\n",
            "Epoch 3, Step 21018, Loss: 1.6574428081512451\n",
            "Epoch 3, Step 21019, Loss: 1.7295238971710205\n",
            "Epoch 3, Step 21020, Loss: 0.9608021974563599\n",
            "Epoch 3, Step 21021, Loss: 1.8943488597869873\n",
            "Epoch 3, Step 21022, Loss: 2.2874653339385986\n",
            "Epoch 3, Step 21023, Loss: 1.9328185319900513\n",
            "Epoch 3, Step 21024, Loss: 1.4751225709915161\n",
            "Epoch 3, Step 21025, Loss: 1.6333352327346802\n",
            "Epoch 3, Step 21026, Loss: 0.7973674535751343\n",
            "Epoch 3, Step 21027, Loss: 2.0512661933898926\n",
            "Epoch 3, Step 21028, Loss: 2.1060032844543457\n",
            "Epoch 3, Step 21029, Loss: 1.8358784914016724\n",
            "Epoch 3, Step 21030, Loss: 1.2509920597076416\n",
            "Epoch 3, Step 21031, Loss: 2.214463233947754\n",
            "Epoch 3, Step 21032, Loss: 0.9081037640571594\n",
            "Epoch 3, Step 21033, Loss: 1.4640454053878784\n",
            "Epoch 3, Step 21034, Loss: 1.8480557203292847\n",
            "Epoch 3, Step 21035, Loss: 1.607482671737671\n",
            "Epoch 3, Step 21036, Loss: 1.3466261625289917\n",
            "Epoch 3, Step 21037, Loss: 0.8710859417915344\n",
            "Epoch 3, Step 21038, Loss: 1.1222213506698608\n",
            "Epoch 3, Step 21039, Loss: 1.684382438659668\n",
            "Epoch 3, Step 21040, Loss: 1.444043517112732\n",
            "Epoch 3, Step 21041, Loss: 0.6761317849159241\n",
            "Epoch 3, Step 21042, Loss: 1.6618266105651855\n",
            "Epoch 3, Step 21043, Loss: 1.7900960445404053\n",
            "Epoch 3, Step 21044, Loss: 2.0933375358581543\n",
            "Epoch 3, Step 21045, Loss: 2.443988561630249\n",
            "Epoch 3, Step 21046, Loss: 1.8930450677871704\n",
            "Epoch 3, Step 21047, Loss: 2.176729440689087\n",
            "Epoch 3, Step 21048, Loss: 1.4002621173858643\n",
            "Epoch 3, Step 21049, Loss: 0.7432769536972046\n",
            "Epoch 3, Step 21050, Loss: 1.5922714471817017\n",
            "Epoch 3, Step 21051, Loss: 0.7101913690567017\n",
            "Epoch 3, Step 21052, Loss: 1.4505795240402222\n",
            "Epoch 3, Step 21053, Loss: 2.1041648387908936\n",
            "Epoch 3, Step 21054, Loss: 1.4996740818023682\n",
            "Epoch 3, Step 21055, Loss: 1.9722716808319092\n",
            "Epoch 3, Step 21056, Loss: 1.2278711795806885\n",
            "Epoch 3, Step 21057, Loss: 1.959874153137207\n",
            "Epoch 3, Step 21058, Loss: 1.8779892921447754\n",
            "Epoch 3, Step 21059, Loss: 1.2412863969802856\n",
            "Epoch 3, Step 21060, Loss: 1.3018066883087158\n",
            "Epoch 3, Step 21061, Loss: 1.4026697874069214\n",
            "Epoch 3, Step 21062, Loss: 2.0618603229522705\n",
            "Epoch 3, Step 21063, Loss: 1.5195033550262451\n",
            "Epoch 3, Step 21064, Loss: 1.6290901899337769\n",
            "Epoch 3, Step 21065, Loss: 1.4356361627578735\n",
            "Epoch 3, Step 21066, Loss: 1.634268045425415\n",
            "Epoch 3, Step 21067, Loss: 2.1029157638549805\n",
            "Epoch 3, Step 21068, Loss: 1.4086790084838867\n",
            "Epoch 3, Step 21069, Loss: 1.889618992805481\n",
            "Epoch 3, Step 21070, Loss: 0.7327021360397339\n",
            "Epoch 3, Step 21071, Loss: 2.1678762435913086\n",
            "Epoch 3, Step 21072, Loss: 1.049560785293579\n",
            "Epoch 3, Step 21073, Loss: 1.3900519609451294\n",
            "Epoch 3, Step 21074, Loss: 2.2568557262420654\n",
            "Epoch 3, Step 21075, Loss: 2.316619634628296\n",
            "Epoch 3, Step 21076, Loss: 1.5529017448425293\n",
            "Epoch 3, Step 21077, Loss: 1.8810882568359375\n",
            "Epoch 3, Step 21078, Loss: 1.635475516319275\n",
            "Epoch 3, Step 21079, Loss: 2.2251102924346924\n",
            "Epoch 3, Step 21080, Loss: 2.013077735900879\n",
            "Epoch 3, Step 21081, Loss: 1.5413074493408203\n",
            "Epoch 3, Step 21082, Loss: 2.154858112335205\n",
            "Epoch 3, Step 21083, Loss: 1.6234104633331299\n",
            "Epoch 3, Step 21084, Loss: 1.14716637134552\n",
            "Epoch 3, Step 21085, Loss: 0.5436109304428101\n",
            "Epoch 3, Step 21086, Loss: 1.3912025690078735\n",
            "Epoch 3, Step 21087, Loss: 2.197612762451172\n",
            "Epoch 3, Step 21088, Loss: 1.8156445026397705\n",
            "Epoch 3, Step 21089, Loss: 0.8162302374839783\n",
            "Epoch 3, Step 21090, Loss: 0.9580241441726685\n",
            "Epoch 3, Step 21091, Loss: 1.913368582725525\n",
            "Epoch 3, Step 21092, Loss: 1.890926718711853\n",
            "Epoch 3, Step 21093, Loss: 1.4869838953018188\n",
            "Epoch 3, Step 21094, Loss: 2.392550230026245\n",
            "Epoch 3, Step 21095, Loss: 2.279428482055664\n",
            "Epoch 3, Step 21096, Loss: 1.4620109796524048\n",
            "Epoch 3, Step 21097, Loss: 1.624743103981018\n",
            "Epoch 3, Step 21098, Loss: 1.4549912214279175\n",
            "Epoch 3, Step 21099, Loss: 1.9097453355789185\n",
            "Epoch 3, Step 21100, Loss: 2.1584789752960205\n",
            "Epoch 3, Step 21101, Loss: 2.0702927112579346\n",
            "Epoch 3, Step 21102, Loss: 1.620652198791504\n",
            "Epoch 3, Step 21103, Loss: 2.6663310527801514\n",
            "Epoch 3, Step 21104, Loss: 0.3371942341327667\n",
            "Epoch 3, Step 21105, Loss: 1.4232981204986572\n",
            "Epoch 3, Step 21106, Loss: 1.1567962169647217\n",
            "Epoch 3, Step 21107, Loss: 2.3562231063842773\n",
            "Epoch 3, Step 21108, Loss: 2.6619746685028076\n",
            "Epoch 3, Step 21109, Loss: 1.3684630393981934\n",
            "Epoch 3, Step 21110, Loss: 1.5905877351760864\n",
            "Epoch 3, Step 21111, Loss: 1.8288577795028687\n",
            "Epoch 3, Step 21112, Loss: 2.1335606575012207\n",
            "Epoch 3, Step 21113, Loss: 1.0362281799316406\n",
            "Epoch 3, Step 21114, Loss: 1.5113929510116577\n",
            "Epoch 3, Step 21115, Loss: 1.6632490158081055\n",
            "Epoch 3, Step 21116, Loss: 1.7986782789230347\n",
            "Epoch 3, Step 21117, Loss: 2.0242176055908203\n",
            "Epoch 3, Step 21118, Loss: 2.3096487522125244\n",
            "Epoch 3, Step 21119, Loss: 2.072097063064575\n",
            "Epoch 3, Step 21120, Loss: 1.8758225440979004\n",
            "Epoch 3, Step 21121, Loss: 1.7255780696868896\n",
            "Epoch 3, Step 21122, Loss: 1.7186790704727173\n",
            "Epoch 3, Step 21123, Loss: 1.6589009761810303\n",
            "Epoch 3, Step 21124, Loss: 1.4835046529769897\n",
            "Epoch 3, Step 21125, Loss: 0.6853170394897461\n",
            "Epoch 3, Step 21126, Loss: 0.6735687851905823\n",
            "Epoch 3, Step 21127, Loss: 1.2969897985458374\n",
            "Epoch 3, Step 21128, Loss: 1.7476739883422852\n",
            "Epoch 3, Step 21129, Loss: 2.148977279663086\n",
            "Epoch 3, Step 21130, Loss: 2.9908223152160645\n",
            "Epoch 3, Step 21131, Loss: 2.2563953399658203\n",
            "Epoch 3, Step 21132, Loss: 0.856378972530365\n",
            "Epoch 3, Step 21133, Loss: 1.1919822692871094\n",
            "Epoch 3, Step 21134, Loss: 1.4229668378829956\n",
            "Epoch 3, Step 21135, Loss: 2.0840044021606445\n",
            "Epoch 3, Step 21136, Loss: 1.2746400833129883\n",
            "Epoch 3, Step 21137, Loss: 1.968320369720459\n",
            "Epoch 3, Step 21138, Loss: 1.3208916187286377\n",
            "Epoch 3, Step 21139, Loss: 0.9995137453079224\n",
            "Epoch 3, Step 21140, Loss: 1.3892643451690674\n",
            "Epoch 3, Step 21141, Loss: 1.14080810546875\n",
            "Epoch 3, Step 21142, Loss: 1.6007702350616455\n",
            "Epoch 3, Step 21143, Loss: 1.7932913303375244\n",
            "Epoch 3, Step 21144, Loss: 2.147045612335205\n",
            "Epoch 3, Step 21145, Loss: 0.7039147615432739\n",
            "Epoch 3, Step 21146, Loss: 1.8189910650253296\n",
            "Epoch 3, Step 21147, Loss: 3.559509754180908\n",
            "Epoch 3, Step 21148, Loss: 2.3268918991088867\n",
            "Epoch 3, Step 21149, Loss: 2.3877675533294678\n",
            "Epoch 3, Step 21150, Loss: 2.0398290157318115\n",
            "Epoch 3, Step 21151, Loss: 0.9665176272392273\n",
            "Epoch 3, Step 21152, Loss: 1.4760087728500366\n",
            "Epoch 3, Step 21153, Loss: 1.3270529508590698\n",
            "Epoch 3, Step 21154, Loss: 2.133226156234741\n",
            "Epoch 3, Step 21155, Loss: 1.9905071258544922\n",
            "Epoch 3, Step 21156, Loss: 1.2862229347229004\n",
            "Epoch 3, Step 21157, Loss: 1.2988113164901733\n",
            "Epoch 3, Step 21158, Loss: 2.8351850509643555\n",
            "Epoch 3, Step 21159, Loss: 2.06978178024292\n",
            "Epoch 3, Step 21160, Loss: 3.1897900104522705\n",
            "Epoch 3, Step 21161, Loss: 1.819683313369751\n",
            "Epoch 3, Step 21162, Loss: 2.324519157409668\n",
            "Epoch 3, Step 21163, Loss: 1.1303284168243408\n",
            "Epoch 3, Step 21164, Loss: 1.1110451221466064\n",
            "Epoch 3, Step 21165, Loss: 1.3725862503051758\n",
            "Epoch 3, Step 21166, Loss: 1.579215168952942\n",
            "Epoch 3, Step 21167, Loss: 1.410431146621704\n",
            "Epoch 3, Step 21168, Loss: 2.211143970489502\n",
            "Epoch 3, Step 21169, Loss: 1.252461314201355\n",
            "Epoch 3, Step 21170, Loss: 2.0300822257995605\n",
            "Epoch 3, Step 21171, Loss: 1.60751211643219\n",
            "Epoch 3, Step 21172, Loss: 1.7924307584762573\n",
            "Epoch 3, Step 21173, Loss: 0.9617871642112732\n",
            "Epoch 3, Step 21174, Loss: 1.2544364929199219\n",
            "Epoch 3, Step 21175, Loss: 1.785011887550354\n",
            "Epoch 3, Step 21176, Loss: 2.3109662532806396\n",
            "Epoch 3, Step 21177, Loss: 2.217172622680664\n",
            "Epoch 3, Step 21178, Loss: 1.5773197412490845\n",
            "Epoch 3, Step 21179, Loss: 1.764139175415039\n",
            "Epoch 3, Step 21180, Loss: 1.651503324508667\n",
            "Epoch 3, Step 21181, Loss: 1.2324211597442627\n",
            "Epoch 3, Step 21182, Loss: 0.9089763164520264\n",
            "Epoch 3, Step 21183, Loss: 1.4200010299682617\n",
            "Epoch 3, Step 21184, Loss: 2.3306264877319336\n",
            "Epoch 3, Step 21185, Loss: 2.1165268421173096\n",
            "Epoch 3, Step 21186, Loss: 1.8857483863830566\n",
            "Epoch 3, Step 21187, Loss: 0.6247442364692688\n",
            "Epoch 3, Step 21188, Loss: 1.3529222011566162\n",
            "Epoch 3, Step 21189, Loss: 1.2902454137802124\n",
            "Epoch 3, Step 21190, Loss: 1.2788548469543457\n",
            "Epoch 3, Step 21191, Loss: 1.4946848154067993\n",
            "Epoch 3, Step 21192, Loss: 1.5634480714797974\n",
            "Epoch 3, Step 21193, Loss: 1.6445778608322144\n",
            "Epoch 3, Step 21194, Loss: 1.44479238986969\n",
            "Epoch 3, Step 21195, Loss: 1.5641330480575562\n",
            "Epoch 3, Step 21196, Loss: 1.3725883960723877\n",
            "Epoch 3, Step 21197, Loss: 1.3518180847167969\n",
            "Epoch 3, Step 21198, Loss: 1.5754200220108032\n",
            "Epoch 3, Step 21199, Loss: 2.3052191734313965\n",
            "Epoch 3, Step 21200, Loss: 2.0141332149505615\n",
            "Epoch 3, Step 21201, Loss: 1.4448360204696655\n",
            "Epoch 3, Step 21202, Loss: 1.65097177028656\n",
            "Epoch 3, Step 21203, Loss: 1.708494782447815\n",
            "Epoch 3, Step 21204, Loss: 2.581254720687866\n",
            "Epoch 3, Step 21205, Loss: 1.7392876148223877\n",
            "Epoch 3, Step 21206, Loss: 2.42801833152771\n",
            "Epoch 3, Step 21207, Loss: 0.9421315789222717\n",
            "Epoch 3, Step 21208, Loss: 1.6221325397491455\n",
            "Epoch 3, Step 21209, Loss: 0.9892504811286926\n",
            "Epoch 3, Step 21210, Loss: 1.7932244539260864\n",
            "Epoch 3, Step 21211, Loss: 2.0978176593780518\n",
            "Epoch 3, Step 21212, Loss: 1.9578404426574707\n",
            "Epoch 3, Step 21213, Loss: 1.114310622215271\n",
            "Epoch 3, Step 21214, Loss: 1.161624550819397\n",
            "Epoch 3, Step 21215, Loss: 1.6395270824432373\n",
            "Epoch 3, Step 21216, Loss: 1.3055287599563599\n",
            "Epoch 3, Step 21217, Loss: 1.9127342700958252\n",
            "Epoch 3, Step 21218, Loss: 0.9087814688682556\n",
            "Epoch 3, Step 21219, Loss: 1.766817569732666\n",
            "Epoch 3, Step 21220, Loss: 1.1929911375045776\n",
            "Epoch 3, Step 21221, Loss: 1.999678611755371\n",
            "Epoch 3, Step 21222, Loss: 1.386590600013733\n",
            "Epoch 3, Step 21223, Loss: 0.6644749045372009\n",
            "Epoch 3, Step 21224, Loss: 0.7706844210624695\n",
            "Epoch 3, Step 21225, Loss: 1.5549391508102417\n",
            "Epoch 3, Step 21226, Loss: 1.3412973880767822\n",
            "Epoch 3, Step 21227, Loss: 0.7988134622573853\n",
            "Epoch 3, Step 21228, Loss: 1.5803236961364746\n",
            "Epoch 3, Step 21229, Loss: 2.488290309906006\n",
            "Epoch 3, Step 21230, Loss: 1.8763376474380493\n",
            "Epoch 3, Step 21231, Loss: 1.2536284923553467\n",
            "Epoch 3, Step 21232, Loss: 1.3505688905715942\n",
            "Epoch 3, Step 21233, Loss: 2.3321104049682617\n",
            "Epoch 3, Step 21234, Loss: 1.968090295791626\n",
            "Epoch 3, Step 21235, Loss: 0.7602578401565552\n",
            "Epoch 3, Step 21236, Loss: 1.2839806079864502\n",
            "Epoch 3, Step 21237, Loss: 2.323072910308838\n",
            "Epoch 3, Step 21238, Loss: 1.7720999717712402\n",
            "Epoch 3, Step 21239, Loss: 1.6634433269500732\n",
            "Epoch 3, Step 21240, Loss: 1.8466962575912476\n",
            "Epoch 3, Step 21241, Loss: 2.6946933269500732\n",
            "Epoch 3, Step 21242, Loss: 1.014592170715332\n",
            "Epoch 3, Step 21243, Loss: 1.8165266513824463\n",
            "Epoch 3, Step 21244, Loss: 0.7374593615531921\n",
            "Epoch 3, Step 21245, Loss: 1.5433154106140137\n",
            "Epoch 3, Step 21246, Loss: 1.6839298009872437\n",
            "Epoch 3, Step 21247, Loss: 2.1072216033935547\n",
            "Epoch 3, Step 21248, Loss: 1.9707096815109253\n",
            "Epoch 3, Step 21249, Loss: 1.8712810277938843\n",
            "Epoch 3, Step 21250, Loss: 2.026892900466919\n",
            "Epoch 3, Step 21251, Loss: 3.5531437397003174\n",
            "Epoch 3, Step 21252, Loss: 1.812535285949707\n",
            "Epoch 3, Step 21253, Loss: 1.8776625394821167\n",
            "Epoch 3, Step 21254, Loss: 1.0603959560394287\n",
            "Epoch 3, Step 21255, Loss: 1.0646862983703613\n",
            "Epoch 3, Step 21256, Loss: 0.7068125605583191\n",
            "Epoch 3, Step 21257, Loss: 0.8857377767562866\n",
            "Epoch 3, Step 21258, Loss: 1.1795077323913574\n",
            "Epoch 3, Step 21259, Loss: 1.948509693145752\n",
            "Epoch 3, Step 21260, Loss: 1.9851785898208618\n",
            "Epoch 3, Step 21261, Loss: 2.0306084156036377\n",
            "Epoch 3, Step 21262, Loss: 1.7129029035568237\n",
            "Epoch 3, Step 21263, Loss: 2.388977527618408\n",
            "Epoch 3, Step 21264, Loss: 2.7032978534698486\n",
            "Epoch 3, Step 21265, Loss: 2.289957046508789\n",
            "Epoch 3, Step 21266, Loss: 1.2647522687911987\n",
            "Epoch 3, Step 21267, Loss: 0.6663301587104797\n",
            "Epoch 3, Step 21268, Loss: 0.6026232242584229\n",
            "Epoch 3, Step 21269, Loss: 2.285250663757324\n",
            "Epoch 3, Step 21270, Loss: 0.7612256407737732\n",
            "Epoch 3, Step 21271, Loss: 2.5670788288116455\n",
            "Epoch 3, Step 21272, Loss: 0.9267441630363464\n",
            "Epoch 3, Step 21273, Loss: 0.6423794031143188\n",
            "Epoch 3, Step 21274, Loss: 2.092069149017334\n",
            "Epoch 3, Step 21275, Loss: 0.8342683911323547\n",
            "Epoch 3, Step 21276, Loss: 0.8246279954910278\n",
            "Epoch 3, Step 21277, Loss: 1.8390039205551147\n",
            "Epoch 3, Step 21278, Loss: 2.1647486686706543\n",
            "Epoch 3, Step 21279, Loss: 2.00284481048584\n",
            "Epoch 3, Step 21280, Loss: 1.6994051933288574\n",
            "Epoch 3, Step 21281, Loss: 2.309072732925415\n",
            "Epoch 3, Step 21282, Loss: 2.4960663318634033\n",
            "Epoch 3, Step 21283, Loss: 1.479637622833252\n",
            "Epoch 3, Step 21284, Loss: 1.2350887060165405\n",
            "Epoch 3, Step 21285, Loss: 2.106102466583252\n",
            "Epoch 3, Step 21286, Loss: 2.225545883178711\n",
            "Epoch 3, Step 21287, Loss: 2.3288886547088623\n",
            "Epoch 3, Step 21288, Loss: 1.2257938385009766\n",
            "Epoch 3, Step 21289, Loss: 1.2945811748504639\n",
            "Epoch 3, Step 21290, Loss: 1.112311601638794\n",
            "Epoch 3, Step 21291, Loss: 1.4500800371170044\n",
            "Epoch 3, Step 21292, Loss: 2.5766148567199707\n",
            "Epoch 3, Step 21293, Loss: 0.985181450843811\n",
            "Epoch 3, Step 21294, Loss: 1.2698705196380615\n",
            "Epoch 3, Step 21295, Loss: 1.3163729906082153\n",
            "Epoch 3, Step 21296, Loss: 1.810611367225647\n",
            "Epoch 3, Step 21297, Loss: 1.3403726816177368\n",
            "Epoch 3, Step 21298, Loss: 1.6504179239273071\n",
            "Epoch 3, Step 21299, Loss: 1.623872995376587\n",
            "Epoch 3, Step 21300, Loss: 1.655167579650879\n",
            "Epoch 3, Step 21301, Loss: 1.821912407875061\n",
            "Epoch 3, Step 21302, Loss: 1.9816222190856934\n",
            "Epoch 3, Step 21303, Loss: 1.4932860136032104\n",
            "Epoch 3, Step 21304, Loss: 1.9757452011108398\n",
            "Epoch 3, Step 21305, Loss: 0.5747101902961731\n",
            "Epoch 3, Step 21306, Loss: 1.0056689977645874\n",
            "Epoch 3, Step 21307, Loss: 1.6339380741119385\n",
            "Epoch 3, Step 21308, Loss: 2.3463118076324463\n",
            "Epoch 3, Step 21309, Loss: 1.1095283031463623\n",
            "Epoch 3, Step 21310, Loss: 2.3985595703125\n",
            "Epoch 3, Step 21311, Loss: 1.2050520181655884\n",
            "Epoch 3, Step 21312, Loss: 1.5114542245864868\n",
            "Epoch 3, Step 21313, Loss: 1.6617252826690674\n",
            "Epoch 3, Step 21314, Loss: 1.5398305654525757\n",
            "Epoch 3, Step 21315, Loss: 1.7994601726531982\n",
            "Epoch 3, Step 21316, Loss: 2.7152597904205322\n",
            "Epoch 3, Step 21317, Loss: 1.8116247653961182\n",
            "Epoch 3, Step 21318, Loss: 2.073866605758667\n",
            "Epoch 3, Step 21319, Loss: 0.6672807931900024\n",
            "Epoch 3, Step 21320, Loss: 1.4334956407546997\n",
            "Epoch 3, Step 21321, Loss: 0.935820460319519\n",
            "Epoch 3, Step 21322, Loss: 1.814315915107727\n",
            "Epoch 3, Step 21323, Loss: 1.7019115686416626\n",
            "Epoch 3, Step 21324, Loss: 1.7165780067443848\n",
            "Epoch 3, Step 21325, Loss: 1.411184310913086\n",
            "Epoch 3, Step 21326, Loss: 1.0766000747680664\n",
            "Epoch 3, Step 21327, Loss: 1.0105077028274536\n",
            "Epoch 3, Step 21328, Loss: 0.5352228879928589\n",
            "Epoch 3, Step 21329, Loss: 1.023443341255188\n",
            "Epoch 3, Step 21330, Loss: 1.7125459909439087\n",
            "Epoch 3, Step 21331, Loss: 1.3509927988052368\n",
            "Epoch 3, Step 21332, Loss: 2.201077461242676\n",
            "Epoch 3, Step 21333, Loss: 1.0847824811935425\n",
            "Epoch 3, Step 21334, Loss: 0.864389955997467\n",
            "Epoch 3, Step 21335, Loss: 1.2977263927459717\n",
            "Epoch 3, Step 21336, Loss: 0.8650352954864502\n",
            "Epoch 3, Step 21337, Loss: 1.1425671577453613\n",
            "Epoch 3, Step 21338, Loss: 0.9269002676010132\n",
            "Epoch 3, Step 21339, Loss: 2.080942153930664\n",
            "Epoch 3, Step 21340, Loss: 0.8170113563537598\n",
            "Epoch 3, Step 21341, Loss: 2.217402458190918\n",
            "Epoch 3, Step 21342, Loss: 2.996983289718628\n",
            "Epoch 3, Step 21343, Loss: 1.2215460538864136\n",
            "Epoch 3, Step 21344, Loss: 1.137947916984558\n",
            "Epoch 3, Step 21345, Loss: 1.108189344406128\n",
            "Epoch 3, Step 21346, Loss: 1.7920196056365967\n",
            "Epoch 3, Step 21347, Loss: 1.1087384223937988\n",
            "Epoch 3, Step 21348, Loss: 2.0568861961364746\n",
            "Epoch 3, Step 21349, Loss: 1.7522488832473755\n",
            "Epoch 3, Step 21350, Loss: 1.517483115196228\n",
            "Epoch 3, Step 21351, Loss: 2.3139193058013916\n",
            "Epoch 3, Step 21352, Loss: 1.6779824495315552\n",
            "Epoch 3, Step 21353, Loss: 1.975252628326416\n",
            "Epoch 3, Step 21354, Loss: 1.330810546875\n",
            "Epoch 3, Step 21355, Loss: 1.197147011756897\n",
            "Epoch 3, Step 21356, Loss: 1.7114607095718384\n",
            "Epoch 3, Step 21357, Loss: 0.9651128053665161\n",
            "Epoch 3, Step 21358, Loss: 1.275685429573059\n",
            "Epoch 3, Step 21359, Loss: 1.1233065128326416\n",
            "Epoch 3, Step 21360, Loss: 1.493401288986206\n",
            "Epoch 3, Step 21361, Loss: 1.7144582271575928\n",
            "Epoch 3, Step 21362, Loss: 1.513327717781067\n",
            "Epoch 3, Step 21363, Loss: 1.859542965888977\n",
            "Epoch 3, Step 21364, Loss: 1.5509135723114014\n",
            "Epoch 3, Step 21365, Loss: 0.7349303960800171\n",
            "Epoch 3, Step 21366, Loss: 1.2837799787521362\n",
            "Epoch 3, Step 21367, Loss: 1.154900074005127\n",
            "Epoch 3, Step 21368, Loss: 1.2153016328811646\n",
            "Epoch 3, Step 21369, Loss: 0.6419044137001038\n",
            "Epoch 3, Step 21370, Loss: 1.1950794458389282\n",
            "Epoch 3, Step 21371, Loss: 1.509597659111023\n",
            "Epoch 3, Step 21372, Loss: 2.505089521408081\n",
            "Epoch 3, Step 21373, Loss: 1.54069185256958\n",
            "Epoch 3, Step 21374, Loss: 1.5003788471221924\n",
            "Epoch 3, Step 21375, Loss: 2.0840554237365723\n",
            "Epoch 3, Step 21376, Loss: 1.7327169179916382\n",
            "Epoch 3, Step 21377, Loss: 1.4100438356399536\n",
            "Epoch 3, Step 21378, Loss: 1.0785706043243408\n",
            "Epoch 3, Step 21379, Loss: 2.9295339584350586\n",
            "Epoch 3, Step 21380, Loss: 2.339452028274536\n",
            "Epoch 3, Step 21381, Loss: 1.1942368745803833\n",
            "Epoch 3, Step 21382, Loss: 1.5489850044250488\n",
            "Epoch 3, Step 21383, Loss: 1.4617241621017456\n",
            "Epoch 3, Step 21384, Loss: 1.6046139001846313\n",
            "Epoch 3, Step 21385, Loss: 1.3984578847885132\n",
            "Epoch 3, Step 21386, Loss: 2.0108745098114014\n",
            "Epoch 3, Step 21387, Loss: 1.9262765645980835\n",
            "Epoch 3, Step 21388, Loss: 1.2445929050445557\n",
            "Epoch 3, Step 21389, Loss: 0.6535643935203552\n",
            "Epoch 3, Step 21390, Loss: 1.2658183574676514\n",
            "Epoch 3, Step 21391, Loss: 1.8519377708435059\n",
            "Epoch 3, Step 21392, Loss: 1.2564983367919922\n",
            "Epoch 3, Step 21393, Loss: 1.28338623046875\n",
            "Epoch 3, Step 21394, Loss: 1.777679443359375\n",
            "Epoch 3, Step 21395, Loss: 1.9941554069519043\n",
            "Epoch 3, Step 21396, Loss: 0.9336485266685486\n",
            "Epoch 3, Step 21397, Loss: 1.5087566375732422\n",
            "Epoch 3, Step 21398, Loss: 1.6628360748291016\n",
            "Epoch 3, Step 21399, Loss: 1.1110999584197998\n",
            "Epoch 3, Step 21400, Loss: 0.41610202193260193\n",
            "Epoch 3, Step 21401, Loss: 0.8345938920974731\n",
            "Epoch 3, Step 21402, Loss: 1.7829785346984863\n",
            "Epoch 3, Step 21403, Loss: 0.9382053017616272\n",
            "Epoch 3, Step 21404, Loss: 1.5461366176605225\n",
            "Epoch 3, Step 21405, Loss: 1.1141070127487183\n",
            "Epoch 3, Step 21406, Loss: 0.5982662439346313\n",
            "Epoch 3, Step 21407, Loss: 1.7210170030593872\n",
            "Epoch 3, Step 21408, Loss: 2.0820093154907227\n",
            "Epoch 3, Step 21409, Loss: 1.3168469667434692\n",
            "Epoch 3, Step 21410, Loss: 1.349965214729309\n",
            "Epoch 3, Step 21411, Loss: 1.392967939376831\n",
            "Epoch 3, Step 21412, Loss: 1.4768598079681396\n",
            "Epoch 3, Step 21413, Loss: 1.3510167598724365\n",
            "Epoch 3, Step 21414, Loss: 1.9240310192108154\n",
            "Epoch 3, Step 21415, Loss: 2.0722756385803223\n",
            "Epoch 3, Step 21416, Loss: 1.8119317293167114\n",
            "Epoch 3, Step 21417, Loss: 0.7175652980804443\n",
            "Epoch 3, Step 21418, Loss: 0.5879558324813843\n",
            "Epoch 3, Step 21419, Loss: 1.6374398469924927\n",
            "Epoch 3, Step 21420, Loss: 1.749260663986206\n",
            "Epoch 3, Step 21421, Loss: 1.7045881748199463\n",
            "Epoch 3, Step 21422, Loss: 2.2267234325408936\n",
            "Epoch 3, Step 21423, Loss: 1.3192150592803955\n",
            "Epoch 3, Step 21424, Loss: 0.7052086591720581\n",
            "Epoch 3, Step 21425, Loss: 1.3995753526687622\n",
            "Epoch 3, Step 21426, Loss: 1.3970948457717896\n",
            "Epoch 3, Step 21427, Loss: 1.9127355813980103\n",
            "Epoch 3, Step 21428, Loss: 1.9825146198272705\n",
            "Epoch 3, Step 21429, Loss: 2.1212236881256104\n",
            "Epoch 3, Step 21430, Loss: 2.0665292739868164\n",
            "Epoch 3, Step 21431, Loss: 1.8901269435882568\n",
            "Epoch 3, Step 21432, Loss: 1.0789402723312378\n",
            "Epoch 3, Step 21433, Loss: 1.597154974937439\n",
            "Epoch 3, Step 21434, Loss: 0.8252080082893372\n",
            "Epoch 3, Step 21435, Loss: 0.5800803303718567\n",
            "Epoch 3, Step 21436, Loss: 2.5725302696228027\n",
            "Epoch 3, Step 21437, Loss: 0.908141016960144\n",
            "Epoch 3, Step 21438, Loss: 2.090970277786255\n",
            "Epoch 3, Step 21439, Loss: 1.523250937461853\n",
            "Epoch 3, Step 21440, Loss: 2.294846534729004\n",
            "Epoch 3, Step 21441, Loss: 1.3067129850387573\n",
            "Epoch 3, Step 21442, Loss: 1.427689552307129\n",
            "Epoch 3, Step 21443, Loss: 1.2535756826400757\n",
            "Epoch 3, Step 21444, Loss: 2.3610739707946777\n",
            "Epoch 3, Step 21445, Loss: 1.4916961193084717\n",
            "Epoch 3, Step 21446, Loss: 0.890937328338623\n",
            "Epoch 3, Step 21447, Loss: 1.5862491130828857\n",
            "Epoch 3, Step 21448, Loss: 2.1540658473968506\n",
            "Epoch 3, Step 21449, Loss: 2.4003076553344727\n",
            "Epoch 3, Step 21450, Loss: 2.850071668624878\n",
            "Epoch 3, Step 21451, Loss: 1.7014631032943726\n",
            "Epoch 3, Step 21452, Loss: 1.9267730712890625\n",
            "Epoch 3, Step 21453, Loss: 1.3623437881469727\n",
            "Epoch 3, Step 21454, Loss: 1.75596022605896\n",
            "Epoch 3, Step 21455, Loss: 1.0810085535049438\n",
            "Epoch 3, Step 21456, Loss: 1.4556372165679932\n",
            "Epoch 3, Step 21457, Loss: 1.3111445903778076\n",
            "Epoch 3, Step 21458, Loss: 2.5460100173950195\n",
            "Epoch 3, Step 21459, Loss: 0.9772093892097473\n",
            "Epoch 3, Step 21460, Loss: 1.1601996421813965\n",
            "Epoch 3, Step 21461, Loss: 1.1592217683792114\n",
            "Epoch 3, Step 21462, Loss: 1.3458130359649658\n",
            "Epoch 3, Step 21463, Loss: 1.0765937566757202\n",
            "Epoch 3, Step 21464, Loss: 2.3734872341156006\n",
            "Epoch 3, Step 21465, Loss: 1.1889091730117798\n",
            "Epoch 3, Step 21466, Loss: 2.0218663215637207\n",
            "Epoch 3, Step 21467, Loss: 1.3688794374465942\n",
            "Epoch 3, Step 21468, Loss: 0.6448904275894165\n",
            "Epoch 3, Step 21469, Loss: 1.0834506750106812\n",
            "Epoch 3, Step 21470, Loss: 1.4753259420394897\n",
            "Epoch 3, Step 21471, Loss: 0.5419367551803589\n",
            "Epoch 3, Step 21472, Loss: 0.9540224075317383\n",
            "Epoch 3, Step 21473, Loss: 1.152478575706482\n",
            "Epoch 3, Step 21474, Loss: 2.4634156227111816\n",
            "Epoch 3, Step 21475, Loss: 2.2219479084014893\n",
            "Epoch 3, Step 21476, Loss: 2.2731873989105225\n",
            "Epoch 3, Step 21477, Loss: 2.0078911781311035\n",
            "Epoch 3, Step 21478, Loss: 1.8982206583023071\n",
            "Epoch 3, Step 21479, Loss: 2.647066354751587\n",
            "Epoch 3, Step 21480, Loss: 1.865976095199585\n",
            "Epoch 3, Step 21481, Loss: 1.7811987400054932\n",
            "Epoch 3, Step 21482, Loss: 1.6576615571975708\n",
            "Epoch 3, Step 21483, Loss: 1.5184825658798218\n",
            "Epoch 3, Step 21484, Loss: 1.3515146970748901\n",
            "Epoch 3, Step 21485, Loss: 1.714380145072937\n",
            "Epoch 3, Step 21486, Loss: 0.6606096029281616\n",
            "Epoch 3, Step 21487, Loss: 1.8088536262512207\n",
            "Epoch 3, Step 21488, Loss: 2.183051347732544\n",
            "Epoch 3, Step 21489, Loss: 0.6907015442848206\n",
            "Epoch 3, Step 21490, Loss: 2.3699588775634766\n",
            "Epoch 3, Step 21491, Loss: 2.2739944458007812\n",
            "Epoch 3, Step 21492, Loss: 1.3917583227157593\n",
            "Epoch 3, Step 21493, Loss: 1.1430691480636597\n",
            "Epoch 3, Step 21494, Loss: 2.238330125808716\n",
            "Epoch 3, Step 21495, Loss: 1.6583949327468872\n",
            "Epoch 3, Step 21496, Loss: 1.7343754768371582\n",
            "Epoch 3, Step 21497, Loss: 1.2059683799743652\n",
            "Epoch 3, Step 21498, Loss: 2.2789087295532227\n",
            "Epoch 3, Step 21499, Loss: 1.8939509391784668\n",
            "Epoch 3, Step 21500, Loss: 0.6758257746696472\n",
            "Epoch 3, Step 21501, Loss: 1.4710720777511597\n",
            "Epoch 3, Step 21502, Loss: 0.4504375755786896\n",
            "Epoch 3, Step 21503, Loss: 0.5949172973632812\n",
            "Epoch 3, Step 21504, Loss: 0.7289990186691284\n",
            "Epoch 3, Step 21505, Loss: 1.2125307321548462\n",
            "Epoch 3, Step 21506, Loss: 1.1545751094818115\n",
            "Epoch 3, Step 21507, Loss: 2.0223917961120605\n",
            "Epoch 3, Step 21508, Loss: 1.8408490419387817\n",
            "Epoch 3, Step 21509, Loss: 1.661503791809082\n",
            "Epoch 3, Step 21510, Loss: 2.0342793464660645\n",
            "Epoch 3, Step 21511, Loss: 0.7771511673927307\n",
            "Epoch 3, Step 21512, Loss: 0.5788812041282654\n",
            "Epoch 3, Step 21513, Loss: 1.7467293739318848\n",
            "Epoch 3, Step 21514, Loss: 1.1685434579849243\n",
            "Epoch 3, Step 21515, Loss: 2.26861834526062\n",
            "Epoch 3, Step 21516, Loss: 1.4825048446655273\n",
            "Epoch 3, Step 21517, Loss: 1.559887409210205\n",
            "Epoch 3, Step 21518, Loss: 1.9209939241409302\n",
            "Epoch 3, Step 21519, Loss: 0.7316536903381348\n",
            "Epoch 3, Step 21520, Loss: 1.3215824365615845\n",
            "Epoch 3, Step 21521, Loss: 1.7307032346725464\n",
            "Epoch 3, Step 21522, Loss: 2.292509078979492\n",
            "Epoch 3, Step 21523, Loss: 0.4722610414028168\n",
            "Epoch 3, Step 21524, Loss: 1.8290773630142212\n",
            "Epoch 3, Step 21525, Loss: 1.5234036445617676\n",
            "Epoch 3, Step 21526, Loss: 2.090350866317749\n",
            "Epoch 3, Step 21527, Loss: 1.395120620727539\n",
            "Epoch 3, Step 21528, Loss: 1.6082042455673218\n",
            "Epoch 3, Step 21529, Loss: 1.5452719926834106\n",
            "Epoch 3, Step 21530, Loss: 2.1495015621185303\n",
            "Epoch 3, Step 21531, Loss: 0.716597855091095\n",
            "Epoch 3, Step 21532, Loss: 1.38605535030365\n",
            "Epoch 3, Step 21533, Loss: 1.3272979259490967\n",
            "Epoch 3, Step 21534, Loss: 1.685778260231018\n",
            "Epoch 3, Step 21535, Loss: 1.061477780342102\n",
            "Epoch 3, Step 21536, Loss: 2.5683557987213135\n",
            "Epoch 3, Step 21537, Loss: 1.0695464611053467\n",
            "Epoch 3, Step 21538, Loss: 2.288820266723633\n",
            "Epoch 3, Step 21539, Loss: 2.022526741027832\n",
            "Epoch 3, Step 21540, Loss: 1.3266977071762085\n",
            "Epoch 3, Step 21541, Loss: 1.224077820777893\n",
            "Epoch 3, Step 21542, Loss: 2.3467109203338623\n",
            "Epoch 3, Step 21543, Loss: 1.052090048789978\n",
            "Epoch 3, Step 21544, Loss: 1.4441581964492798\n",
            "Epoch 3, Step 21545, Loss: 1.885108470916748\n",
            "Epoch 3, Step 21546, Loss: 1.9852837324142456\n",
            "Epoch 3, Step 21547, Loss: 2.3518433570861816\n",
            "Epoch 3, Step 21548, Loss: 2.611586570739746\n",
            "Epoch 3, Step 21549, Loss: 1.775935411453247\n",
            "Epoch 3, Step 21550, Loss: 0.67472904920578\n",
            "Epoch 3, Step 21551, Loss: 2.326801300048828\n",
            "Epoch 3, Step 21552, Loss: 2.3342981338500977\n",
            "Epoch 3, Step 21553, Loss: 2.3960018157958984\n",
            "Epoch 3, Step 21554, Loss: 2.0255603790283203\n",
            "Epoch 3, Step 21555, Loss: 2.1221837997436523\n",
            "Epoch 3, Step 21556, Loss: 2.3185625076293945\n",
            "Epoch 3, Step 21557, Loss: 2.283578634262085\n",
            "Epoch 3, Step 21558, Loss: 1.7813246250152588\n",
            "Epoch 3, Step 21559, Loss: 0.5864376425743103\n",
            "Epoch 3, Step 21560, Loss: 0.6783424615859985\n",
            "Epoch 3, Step 21561, Loss: 1.4005963802337646\n",
            "Epoch 3, Step 21562, Loss: 1.5713955163955688\n",
            "Epoch 3, Step 21563, Loss: 0.6744127869606018\n",
            "Epoch 3, Step 21564, Loss: 1.6991692781448364\n",
            "Epoch 3, Step 21565, Loss: 1.4637866020202637\n",
            "Epoch 3, Step 21566, Loss: 1.8443689346313477\n",
            "Epoch 3, Step 21567, Loss: 2.694033145904541\n",
            "Epoch 3, Step 21568, Loss: 1.5752557516098022\n",
            "Epoch 3, Step 21569, Loss: 1.868337869644165\n",
            "Epoch 3, Step 21570, Loss: 1.7286626100540161\n",
            "Epoch 3, Step 21571, Loss: 1.5724759101867676\n",
            "Epoch 3, Step 21572, Loss: 2.1948838233947754\n",
            "Epoch 3, Step 21573, Loss: 2.2066664695739746\n",
            "Epoch 3, Step 21574, Loss: 2.1003997325897217\n",
            "Epoch 3, Step 21575, Loss: 1.2003505229949951\n",
            "Epoch 3, Step 21576, Loss: 2.2570927143096924\n",
            "Epoch 3, Step 21577, Loss: 1.1934031248092651\n",
            "Epoch 3, Step 21578, Loss: 2.2595040798187256\n",
            "Epoch 3, Step 21579, Loss: 1.9164308309555054\n",
            "Epoch 3, Step 21580, Loss: 1.7537894248962402\n",
            "Epoch 3, Step 21581, Loss: 1.3940013647079468\n",
            "Epoch 3, Step 21582, Loss: 1.7375248670578003\n",
            "Epoch 3, Step 21583, Loss: 2.5893537998199463\n",
            "Epoch 3, Step 21584, Loss: 1.4737058877944946\n",
            "Epoch 3, Step 21585, Loss: 0.8126999735832214\n",
            "Epoch 3, Step 21586, Loss: 1.8352138996124268\n",
            "Epoch 3, Step 21587, Loss: 1.9247610569000244\n",
            "Epoch 3, Step 21588, Loss: 1.7739735841751099\n",
            "Epoch 3, Step 21589, Loss: 1.3500864505767822\n",
            "Epoch 3, Step 21590, Loss: 1.5506153106689453\n",
            "Epoch 3, Step 21591, Loss: 1.6367629766464233\n",
            "Epoch 3, Step 21592, Loss: 1.4876811504364014\n",
            "Epoch 3, Step 21593, Loss: 1.7267894744873047\n",
            "Epoch 3, Step 21594, Loss: 1.9957996606826782\n",
            "Epoch 3, Step 21595, Loss: 1.9895673990249634\n",
            "Epoch 3, Step 21596, Loss: 2.53609037399292\n",
            "Epoch 3, Step 21597, Loss: 1.2153326272964478\n",
            "Epoch 3, Step 21598, Loss: 2.6431219577789307\n",
            "Epoch 3, Step 21599, Loss: 0.819797694683075\n",
            "Epoch 3, Step 21600, Loss: 2.366945505142212\n",
            "Epoch 3, Step 21601, Loss: 2.6906261444091797\n",
            "Epoch 3, Step 21602, Loss: 0.5710031986236572\n",
            "Epoch 3, Step 21603, Loss: 1.262163758277893\n",
            "Epoch 3, Step 21604, Loss: 1.8499667644500732\n",
            "Epoch 3, Step 21605, Loss: 1.7993876934051514\n",
            "Epoch 3, Step 21606, Loss: 1.8132320642471313\n",
            "Epoch 3, Step 21607, Loss: 2.376626491546631\n",
            "Epoch 3, Step 21608, Loss: 1.348660945892334\n",
            "Epoch 3, Step 21609, Loss: 0.8883923292160034\n",
            "Epoch 3, Step 21610, Loss: 1.3962513208389282\n",
            "Epoch 3, Step 21611, Loss: 1.4277033805847168\n",
            "Epoch 3, Step 21612, Loss: 1.3128877878189087\n",
            "Epoch 3, Step 21613, Loss: 2.1000118255615234\n",
            "Epoch 3, Step 21614, Loss: 0.5932029485702515\n",
            "Epoch 3, Step 21615, Loss: 1.329093337059021\n",
            "Epoch 3, Step 21616, Loss: 1.1291533708572388\n",
            "Epoch 3, Step 21617, Loss: 1.7163089513778687\n",
            "Epoch 3, Step 21618, Loss: 1.2237268686294556\n",
            "Epoch 3, Step 21619, Loss: 1.2621445655822754\n",
            "Epoch 3, Step 21620, Loss: 1.4852505922317505\n",
            "Epoch 3, Step 21621, Loss: 1.358605146408081\n",
            "Epoch 3, Step 21622, Loss: 1.582754373550415\n",
            "Epoch 3, Step 21623, Loss: 2.2655346393585205\n",
            "Epoch 3, Step 21624, Loss: 1.3591183423995972\n",
            "Epoch 3, Step 21625, Loss: 1.2618074417114258\n",
            "Epoch 3, Step 21626, Loss: 1.2491018772125244\n",
            "Epoch 3, Step 21627, Loss: 2.465200662612915\n",
            "Epoch 3, Step 21628, Loss: 1.976543664932251\n",
            "Epoch 3, Step 21629, Loss: 2.128588914871216\n",
            "Epoch 3, Step 21630, Loss: 2.1247060298919678\n",
            "Epoch 3, Step 21631, Loss: 1.3293083906173706\n",
            "Epoch 3, Step 21632, Loss: 2.3876702785491943\n",
            "Epoch 3, Step 21633, Loss: 1.3450452089309692\n",
            "Epoch 3, Step 21634, Loss: 1.5457851886749268\n",
            "Epoch 3, Step 21635, Loss: 1.6233361959457397\n",
            "Epoch 3, Step 21636, Loss: 0.5097395181655884\n",
            "Epoch 3, Step 21637, Loss: 1.4596391916275024\n",
            "Epoch 3, Step 21638, Loss: 0.644330620765686\n",
            "Epoch 3, Step 21639, Loss: 2.1345129013061523\n",
            "Epoch 3, Step 21640, Loss: 1.2339355945587158\n",
            "Epoch 3, Step 21641, Loss: 1.112479329109192\n",
            "Epoch 3, Step 21642, Loss: 1.4109898805618286\n",
            "Epoch 3, Step 21643, Loss: 1.483237385749817\n",
            "Epoch 3, Step 21644, Loss: 2.4423532485961914\n",
            "Epoch 3, Step 21645, Loss: 0.9422364234924316\n",
            "Epoch 3, Step 21646, Loss: 1.7129712104797363\n",
            "Epoch 3, Step 21647, Loss: 1.5314319133758545\n",
            "Epoch 3, Step 21648, Loss: 1.4776240587234497\n",
            "Epoch 3, Step 21649, Loss: 0.6135433316230774\n",
            "Epoch 3, Step 21650, Loss: 1.7129634618759155\n",
            "Epoch 3, Step 21651, Loss: 1.3984496593475342\n",
            "Epoch 3, Step 21652, Loss: 2.1591312885284424\n",
            "Epoch 3, Step 21653, Loss: 1.2223554849624634\n",
            "Epoch 3, Step 21654, Loss: 1.3030890226364136\n",
            "Epoch 3, Step 21655, Loss: 2.1304771900177\n",
            "Epoch 3, Step 21656, Loss: 1.3359192609786987\n",
            "Epoch 3, Step 21657, Loss: 2.8138248920440674\n",
            "Epoch 3, Step 21658, Loss: 2.7278153896331787\n",
            "Epoch 3, Step 21659, Loss: 0.7075484395027161\n",
            "Epoch 3, Step 21660, Loss: 2.4791319370269775\n",
            "Epoch 3, Step 21661, Loss: 1.8274980783462524\n",
            "Epoch 3, Step 21662, Loss: 2.2659833431243896\n",
            "Epoch 3, Step 21663, Loss: 1.9687913656234741\n",
            "Epoch 3, Step 21664, Loss: 0.7061972618103027\n",
            "Epoch 3, Step 21665, Loss: 1.6816027164459229\n",
            "Epoch 3, Step 21666, Loss: 2.723621368408203\n",
            "Epoch 3, Step 21667, Loss: 1.5780842304229736\n",
            "Epoch 3, Step 21668, Loss: 1.8978697061538696\n",
            "Epoch 3, Step 21669, Loss: 1.3224362134933472\n",
            "Epoch 3, Step 21670, Loss: 1.257102370262146\n",
            "Epoch 3, Step 21671, Loss: 2.1978259086608887\n",
            "Epoch 3, Step 21672, Loss: 1.6061025857925415\n",
            "Epoch 3, Step 21673, Loss: 1.2136982679367065\n",
            "Epoch 3, Step 21674, Loss: 1.3926193714141846\n",
            "Epoch 3, Step 21675, Loss: 1.7535289525985718\n",
            "Epoch 3, Step 21676, Loss: 2.20327091217041\n",
            "Epoch 3, Step 21677, Loss: 1.7601839303970337\n",
            "Epoch 3, Step 21678, Loss: 1.121410608291626\n",
            "Epoch 3, Step 21679, Loss: 1.9665820598602295\n",
            "Epoch 3, Step 21680, Loss: 1.8234236240386963\n",
            "Epoch 3, Step 21681, Loss: 2.5848348140716553\n",
            "Epoch 3, Step 21682, Loss: 1.7634105682373047\n",
            "Epoch 3, Step 21683, Loss: 0.5997288823127747\n",
            "Epoch 3, Step 21684, Loss: 1.6098241806030273\n",
            "Epoch 3, Step 21685, Loss: 1.833945631980896\n",
            "Epoch 3, Step 21686, Loss: 1.0249077081680298\n",
            "Epoch 3, Step 21687, Loss: 0.9928566813468933\n",
            "Epoch 3, Step 21688, Loss: 1.3618719577789307\n",
            "Epoch 3, Step 21689, Loss: 2.2083990573883057\n",
            "Epoch 3, Step 21690, Loss: 2.363569974899292\n",
            "Epoch 3, Step 21691, Loss: 2.212895631790161\n",
            "Epoch 3, Step 21692, Loss: 1.1669354438781738\n",
            "Epoch 3, Step 21693, Loss: 1.5682876110076904\n",
            "Epoch 3, Step 21694, Loss: 1.661040186882019\n",
            "Epoch 3, Step 21695, Loss: 0.8172107338905334\n",
            "Epoch 3, Step 21696, Loss: 2.065753936767578\n",
            "Epoch 3, Step 21697, Loss: 0.8787292242050171\n",
            "Epoch 3, Step 21698, Loss: 2.2226150035858154\n",
            "Epoch 3, Step 21699, Loss: 1.1677829027175903\n",
            "Epoch 3, Step 21700, Loss: 1.8999367952346802\n",
            "Epoch 3, Step 21701, Loss: 1.445712685585022\n",
            "Epoch 3, Step 21702, Loss: 1.2563191652297974\n",
            "Epoch 3, Step 21703, Loss: 2.6143651008605957\n",
            "Epoch 3, Step 21704, Loss: 2.5130341053009033\n",
            "Epoch 3, Step 21705, Loss: 2.7580244541168213\n",
            "Epoch 3, Step 21706, Loss: 1.3035526275634766\n",
            "Epoch 3, Step 21707, Loss: 1.5571486949920654\n",
            "Epoch 3, Step 21708, Loss: 1.812414526939392\n",
            "Epoch 3, Step 21709, Loss: 1.5646733045578003\n",
            "Epoch 3, Step 21710, Loss: 1.1111931800842285\n",
            "Epoch 3, Step 21711, Loss: 1.3997925519943237\n",
            "Epoch 3, Step 21712, Loss: 2.8643250465393066\n",
            "Epoch 3, Step 21713, Loss: 1.2879667282104492\n",
            "Epoch 3, Step 21714, Loss: 1.1897131204605103\n",
            "Epoch 3, Step 21715, Loss: 0.965485692024231\n",
            "Epoch 3, Step 21716, Loss: 1.0140116214752197\n",
            "Epoch 3, Step 21717, Loss: 1.6781069040298462\n",
            "Epoch 3, Step 21718, Loss: 1.0931109189987183\n",
            "Epoch 3, Step 21719, Loss: 1.2758342027664185\n",
            "Epoch 3, Step 21720, Loss: 2.8071749210357666\n",
            "Epoch 3, Step 21721, Loss: 1.5829746723175049\n",
            "Epoch 3, Step 21722, Loss: 1.1769756078720093\n",
            "Epoch 3, Step 21723, Loss: 1.9960094690322876\n",
            "Epoch 3, Step 21724, Loss: 1.3924267292022705\n",
            "Epoch 3, Step 21725, Loss: 2.0829834938049316\n",
            "Epoch 3, Step 21726, Loss: 2.2184903621673584\n",
            "Epoch 3, Step 21727, Loss: 0.7679522633552551\n",
            "Epoch 3, Step 21728, Loss: 2.2915220260620117\n",
            "Epoch 3, Step 21729, Loss: 1.9752496480941772\n",
            "Epoch 3, Step 21730, Loss: 2.5487804412841797\n",
            "Epoch 3, Step 21731, Loss: 1.7663557529449463\n",
            "Epoch 3, Step 21732, Loss: 1.9377567768096924\n",
            "Epoch 3, Step 21733, Loss: 1.4457584619522095\n",
            "Epoch 3, Step 21734, Loss: 1.9285391569137573\n",
            "Epoch 3, Step 21735, Loss: 1.327399730682373\n",
            "Epoch 3, Step 21736, Loss: 1.5860686302185059\n",
            "Epoch 3, Step 21737, Loss: 1.8067394495010376\n",
            "Epoch 3, Step 21738, Loss: 2.0839693546295166\n",
            "Epoch 3, Step 21739, Loss: 0.7273791432380676\n",
            "Epoch 3, Step 21740, Loss: 2.5358567237854004\n",
            "Epoch 3, Step 21741, Loss: 2.1446173191070557\n",
            "Epoch 3, Step 21742, Loss: 1.5131703615188599\n",
            "Epoch 3, Step 21743, Loss: 1.3771278858184814\n",
            "Epoch 3, Step 21744, Loss: 1.5269633531570435\n",
            "Epoch 3, Step 21745, Loss: 1.0179476737976074\n",
            "Epoch 3, Step 21746, Loss: 1.7666997909545898\n",
            "Epoch 3, Step 21747, Loss: 1.8194886445999146\n",
            "Epoch 3, Step 21748, Loss: 0.9957005381584167\n",
            "Epoch 3, Step 21749, Loss: 2.3417441844940186\n",
            "Epoch 3, Step 21750, Loss: 1.5520256757736206\n",
            "Epoch 3, Step 21751, Loss: 0.8920115828514099\n",
            "Epoch 3, Step 21752, Loss: 1.7328574657440186\n",
            "Epoch 3, Step 21753, Loss: 2.2491207122802734\n",
            "Epoch 3, Step 21754, Loss: 1.6535224914550781\n",
            "Epoch 3, Step 21755, Loss: 1.4152096509933472\n",
            "Epoch 3, Step 21756, Loss: 1.5302711725234985\n",
            "Epoch 3, Step 21757, Loss: 1.5218117237091064\n",
            "Epoch 3, Step 21758, Loss: 0.9062113761901855\n",
            "Epoch 3, Step 21759, Loss: 1.403933048248291\n",
            "Epoch 3, Step 21760, Loss: 2.460642099380493\n",
            "Epoch 3, Step 21761, Loss: 0.6167051196098328\n",
            "Epoch 3, Step 21762, Loss: 0.8298140168190002\n",
            "Epoch 3, Step 21763, Loss: 1.8858637809753418\n",
            "Epoch 3, Step 21764, Loss: 1.192115306854248\n",
            "Epoch 3, Step 21765, Loss: 1.637000560760498\n",
            "Epoch 3, Step 21766, Loss: 1.1984944343566895\n",
            "Epoch 3, Step 21767, Loss: 1.7075153589248657\n",
            "Epoch 3, Step 21768, Loss: 0.973034679889679\n",
            "Epoch 3, Step 21769, Loss: 1.5191538333892822\n",
            "Epoch 3, Step 21770, Loss: 0.8142793774604797\n",
            "Epoch 3, Step 21771, Loss: 2.756213903427124\n",
            "Epoch 3, Step 21772, Loss: 1.527511715888977\n",
            "Epoch 3, Step 21773, Loss: 0.7402785420417786\n",
            "Epoch 3, Step 21774, Loss: 0.8265348076820374\n",
            "Epoch 3, Step 21775, Loss: 1.9255558252334595\n",
            "Epoch 3, Step 21776, Loss: 0.8366835117340088\n",
            "Epoch 3, Step 21777, Loss: 0.8956959843635559\n",
            "Epoch 3, Step 21778, Loss: 0.7775527834892273\n",
            "Epoch 3, Step 21779, Loss: 0.6387431621551514\n",
            "Epoch 3, Step 21780, Loss: 1.0590323209762573\n",
            "Epoch 3, Step 21781, Loss: 2.459205150604248\n",
            "Epoch 3, Step 21782, Loss: 2.545520305633545\n",
            "Epoch 3, Step 21783, Loss: 2.3812358379364014\n",
            "Epoch 3, Step 21784, Loss: 1.8411552906036377\n",
            "Epoch 3, Step 21785, Loss: 0.8335850238800049\n",
            "Epoch 3, Step 21786, Loss: 1.3015609979629517\n",
            "Epoch 3, Step 21787, Loss: 1.6473218202590942\n",
            "Epoch 3, Step 21788, Loss: 1.2750792503356934\n",
            "Epoch 3, Step 21789, Loss: 2.1049931049346924\n",
            "Epoch 3, Step 21790, Loss: 0.7630729079246521\n",
            "Epoch 3, Step 21791, Loss: 2.1777548789978027\n",
            "Epoch 3, Step 21792, Loss: 2.417416572570801\n",
            "Epoch 3, Step 21793, Loss: 1.5560460090637207\n",
            "Epoch 3, Step 21794, Loss: 1.5672610998153687\n",
            "Epoch 3, Step 21795, Loss: 2.770622968673706\n",
            "Epoch 3, Step 21796, Loss: 1.5733641386032104\n",
            "Epoch 3, Step 21797, Loss: 0.8503944873809814\n",
            "Epoch 3, Step 21798, Loss: 1.0574601888656616\n",
            "Epoch 3, Step 21799, Loss: 1.4991792440414429\n",
            "Epoch 3, Step 21800, Loss: 1.9577465057373047\n",
            "Epoch 3, Step 21801, Loss: 1.8784232139587402\n",
            "Epoch 3, Step 21802, Loss: 1.092810869216919\n",
            "Epoch 3, Step 21803, Loss: 1.744580626487732\n",
            "Epoch 3, Step 21804, Loss: 1.803435206413269\n",
            "Epoch 3, Step 21805, Loss: 0.9707100987434387\n",
            "Epoch 3, Step 21806, Loss: 1.8568480014801025\n",
            "Epoch 3, Step 21807, Loss: 1.1954655647277832\n",
            "Epoch 3, Step 21808, Loss: 0.9645074009895325\n",
            "Epoch 3, Step 21809, Loss: 2.498305559158325\n",
            "Epoch 3, Step 21810, Loss: 1.7198610305786133\n",
            "Epoch 3, Step 21811, Loss: 1.8252021074295044\n",
            "Epoch 3, Step 21812, Loss: 2.1963701248168945\n",
            "Epoch 3, Step 21813, Loss: 2.2549333572387695\n",
            "Epoch 3, Step 21814, Loss: 1.5669589042663574\n",
            "Epoch 3, Step 21815, Loss: 2.295400381088257\n",
            "Epoch 3, Step 21816, Loss: 1.637615442276001\n",
            "Epoch 3, Step 21817, Loss: 1.558984637260437\n",
            "Epoch 3, Step 21818, Loss: 1.2759827375411987\n",
            "Epoch 3, Step 21819, Loss: 1.223486065864563\n",
            "Epoch 3, Step 21820, Loss: 1.5033607482910156\n",
            "Epoch 3, Step 21821, Loss: 1.698486566543579\n",
            "Epoch 3, Step 21822, Loss: 1.776344895362854\n",
            "Epoch 3, Step 21823, Loss: 1.6581758260726929\n",
            "Epoch 3, Step 21824, Loss: 2.5165791511535645\n",
            "Epoch 3, Step 21825, Loss: 1.1189310550689697\n",
            "Epoch 3, Step 21826, Loss: 1.254101037979126\n",
            "Epoch 3, Step 21827, Loss: 1.306278109550476\n",
            "Epoch 3, Step 21828, Loss: 0.8773641586303711\n",
            "Epoch 3, Step 21829, Loss: 1.8845208883285522\n",
            "Epoch 3, Step 21830, Loss: 1.1494622230529785\n",
            "Epoch 3, Step 21831, Loss: 2.1176159381866455\n",
            "Epoch 3, Step 21832, Loss: 1.494689702987671\n",
            "Epoch 3, Step 21833, Loss: 1.0116209983825684\n",
            "Epoch 3, Step 21834, Loss: 1.2071815729141235\n",
            "Epoch 3, Step 21835, Loss: 1.696645736694336\n",
            "Epoch 3, Step 21836, Loss: 0.9109861254692078\n",
            "Epoch 3, Step 21837, Loss: 1.6891895532608032\n",
            "Epoch 3, Step 21838, Loss: 2.2444043159484863\n",
            "Epoch 3, Step 21839, Loss: 2.053215742111206\n",
            "Epoch 3, Step 21840, Loss: 1.8646953105926514\n",
            "Epoch 3, Step 21841, Loss: 1.292341947555542\n",
            "Epoch 3, Step 21842, Loss: 1.762769341468811\n",
            "Epoch 3, Step 21843, Loss: 1.68465256690979\n",
            "Epoch 3, Step 21844, Loss: 1.9450923204421997\n",
            "Epoch 3, Step 21845, Loss: 1.825202226638794\n",
            "Epoch 3, Step 21846, Loss: 1.5120967626571655\n",
            "Epoch 3, Step 21847, Loss: 2.178793430328369\n",
            "Epoch 3, Step 21848, Loss: 0.45704835653305054\n",
            "Epoch 3, Step 21849, Loss: 1.7086045742034912\n",
            "Epoch 3, Step 21850, Loss: 2.1361241340637207\n",
            "Epoch 3, Step 21851, Loss: 2.5591206550598145\n",
            "Epoch 3, Step 21852, Loss: 1.2571935653686523\n",
            "Epoch 3, Step 21853, Loss: 1.745462417602539\n",
            "Epoch 3, Step 21854, Loss: 0.6857467293739319\n",
            "Epoch 3, Step 21855, Loss: 0.9661490321159363\n",
            "Epoch 3, Step 21856, Loss: 1.235366940498352\n",
            "Epoch 3, Step 21857, Loss: 0.5147103071212769\n",
            "Epoch 3, Step 21858, Loss: 1.3646241426467896\n",
            "Epoch 3, Step 21859, Loss: 2.119565963745117\n",
            "Epoch 3, Step 21860, Loss: 2.2338194847106934\n",
            "Epoch 3, Step 21861, Loss: 1.3496736288070679\n",
            "Epoch 3, Step 21862, Loss: 1.0464123487472534\n",
            "Epoch 3, Step 21863, Loss: 1.1997770071029663\n",
            "Epoch 3, Step 21864, Loss: 1.6995211839675903\n",
            "Epoch 3, Step 21865, Loss: 1.716288447380066\n",
            "Epoch 3, Step 21866, Loss: 1.241189956665039\n",
            "Epoch 3, Step 21867, Loss: 1.6932553052902222\n",
            "Epoch 3, Step 21868, Loss: 2.1847827434539795\n",
            "Epoch 3, Step 21869, Loss: 2.1750235557556152\n",
            "Epoch 3, Step 21870, Loss: 0.8015993237495422\n",
            "Epoch 3, Step 21871, Loss: 1.967885136604309\n",
            "Epoch 3, Step 21872, Loss: 1.0298808813095093\n",
            "Epoch 3, Step 21873, Loss: 1.9224125146865845\n",
            "Epoch 3, Step 21874, Loss: 1.5562047958374023\n",
            "Epoch 3, Step 21875, Loss: 1.4038532972335815\n",
            "Epoch 3, Step 21876, Loss: 1.7539234161376953\n",
            "Epoch 3, Step 21877, Loss: 1.4512169361114502\n",
            "Epoch 3, Step 21878, Loss: 2.612542152404785\n",
            "Epoch 3, Step 21879, Loss: 2.3534932136535645\n",
            "Epoch 3, Step 21880, Loss: 2.351473808288574\n",
            "Epoch 3, Step 21881, Loss: 0.4643412232398987\n",
            "Epoch 3, Step 21882, Loss: 1.5667930841445923\n",
            "Epoch 3, Step 21883, Loss: 0.8830394744873047\n",
            "Epoch 3, Step 21884, Loss: 0.7829496264457703\n",
            "Epoch 3, Step 21885, Loss: 2.0271782875061035\n",
            "Epoch 3, Step 21886, Loss: 2.183919668197632\n",
            "Epoch 3, Step 21887, Loss: 0.7823212742805481\n",
            "Epoch 3, Step 21888, Loss: 0.8551940321922302\n",
            "Epoch 3, Step 21889, Loss: 0.8166964054107666\n",
            "Epoch 3, Step 21890, Loss: 1.6711697578430176\n",
            "Epoch 3, Step 21891, Loss: 1.840898871421814\n",
            "Epoch 3, Step 21892, Loss: 1.5938537120819092\n",
            "Epoch 3, Step 21893, Loss: 2.631378412246704\n",
            "Epoch 3, Step 21894, Loss: 1.7518707513809204\n",
            "Epoch 3, Step 21895, Loss: 0.8696000576019287\n",
            "Epoch 3, Step 21896, Loss: 2.127647876739502\n",
            "Epoch 3, Step 21897, Loss: 2.429750680923462\n",
            "Epoch 3, Step 21898, Loss: 1.3985157012939453\n",
            "Epoch 3, Step 21899, Loss: 1.8522529602050781\n",
            "Epoch 3, Step 21900, Loss: 3.2893869876861572\n",
            "Epoch 3, Step 21901, Loss: 1.1694135665893555\n",
            "Epoch 3, Step 21902, Loss: 1.6354444026947021\n",
            "Epoch 3, Step 21903, Loss: 2.4421627521514893\n",
            "Epoch 3, Step 21904, Loss: 1.7571041584014893\n",
            "Epoch 3, Step 21905, Loss: 1.7618792057037354\n",
            "Epoch 3, Step 21906, Loss: 1.7113139629364014\n",
            "Epoch 3, Step 21907, Loss: 2.0425758361816406\n",
            "Epoch 3, Step 21908, Loss: 1.8336081504821777\n",
            "Epoch 3, Step 21909, Loss: 1.5076203346252441\n",
            "Epoch 3, Step 21910, Loss: 1.4215102195739746\n",
            "Epoch 3, Step 21911, Loss: 1.445756196975708\n",
            "Epoch 3, Step 21912, Loss: 1.2045843601226807\n",
            "Epoch 3, Step 21913, Loss: 1.7158594131469727\n",
            "Epoch 3, Step 21914, Loss: 1.9795678853988647\n",
            "Epoch 3, Step 21915, Loss: 0.8963245153427124\n",
            "Epoch 3, Step 21916, Loss: 1.7597557306289673\n",
            "Epoch 3, Step 21917, Loss: 0.6324519515037537\n",
            "Epoch 3, Step 21918, Loss: 0.7242567539215088\n",
            "Epoch 3, Step 21919, Loss: 1.7585092782974243\n",
            "Epoch 3, Step 21920, Loss: 1.817796230316162\n",
            "Epoch 3, Step 21921, Loss: 2.487983465194702\n",
            "Epoch 3, Step 21922, Loss: 1.8073903322219849\n",
            "Epoch 3, Step 21923, Loss: 2.1120729446411133\n",
            "Epoch 3, Step 21924, Loss: 1.0753016471862793\n",
            "Epoch 3, Step 21925, Loss: 0.39962059259414673\n",
            "Epoch 3, Step 21926, Loss: 1.5770268440246582\n",
            "Epoch 3, Step 21927, Loss: 1.5715075731277466\n",
            "Epoch 3, Step 21928, Loss: 1.9073710441589355\n",
            "Epoch 3, Step 21929, Loss: 0.5719038844108582\n",
            "Epoch 3, Step 21930, Loss: 2.3676435947418213\n",
            "Epoch 3, Step 21931, Loss: 1.3919956684112549\n",
            "Epoch 3, Step 21932, Loss: 0.6268166899681091\n",
            "Epoch 3, Step 21933, Loss: 1.296213984489441\n",
            "Epoch 3, Step 21934, Loss: 2.0703835487365723\n",
            "Epoch 3, Step 21935, Loss: 0.6932810544967651\n",
            "Epoch 3, Step 21936, Loss: 1.558449625968933\n",
            "Epoch 3, Step 21937, Loss: 0.9527403116226196\n",
            "Epoch 3, Step 21938, Loss: 0.33897149562835693\n",
            "Epoch 3, Step 21939, Loss: 2.186314344406128\n",
            "Epoch 3, Step 21940, Loss: 1.263993263244629\n",
            "Epoch 3, Step 21941, Loss: 1.1750258207321167\n",
            "Epoch 3, Step 21942, Loss: 2.0235493183135986\n",
            "Epoch 3, Step 21943, Loss: 0.43229880928993225\n",
            "Epoch 3, Step 21944, Loss: 0.9427703022956848\n",
            "Epoch 3, Step 21945, Loss: 2.451979875564575\n",
            "Epoch 3, Step 21946, Loss: 0.9548630714416504\n",
            "Epoch 3, Step 21947, Loss: 1.2211358547210693\n",
            "Epoch 3, Step 21948, Loss: 1.9787697792053223\n",
            "Epoch 3, Step 21949, Loss: 1.4039204120635986\n",
            "Epoch 3, Step 21950, Loss: 1.1724576950073242\n",
            "Epoch 3, Step 21951, Loss: 2.114075183868408\n",
            "Epoch 3, Step 21952, Loss: 2.072011709213257\n",
            "Epoch 3, Step 21953, Loss: 2.551116704940796\n",
            "Epoch 3, Step 21954, Loss: 1.3918496370315552\n",
            "Epoch 3, Step 21955, Loss: 1.5154145956039429\n",
            "Epoch 3, Step 21956, Loss: 1.3830891847610474\n",
            "Epoch 3, Step 21957, Loss: 2.5370664596557617\n",
            "Epoch 3, Step 21958, Loss: 1.930301308631897\n",
            "Epoch 3, Step 21959, Loss: 2.1446421146392822\n",
            "Epoch 3, Step 21960, Loss: 2.473978042602539\n",
            "Epoch 3, Step 21961, Loss: 0.5288944244384766\n",
            "Epoch 3, Step 21962, Loss: 1.9270389080047607\n",
            "Epoch 3, Step 21963, Loss: 0.7283431887626648\n",
            "Epoch 3, Step 21964, Loss: 0.6185585260391235\n",
            "Epoch 3, Step 21965, Loss: 1.8394620418548584\n",
            "Epoch 3, Step 21966, Loss: 1.8490326404571533\n",
            "Epoch 3, Step 21967, Loss: 0.5335922241210938\n",
            "Epoch 3, Step 21968, Loss: 1.9574618339538574\n",
            "Epoch 3, Step 21969, Loss: 1.9896560907363892\n",
            "Epoch 3, Step 21970, Loss: 1.275206208229065\n",
            "Epoch 3, Step 21971, Loss: 2.1608059406280518\n",
            "Epoch 3, Step 21972, Loss: 2.016404628753662\n",
            "Epoch 3, Step 21973, Loss: 1.617898941040039\n",
            "Epoch 3, Step 21974, Loss: 1.2408726215362549\n",
            "Epoch 3, Step 21975, Loss: 1.5987190008163452\n",
            "Epoch 3, Step 21976, Loss: 2.0868303775787354\n",
            "Epoch 3, Step 21977, Loss: 1.5318241119384766\n",
            "Epoch 3, Step 21978, Loss: 1.6237174272537231\n",
            "Epoch 3, Step 21979, Loss: 1.7585701942443848\n",
            "Epoch 3, Step 21980, Loss: 1.0391740798950195\n",
            "Epoch 3, Step 21981, Loss: 1.1662036180496216\n",
            "Epoch 3, Step 21982, Loss: 1.5376522541046143\n",
            "Epoch 3, Step 21983, Loss: 1.5219130516052246\n",
            "Epoch 3, Step 21984, Loss: 2.2749624252319336\n",
            "Epoch 3, Step 21985, Loss: 0.5562794208526611\n",
            "Epoch 3, Step 21986, Loss: 2.177379846572876\n",
            "Epoch 3, Step 21987, Loss: 0.8888882994651794\n",
            "Epoch 3, Step 21988, Loss: 1.5455387830734253\n",
            "Epoch 3, Step 21989, Loss: 0.9340450763702393\n",
            "Epoch 3, Step 21990, Loss: 1.117510199546814\n",
            "Epoch 3, Step 21991, Loss: 1.7038902044296265\n",
            "Epoch 3, Step 21992, Loss: 0.7219895720481873\n",
            "Epoch 3, Step 21993, Loss: 0.9728602766990662\n",
            "Epoch 3, Step 21994, Loss: 2.1545469760894775\n",
            "Epoch 3, Step 21995, Loss: 1.64406156539917\n",
            "Epoch 3, Step 21996, Loss: 1.8942219018936157\n",
            "Epoch 3, Step 21997, Loss: 1.9120581150054932\n",
            "Epoch 3, Step 21998, Loss: 1.2643532752990723\n",
            "Epoch 3, Step 21999, Loss: 1.2218198776245117\n",
            "Epoch 3, Step 22000, Loss: 1.928662896156311\n",
            "Epoch 3, Step 22001, Loss: 0.5933234691619873\n",
            "Epoch 3, Step 22002, Loss: 1.592515468597412\n",
            "Epoch 3, Step 22003, Loss: 1.5396497249603271\n",
            "Epoch 3, Step 22004, Loss: 1.0792509317398071\n",
            "Epoch 3, Step 22005, Loss: 1.85165536403656\n",
            "Epoch 3, Step 22006, Loss: 1.282106637954712\n",
            "Epoch 3, Step 22007, Loss: 2.045018196105957\n",
            "Epoch 3, Step 22008, Loss: 1.3425263166427612\n",
            "Epoch 3, Step 22009, Loss: 1.9585133790969849\n",
            "Epoch 3, Step 22010, Loss: 1.0450537204742432\n",
            "Epoch 3, Step 22011, Loss: 2.1886019706726074\n",
            "Epoch 3, Step 22012, Loss: 1.1981596946716309\n",
            "Epoch 3, Step 22013, Loss: 1.8487070798873901\n",
            "Epoch 3, Step 22014, Loss: 1.1378642320632935\n",
            "Epoch 3, Step 22015, Loss: 2.1249916553497314\n",
            "Epoch 3, Step 22016, Loss: 1.5624011754989624\n",
            "Epoch 3, Step 22017, Loss: 1.336366057395935\n",
            "Epoch 3, Step 22018, Loss: 1.6035767793655396\n",
            "Epoch 3, Step 22019, Loss: 0.6753199696540833\n",
            "Epoch 3, Step 22020, Loss: 2.830782651901245\n",
            "Epoch 3, Step 22021, Loss: 1.6158512830734253\n",
            "Epoch 3, Step 22022, Loss: 1.201552391052246\n",
            "Epoch 3, Step 22023, Loss: 1.9273040294647217\n",
            "Epoch 3, Step 22024, Loss: 0.8060333728790283\n",
            "Epoch 3, Step 22025, Loss: 1.3384722471237183\n",
            "Epoch 3, Step 22026, Loss: 1.5984505414962769\n",
            "Epoch 3, Step 22027, Loss: 0.44786709547042847\n",
            "Epoch 3, Step 22028, Loss: 1.9374996423721313\n",
            "Epoch 3, Step 22029, Loss: 1.93968665599823\n",
            "Epoch 3, Step 22030, Loss: 1.2300816774368286\n",
            "Epoch 3, Step 22031, Loss: 0.8479665517807007\n",
            "Epoch 3, Step 22032, Loss: 0.4813329875469208\n",
            "Epoch 3, Step 22033, Loss: 1.9024019241333008\n",
            "Epoch 3, Step 22034, Loss: 1.5439777374267578\n",
            "Epoch 3, Step 22035, Loss: 1.840003252029419\n",
            "Epoch 3, Step 22036, Loss: 1.0710911750793457\n",
            "Epoch 3, Step 22037, Loss: 2.0604641437530518\n",
            "Epoch 3, Step 22038, Loss: 1.6347509622573853\n",
            "Epoch 3, Step 22039, Loss: 1.481848955154419\n",
            "Epoch 3, Step 22040, Loss: 1.7998788356781006\n",
            "Epoch 3, Step 22041, Loss: 1.0529348850250244\n",
            "Epoch 3, Step 22042, Loss: 1.56598961353302\n",
            "Epoch 3, Step 22043, Loss: 0.9456982612609863\n",
            "Epoch 3, Step 22044, Loss: 1.8849197626113892\n",
            "Epoch 3, Step 22045, Loss: 2.4005286693573\n",
            "Epoch 3, Step 22046, Loss: 1.8039244413375854\n",
            "Epoch 3, Step 22047, Loss: 1.4764189720153809\n",
            "Epoch 3, Step 22048, Loss: 1.4724336862564087\n",
            "Epoch 3, Step 22049, Loss: 1.4819966554641724\n",
            "Epoch 3, Step 22050, Loss: 1.2509413957595825\n",
            "Epoch 3, Step 22051, Loss: 0.795878529548645\n",
            "Epoch 3, Step 22052, Loss: 1.311231255531311\n",
            "Epoch 3, Step 22053, Loss: 0.5393025279045105\n",
            "Epoch 3, Step 22054, Loss: 1.160374641418457\n",
            "Epoch 3, Step 22055, Loss: 2.1385514736175537\n",
            "Epoch 3, Step 22056, Loss: 1.8648391962051392\n",
            "Epoch 3, Step 22057, Loss: 0.7733852863311768\n",
            "Epoch 3, Step 22058, Loss: 1.1778289079666138\n",
            "Epoch 3, Step 22059, Loss: 1.4250319004058838\n",
            "Epoch 3, Step 22060, Loss: 1.8156707286834717\n",
            "Epoch 3, Step 22061, Loss: 1.3951369524002075\n",
            "Epoch 3, Step 22062, Loss: 0.6283982992172241\n",
            "Epoch 3, Step 22063, Loss: 1.9368374347686768\n",
            "Epoch 3, Step 22064, Loss: 1.539008378982544\n",
            "Epoch 3, Step 22065, Loss: 1.2646278142929077\n",
            "Epoch 3, Step 22066, Loss: 2.3758091926574707\n",
            "Epoch 3, Step 22067, Loss: 1.4572856426239014\n",
            "Epoch 3, Step 22068, Loss: 2.5626096725463867\n",
            "Epoch 3, Step 22069, Loss: 1.700647234916687\n",
            "Epoch 3, Step 22070, Loss: 1.36884343624115\n",
            "Epoch 3, Step 22071, Loss: 0.8469114303588867\n",
            "Epoch 3, Step 22072, Loss: 0.6842264533042908\n",
            "Epoch 3, Step 22073, Loss: 1.3827474117279053\n",
            "Epoch 3, Step 22074, Loss: 2.0397515296936035\n",
            "Epoch 3, Step 22075, Loss: 1.0290380716323853\n",
            "Epoch 3, Step 22076, Loss: 1.1247776746749878\n",
            "Epoch 3, Step 22077, Loss: 1.807698369026184\n",
            "Epoch 3, Step 22078, Loss: 1.468050241470337\n",
            "Epoch 3, Step 22079, Loss: 1.6798161268234253\n",
            "Epoch 3, Step 22080, Loss: 1.0959179401397705\n",
            "Epoch 3, Step 22081, Loss: 1.2616801261901855\n",
            "Epoch 3, Step 22082, Loss: 1.216308832168579\n",
            "Epoch 3, Step 22083, Loss: 1.1531237363815308\n",
            "Epoch 3, Step 22084, Loss: 0.8806783556938171\n",
            "Epoch 3, Step 22085, Loss: 1.0337494611740112\n",
            "Epoch 3, Step 22086, Loss: 2.135913133621216\n",
            "Epoch 3, Step 22087, Loss: 1.7982817888259888\n",
            "Epoch 3, Step 22088, Loss: 0.48655372858047485\n",
            "Epoch 3, Step 22089, Loss: 1.0685677528381348\n",
            "Epoch 3, Step 22090, Loss: 1.7866495847702026\n",
            "Epoch 3, Step 22091, Loss: 1.8791271448135376\n",
            "Epoch 3, Step 22092, Loss: 1.2879340648651123\n",
            "Epoch 3, Step 22093, Loss: 1.3206924200057983\n",
            "Epoch 3, Step 22094, Loss: 1.304457664489746\n",
            "Epoch 3, Step 22095, Loss: 0.923816442489624\n",
            "Epoch 3, Step 22096, Loss: 1.4212857484817505\n",
            "Epoch 3, Step 22097, Loss: 1.636785626411438\n",
            "Epoch 3, Step 22098, Loss: 1.6432892084121704\n",
            "Epoch 3, Step 22099, Loss: 1.9386476278305054\n",
            "Epoch 3, Step 22100, Loss: 2.000732421875\n",
            "Epoch 3, Step 22101, Loss: 2.2819020748138428\n",
            "Epoch 3, Step 22102, Loss: 2.356940507888794\n",
            "Epoch 3, Step 22103, Loss: 2.099278688430786\n",
            "Epoch 3, Step 22104, Loss: 1.6903667449951172\n",
            "Epoch 3, Step 22105, Loss: 2.190946102142334\n",
            "Epoch 3, Step 22106, Loss: 1.0650588274002075\n",
            "Epoch 3, Step 22107, Loss: 1.637864112854004\n",
            "Epoch 3, Step 22108, Loss: 0.4878963530063629\n",
            "Epoch 3, Step 22109, Loss: 2.215627431869507\n",
            "Epoch 3, Step 22110, Loss: 1.5752265453338623\n",
            "Epoch 3, Step 22111, Loss: 1.2231943607330322\n",
            "Epoch 3, Step 22112, Loss: 0.6387581825256348\n",
            "Epoch 3, Step 22113, Loss: 1.5361820459365845\n",
            "Epoch 3, Step 22114, Loss: 2.1739141941070557\n",
            "Epoch 3, Step 22115, Loss: 1.4250067472457886\n",
            "Epoch 3, Step 22116, Loss: 1.3617017269134521\n",
            "Epoch 3, Step 22117, Loss: 1.6207292079925537\n",
            "Epoch 3, Step 22118, Loss: 1.5018430948257446\n",
            "Epoch 3, Step 22119, Loss: 1.1221802234649658\n",
            "Epoch 3, Step 22120, Loss: 1.1335891485214233\n",
            "Epoch 3, Step 22121, Loss: 2.2207980155944824\n",
            "Epoch 3, Step 22122, Loss: 1.2521964311599731\n",
            "Epoch 3, Step 22123, Loss: 1.1779688596725464\n",
            "Epoch 3, Step 22124, Loss: 1.3089779615402222\n",
            "Epoch 3, Step 22125, Loss: 2.4759466648101807\n",
            "Epoch 3, Step 22126, Loss: 1.6687039136886597\n",
            "Epoch 3, Step 22127, Loss: 1.6164764165878296\n",
            "Epoch 3, Step 22128, Loss: 1.721554160118103\n",
            "Epoch 3, Step 22129, Loss: 0.5884186625480652\n",
            "Epoch 3, Step 22130, Loss: 1.5479826927185059\n",
            "Epoch 3, Step 22131, Loss: 2.3168740272521973\n",
            "Epoch 3, Step 22132, Loss: 1.2411690950393677\n",
            "Epoch 3, Step 22133, Loss: 2.5030200481414795\n",
            "Epoch 3, Step 22134, Loss: 1.6611318588256836\n",
            "Epoch 3, Step 22135, Loss: 1.389801025390625\n",
            "Epoch 3, Step 22136, Loss: 0.7201352715492249\n",
            "Epoch 3, Step 22137, Loss: 1.9363319873809814\n",
            "Epoch 3, Step 22138, Loss: 1.769201636314392\n",
            "Epoch 3, Step 22139, Loss: 1.0821880102157593\n",
            "Epoch 3, Step 22140, Loss: 0.7627032995223999\n",
            "Epoch 3, Step 22141, Loss: 0.5604121685028076\n",
            "Epoch 3, Step 22142, Loss: 1.9052677154541016\n",
            "Epoch 3, Step 22143, Loss: 2.032924175262451\n",
            "Epoch 3, Step 22144, Loss: 0.6142032742500305\n",
            "Epoch 3, Step 22145, Loss: 1.4077922105789185\n",
            "Epoch 3, Step 22146, Loss: 2.1736650466918945\n",
            "Epoch 3, Step 22147, Loss: 0.951529860496521\n",
            "Epoch 3, Step 22148, Loss: 1.721030592918396\n",
            "Epoch 3, Step 22149, Loss: 1.5913500785827637\n",
            "Epoch 3, Step 22150, Loss: 1.990737795829773\n",
            "Epoch 3, Step 22151, Loss: 0.7625215649604797\n",
            "Epoch 3, Step 22152, Loss: 2.0409328937530518\n",
            "Epoch 3, Step 22153, Loss: 1.2309213876724243\n",
            "Epoch 3, Step 22154, Loss: 0.7051056027412415\n",
            "Epoch 3, Step 22155, Loss: 1.4901609420776367\n",
            "Epoch 3, Step 22156, Loss: 1.1374258995056152\n",
            "Epoch 3, Step 22157, Loss: 2.410006523132324\n",
            "Epoch 3, Step 22158, Loss: 1.3777415752410889\n",
            "Epoch 3, Step 22159, Loss: 1.5802019834518433\n",
            "Epoch 3, Step 22160, Loss: 1.4132161140441895\n",
            "Epoch 3, Step 22161, Loss: 1.2347004413604736\n",
            "Epoch 3, Step 22162, Loss: 0.4702630341053009\n",
            "Epoch 3, Step 22163, Loss: 0.733832061290741\n",
            "Epoch 3, Step 22164, Loss: 0.9950262308120728\n",
            "Epoch 3, Step 22165, Loss: 0.7256870269775391\n",
            "Epoch 3, Step 22166, Loss: 1.3601263761520386\n",
            "Epoch 3, Step 22167, Loss: 1.7125496864318848\n",
            "Epoch 3, Step 22168, Loss: 1.4484294652938843\n",
            "Epoch 3, Step 22169, Loss: 0.8621416687965393\n",
            "Epoch 3, Step 22170, Loss: 2.0854408740997314\n",
            "Epoch 3, Step 22171, Loss: 0.822195827960968\n",
            "Epoch 3, Step 22172, Loss: 1.3320131301879883\n",
            "Epoch 3, Step 22173, Loss: 1.9283074140548706\n",
            "Epoch 3, Step 22174, Loss: 1.061832308769226\n",
            "Epoch 3, Step 22175, Loss: 1.4078404903411865\n",
            "Epoch 3, Step 22176, Loss: 2.2885735034942627\n",
            "Epoch 3, Step 22177, Loss: 1.9454742670059204\n",
            "Epoch 3, Step 22178, Loss: 1.8557716608047485\n",
            "Epoch 3, Step 22179, Loss: 1.8219835758209229\n",
            "Epoch 3, Step 22180, Loss: 1.7626951932907104\n",
            "Epoch 3, Step 22181, Loss: 1.7341896295547485\n",
            "Epoch 3, Step 22182, Loss: 1.2237768173217773\n",
            "Epoch 3, Step 22183, Loss: 2.196579694747925\n",
            "Epoch 3, Step 22184, Loss: 2.1428871154785156\n",
            "Epoch 3, Step 22185, Loss: 1.7366176843643188\n",
            "Epoch 3, Step 22186, Loss: 0.8613415956497192\n",
            "Epoch 3, Step 22187, Loss: 1.0879292488098145\n",
            "Epoch 3, Step 22188, Loss: 1.4667778015136719\n",
            "Epoch 3, Step 22189, Loss: 1.3974257707595825\n",
            "Epoch 3, Step 22190, Loss: 1.9731007814407349\n",
            "Epoch 3, Step 22191, Loss: 2.2315261363983154\n",
            "Epoch 3, Step 22192, Loss: 0.9122399091720581\n",
            "Epoch 3, Step 22193, Loss: 0.9708203673362732\n",
            "Epoch 3, Step 22194, Loss: 2.906217336654663\n",
            "Epoch 3, Step 22195, Loss: 2.0921947956085205\n",
            "Epoch 3, Step 22196, Loss: 1.622907042503357\n",
            "Epoch 3, Step 22197, Loss: 1.5075150728225708\n",
            "Epoch 3, Step 22198, Loss: 0.8306538462638855\n",
            "Epoch 3, Step 22199, Loss: 1.0519087314605713\n",
            "Epoch 3, Step 22200, Loss: 1.5432405471801758\n",
            "Epoch 3, Step 22201, Loss: 1.8256686925888062\n",
            "Epoch 3, Step 22202, Loss: 1.1297160387039185\n",
            "Epoch 3, Step 22203, Loss: 1.829433560371399\n",
            "Epoch 3, Step 22204, Loss: 1.443484902381897\n",
            "Epoch 3, Step 22205, Loss: 1.7614682912826538\n",
            "Epoch 3, Step 22206, Loss: 1.1827424764633179\n",
            "Epoch 3, Step 22207, Loss: 2.2746031284332275\n",
            "Epoch 3, Step 22208, Loss: 0.6958589553833008\n",
            "Epoch 3, Step 22209, Loss: 0.925728976726532\n",
            "Epoch 3, Step 22210, Loss: 2.4102249145507812\n",
            "Epoch 3, Step 22211, Loss: 1.739406704902649\n",
            "Epoch 3, Step 22212, Loss: 1.5423403978347778\n",
            "Epoch 3, Step 22213, Loss: 1.8795454502105713\n",
            "Epoch 3, Step 22214, Loss: 1.5024282932281494\n",
            "Epoch 3, Step 22215, Loss: 2.1523218154907227\n",
            "Epoch 3, Step 22216, Loss: 1.3893749713897705\n",
            "Epoch 3, Step 22217, Loss: 2.3683788776397705\n",
            "Epoch 3, Step 22218, Loss: 1.2512381076812744\n",
            "Epoch 3, Step 22219, Loss: 1.0160068273544312\n",
            "Epoch 3, Step 22220, Loss: 1.5826306343078613\n",
            "Epoch 3, Step 22221, Loss: 2.74491810798645\n",
            "Epoch 3, Step 22222, Loss: 0.6970900297164917\n",
            "Epoch 3, Step 22223, Loss: 1.126985788345337\n",
            "Epoch 3, Step 22224, Loss: 1.952104926109314\n",
            "Epoch 3, Step 22225, Loss: 1.7407714128494263\n",
            "Epoch 3, Step 22226, Loss: 1.5545384883880615\n",
            "Epoch 3, Step 22227, Loss: 1.7056509256362915\n",
            "Epoch 3, Step 22228, Loss: 1.1939235925674438\n",
            "Epoch 3, Step 22229, Loss: 2.009921073913574\n",
            "Epoch 3, Step 22230, Loss: 1.025909423828125\n",
            "Epoch 3, Step 22231, Loss: 1.3362702131271362\n",
            "Epoch 3, Step 22232, Loss: 1.9354032278060913\n",
            "Epoch 3, Step 22233, Loss: 3.0554280281066895\n",
            "Epoch 3, Step 22234, Loss: 1.9277713298797607\n",
            "Epoch 3, Step 22235, Loss: 1.9477263689041138\n",
            "Epoch 3, Step 22236, Loss: 1.9473474025726318\n",
            "Epoch 3, Step 22237, Loss: 1.0140587091445923\n",
            "Epoch 3, Step 22238, Loss: 2.3101556301116943\n",
            "Epoch 3, Step 22239, Loss: 1.6813890933990479\n",
            "Epoch 3, Step 22240, Loss: 1.3190782070159912\n",
            "Epoch 3, Step 22241, Loss: 2.0110366344451904\n",
            "Epoch 3, Step 22242, Loss: 1.5440285205841064\n",
            "Epoch 3, Step 22243, Loss: 1.9595613479614258\n",
            "Epoch 3, Step 22244, Loss: 1.3000118732452393\n",
            "Epoch 3, Step 22245, Loss: 1.2778592109680176\n",
            "Epoch 3, Step 22246, Loss: 1.168734073638916\n",
            "Epoch 3, Step 22247, Loss: 1.2271132469177246\n",
            "Epoch 3, Step 22248, Loss: 1.2328367233276367\n",
            "Epoch 3, Step 22249, Loss: 0.5809122323989868\n",
            "Epoch 3, Step 22250, Loss: 1.9609612226486206\n",
            "Epoch 3, Step 22251, Loss: 2.1056196689605713\n",
            "Epoch 3, Step 22252, Loss: 2.1004345417022705\n",
            "Epoch 3, Step 22253, Loss: 0.9855665564537048\n",
            "Epoch 3, Step 22254, Loss: 1.4639990329742432\n",
            "Epoch 3, Step 22255, Loss: 0.505488932132721\n",
            "Epoch 3, Step 22256, Loss: 2.159935235977173\n",
            "Epoch 3, Step 22257, Loss: 0.7877087593078613\n",
            "Epoch 3, Step 22258, Loss: 0.6632621884346008\n",
            "Epoch 3, Step 22259, Loss: 1.4583932161331177\n",
            "Epoch 3, Step 22260, Loss: 0.8343749642372131\n",
            "Epoch 3, Step 22261, Loss: 0.7480064034461975\n",
            "Epoch 3, Step 22262, Loss: 2.0291335582733154\n",
            "Epoch 3, Step 22263, Loss: 2.0698776245117188\n",
            "Epoch 3, Step 22264, Loss: 0.9568657279014587\n",
            "Epoch 3, Step 22265, Loss: 1.7506170272827148\n",
            "Epoch 3, Step 22266, Loss: 1.8594549894332886\n",
            "Epoch 3, Step 22267, Loss: 3.20888090133667\n",
            "Epoch 3, Step 22268, Loss: 1.415350079536438\n",
            "Epoch 3, Step 22269, Loss: 1.3545671701431274\n",
            "Epoch 3, Step 22270, Loss: 3.0935702323913574\n",
            "Epoch 3, Step 22271, Loss: 2.4468700885772705\n",
            "Epoch 3, Step 22272, Loss: 2.064162254333496\n",
            "Epoch 3, Step 22273, Loss: 1.352778434753418\n",
            "Epoch 3, Step 22274, Loss: 1.5117526054382324\n",
            "Epoch 3, Step 22275, Loss: 1.4975167512893677\n",
            "Epoch 3, Step 22276, Loss: 1.7937085628509521\n",
            "Epoch 3, Step 22277, Loss: 1.2930164337158203\n",
            "Epoch 3, Step 22278, Loss: 1.0337285995483398\n",
            "Epoch 3, Step 22279, Loss: 1.6408263444900513\n",
            "Epoch 3, Step 22280, Loss: 2.0308566093444824\n",
            "Epoch 3, Step 22281, Loss: 2.591874599456787\n",
            "Epoch 3, Step 22282, Loss: 1.6978399753570557\n",
            "Epoch 3, Step 22283, Loss: 1.0810290575027466\n",
            "Epoch 3, Step 22284, Loss: 1.5149097442626953\n",
            "Epoch 3, Step 22285, Loss: 1.1735577583312988\n",
            "Epoch 3, Step 22286, Loss: 0.9325314164161682\n",
            "Epoch 3, Step 22287, Loss: 1.6032062768936157\n",
            "Epoch 3, Step 22288, Loss: 1.5817030668258667\n",
            "Epoch 3, Step 22289, Loss: 2.0623254776000977\n",
            "Epoch 3, Step 22290, Loss: 1.6920785903930664\n",
            "Epoch 3, Step 22291, Loss: 2.019160270690918\n",
            "Epoch 3, Step 22292, Loss: 1.6125982999801636\n",
            "Epoch 3, Step 22293, Loss: 0.8757497072219849\n",
            "Epoch 3, Step 22294, Loss: 1.026090145111084\n",
            "Epoch 3, Step 22295, Loss: 1.6057119369506836\n",
            "Epoch 3, Step 22296, Loss: 1.5793594121932983\n",
            "Epoch 3, Step 22297, Loss: 1.1167796850204468\n",
            "Epoch 3, Step 22298, Loss: 1.0521342754364014\n",
            "Epoch 3, Step 22299, Loss: 1.745406150817871\n",
            "Epoch 3, Step 22300, Loss: 1.3544468879699707\n",
            "Epoch 3, Step 22301, Loss: 1.7835297584533691\n",
            "Epoch 3, Step 22302, Loss: 1.6685895919799805\n",
            "Epoch 3, Step 22303, Loss: 0.8232797384262085\n",
            "Epoch 3, Step 22304, Loss: 2.1093432903289795\n",
            "Epoch 3, Step 22305, Loss: 2.405200481414795\n",
            "Epoch 3, Step 22306, Loss: 0.7860609292984009\n",
            "Epoch 3, Step 22307, Loss: 1.7499946355819702\n",
            "Epoch 3, Step 22308, Loss: 2.396821975708008\n",
            "Epoch 3, Step 22309, Loss: 1.607454776763916\n",
            "Epoch 3, Step 22310, Loss: 1.7031793594360352\n",
            "Epoch 3, Step 22311, Loss: 1.5521353483200073\n",
            "Epoch 3, Step 22312, Loss: 1.8137873411178589\n",
            "Epoch 3, Step 22313, Loss: 1.7640916109085083\n",
            "Epoch 3, Step 22314, Loss: 1.7234026193618774\n",
            "Epoch 3, Step 22315, Loss: 1.6344150304794312\n",
            "Epoch 3, Step 22316, Loss: 1.1752841472625732\n",
            "Epoch 3, Step 22317, Loss: 2.309497117996216\n",
            "Epoch 3, Step 22318, Loss: 1.893048644065857\n",
            "Epoch 3, Step 22319, Loss: 2.70633864402771\n",
            "Epoch 3, Step 22320, Loss: 1.6584844589233398\n",
            "Epoch 3, Step 22321, Loss: 1.1524451971054077\n",
            "Epoch 3, Step 22322, Loss: 1.3929349184036255\n",
            "Epoch 3, Step 22323, Loss: 1.6979719400405884\n",
            "Epoch 3, Step 22324, Loss: 1.1300992965698242\n",
            "Epoch 3, Step 22325, Loss: 2.194051742553711\n",
            "Epoch 3, Step 22326, Loss: 1.4220927953720093\n",
            "Epoch 3, Step 22327, Loss: 1.0120576620101929\n",
            "Epoch 3, Step 22328, Loss: 2.3479084968566895\n",
            "Epoch 3, Step 22329, Loss: 1.8348242044448853\n",
            "Epoch 3, Step 22330, Loss: 1.5013004541397095\n",
            "Epoch 3, Step 22331, Loss: 1.3457313776016235\n",
            "Epoch 3, Step 22332, Loss: 0.8752105832099915\n",
            "Epoch 3, Step 22333, Loss: 1.6467455625534058\n",
            "Epoch 3, Step 22334, Loss: 1.1805795431137085\n",
            "Epoch 3, Step 22335, Loss: 1.7463219165802002\n",
            "Epoch 3, Step 22336, Loss: 1.0346152782440186\n",
            "Epoch 3, Step 22337, Loss: 1.1976760625839233\n",
            "Epoch 3, Step 22338, Loss: 1.6623667478561401\n",
            "Epoch 3, Step 22339, Loss: 1.270876169204712\n",
            "Epoch 3, Step 22340, Loss: 1.817200779914856\n",
            "Epoch 3, Step 22341, Loss: 1.5565160512924194\n",
            "Epoch 3, Step 22342, Loss: 1.1489194631576538\n",
            "Epoch 3, Step 22343, Loss: 1.1227821111679077\n",
            "Epoch 3, Step 22344, Loss: 2.1568684577941895\n",
            "Epoch 3, Step 22345, Loss: 1.2873905897140503\n",
            "Epoch 3, Step 22346, Loss: 1.8033636808395386\n",
            "Epoch 3, Step 22347, Loss: 1.467012882232666\n",
            "Epoch 3, Step 22348, Loss: 1.3546768426895142\n",
            "Epoch 3, Step 22349, Loss: 1.2763233184814453\n",
            "Epoch 3, Step 22350, Loss: 0.717200517654419\n",
            "Epoch 3, Step 22351, Loss: 1.9041768312454224\n",
            "Epoch 3, Step 22352, Loss: 2.166703701019287\n",
            "Epoch 3, Step 22353, Loss: 1.4290238618850708\n",
            "Epoch 3, Step 22354, Loss: 1.6940349340438843\n",
            "Epoch 3, Step 22355, Loss: 1.671424388885498\n",
            "Epoch 3, Step 22356, Loss: 1.5154279470443726\n",
            "Epoch 3, Step 22357, Loss: 1.5406345129013062\n",
            "Epoch 3, Step 22358, Loss: 1.2859781980514526\n",
            "Epoch 3, Step 22359, Loss: 1.5529406070709229\n",
            "Epoch 3, Step 22360, Loss: 0.8124489188194275\n",
            "Epoch 3, Step 22361, Loss: 2.3011858463287354\n",
            "Epoch 3, Step 22362, Loss: 1.3493746519088745\n",
            "Epoch 3, Step 22363, Loss: 1.1368948221206665\n",
            "Epoch 3, Step 22364, Loss: 0.6833586096763611\n",
            "Epoch 3, Step 22365, Loss: 0.6371393799781799\n",
            "Epoch 3, Step 22366, Loss: 2.290858745574951\n",
            "Epoch 3, Step 22367, Loss: 0.9812921285629272\n",
            "Epoch 3, Step 22368, Loss: 1.9006073474884033\n",
            "Epoch 3, Step 22369, Loss: 0.9386374354362488\n",
            "Epoch 3, Step 22370, Loss: 0.7913814783096313\n",
            "Epoch 3, Step 22371, Loss: 1.4012200832366943\n",
            "Epoch 3, Step 22372, Loss: 1.5200434923171997\n",
            "Epoch 3, Step 22373, Loss: 1.9478545188903809\n",
            "Epoch 3, Step 22374, Loss: 1.4858373403549194\n",
            "Epoch 3, Step 22375, Loss: 2.154975414276123\n",
            "Epoch 3, Step 22376, Loss: 1.9272446632385254\n",
            "Epoch 3, Step 22377, Loss: 1.8260835409164429\n",
            "Epoch 3, Step 22378, Loss: 0.6896982789039612\n",
            "Epoch 3, Step 22379, Loss: 1.2115918397903442\n",
            "Epoch 3, Step 22380, Loss: 1.0590626001358032\n",
            "Epoch 3, Step 22381, Loss: 1.3406730890274048\n",
            "Epoch 3, Step 22382, Loss: 1.2113773822784424\n",
            "Epoch 3, Step 22383, Loss: 1.9181057214736938\n",
            "Epoch 3, Step 22384, Loss: 1.2304942607879639\n",
            "Epoch 3, Step 22385, Loss: 0.96235591173172\n",
            "Epoch 3, Step 22386, Loss: 1.4611921310424805\n",
            "Epoch 3, Step 22387, Loss: 2.069451332092285\n",
            "Epoch 3, Step 22388, Loss: 1.7151364088058472\n",
            "Epoch 3, Step 22389, Loss: 1.7496966123580933\n",
            "Epoch 3, Step 22390, Loss: 2.180032968521118\n",
            "Epoch 3, Step 22391, Loss: 1.822116494178772\n",
            "Epoch 3, Step 22392, Loss: 1.167637586593628\n",
            "Epoch 3, Step 22393, Loss: 0.9987639784812927\n",
            "Epoch 3, Step 22394, Loss: 1.6459418535232544\n",
            "Epoch 3, Step 22395, Loss: 2.0002617835998535\n",
            "Epoch 3, Step 22396, Loss: 1.1316779851913452\n",
            "Epoch 3, Step 22397, Loss: 1.3667815923690796\n",
            "Epoch 3, Step 22398, Loss: 1.6581363677978516\n",
            "Epoch 3, Step 22399, Loss: 1.7264453172683716\n",
            "Epoch 3, Step 22400, Loss: 0.6926602125167847\n",
            "Epoch 3, Step 22401, Loss: 0.7929536700248718\n",
            "Epoch 3, Step 22402, Loss: 0.9447904229164124\n",
            "Epoch 3, Step 22403, Loss: 1.9157326221466064\n",
            "Epoch 3, Step 22404, Loss: 1.367059350013733\n",
            "Epoch 3, Step 22405, Loss: 0.5838683247566223\n",
            "Epoch 3, Step 22406, Loss: 1.4803900718688965\n",
            "Epoch 3, Step 22407, Loss: 0.8964356184005737\n",
            "Epoch 3, Step 22408, Loss: 1.375510811805725\n",
            "Epoch 3, Step 22409, Loss: 1.6840652227401733\n",
            "Epoch 3, Step 22410, Loss: 1.5532234907150269\n",
            "Epoch 3, Step 22411, Loss: 2.3073980808258057\n",
            "Epoch 3, Step 22412, Loss: 2.250336170196533\n",
            "Epoch 3, Step 22413, Loss: 1.901357650756836\n",
            "Epoch 3, Step 22414, Loss: 1.7174996137619019\n",
            "Epoch 3, Step 22415, Loss: 1.7028495073318481\n",
            "Epoch 3, Step 22416, Loss: 1.2722302675247192\n",
            "Epoch 3, Step 22417, Loss: 2.064180850982666\n",
            "Epoch 3, Step 22418, Loss: 1.5007203817367554\n",
            "Epoch 3, Step 22419, Loss: 2.230412483215332\n",
            "Epoch 3, Step 22420, Loss: 1.005126714706421\n",
            "Epoch 3, Step 22421, Loss: 0.8564680218696594\n",
            "Epoch 3, Step 22422, Loss: 0.7703350782394409\n",
            "Epoch 3, Step 22423, Loss: 1.603324294090271\n",
            "Epoch 3, Step 22424, Loss: 2.687863826751709\n",
            "Epoch 3, Step 22425, Loss: 1.58655846118927\n",
            "Epoch 3, Step 22426, Loss: 1.2331339120864868\n",
            "Epoch 3, Step 22427, Loss: 1.274192452430725\n",
            "Epoch 3, Step 22428, Loss: 1.6723575592041016\n",
            "Epoch 3, Step 22429, Loss: 2.8713228702545166\n",
            "Epoch 3, Step 22430, Loss: 1.6102715730667114\n",
            "Epoch 3, Step 22431, Loss: 2.1399552822113037\n",
            "Epoch 3, Step 22432, Loss: 2.18050479888916\n",
            "Epoch 3, Step 22433, Loss: 1.0019197463989258\n",
            "Epoch 3, Step 22434, Loss: 1.8944512605667114\n",
            "Epoch 3, Step 22435, Loss: 1.9232455492019653\n",
            "Epoch 3, Step 22436, Loss: 2.3964273929595947\n",
            "Epoch 3, Step 22437, Loss: 1.3550478219985962\n",
            "Epoch 3, Step 22438, Loss: 1.589112401008606\n",
            "Epoch 3, Step 22439, Loss: 2.069317102432251\n",
            "Epoch 3, Step 22440, Loss: 0.9399601221084595\n",
            "Epoch 3, Step 22441, Loss: 0.6079978346824646\n",
            "Epoch 3, Step 22442, Loss: 1.5511034727096558\n",
            "Epoch 3, Step 22443, Loss: 1.4059271812438965\n",
            "Epoch 3, Step 22444, Loss: 1.4447139501571655\n",
            "Epoch 3, Step 22445, Loss: 2.557133913040161\n",
            "Epoch 3, Step 22446, Loss: 0.3425813913345337\n",
            "Epoch 3, Step 22447, Loss: 1.8765698671340942\n",
            "Epoch 3, Step 22448, Loss: 2.644477605819702\n",
            "Epoch 3, Step 22449, Loss: 1.1574139595031738\n",
            "Epoch 3, Step 22450, Loss: 1.3007410764694214\n",
            "Epoch 3, Step 22451, Loss: 1.863519310951233\n",
            "Epoch 3, Step 22452, Loss: 1.1067277193069458\n",
            "Epoch 3, Step 22453, Loss: 1.857541799545288\n",
            "Epoch 3, Step 22454, Loss: 1.4337427616119385\n",
            "Epoch 3, Step 22455, Loss: 1.7489433288574219\n",
            "Epoch 3, Step 22456, Loss: 2.200773000717163\n",
            "Epoch 3, Step 22457, Loss: 2.118117332458496\n",
            "Epoch 3, Step 22458, Loss: 0.859871506690979\n",
            "Epoch 3, Step 22459, Loss: 1.8701578378677368\n",
            "Epoch 3, Step 22460, Loss: 2.107057809829712\n",
            "Epoch 3, Step 22461, Loss: 2.1738715171813965\n",
            "Epoch 3, Step 22462, Loss: 1.7519395351409912\n",
            "Epoch 3, Step 22463, Loss: 1.124733328819275\n",
            "Epoch 3, Step 22464, Loss: 1.990578055381775\n",
            "Epoch 3, Step 22465, Loss: 2.130143404006958\n",
            "Epoch 3, Step 22466, Loss: 0.8161471486091614\n",
            "Epoch 3, Step 22467, Loss: 1.1092838048934937\n",
            "Epoch 3, Step 22468, Loss: 0.9133128523826599\n",
            "Epoch 3, Step 22469, Loss: 0.7949885725975037\n",
            "Epoch 3, Step 22470, Loss: 1.5235528945922852\n",
            "Epoch 3, Step 22471, Loss: 2.271106719970703\n",
            "Epoch 3, Step 22472, Loss: 0.8874962329864502\n",
            "Epoch 3, Step 22473, Loss: 0.6262205243110657\n",
            "Epoch 3, Step 22474, Loss: 1.0428401231765747\n",
            "Epoch 3, Step 22475, Loss: 1.0355242490768433\n",
            "Epoch 3, Step 22476, Loss: 2.1795108318328857\n",
            "Epoch 3, Step 22477, Loss: 1.1941921710968018\n",
            "Epoch 3, Step 22478, Loss: 1.8093292713165283\n",
            "Epoch 3, Step 22479, Loss: 2.108818531036377\n",
            "Epoch 3, Step 22480, Loss: 1.291267991065979\n",
            "Epoch 3, Step 22481, Loss: 2.2268505096435547\n",
            "Epoch 3, Step 22482, Loss: 1.746861457824707\n",
            "Epoch 3, Step 22483, Loss: 1.1855242252349854\n",
            "Epoch 3, Step 22484, Loss: 2.214348077774048\n",
            "Epoch 3, Step 22485, Loss: 1.0078387260437012\n",
            "Epoch 3, Step 22486, Loss: 0.9908061027526855\n",
            "Epoch 3, Step 22487, Loss: 1.9542382955551147\n",
            "Epoch 3, Step 22488, Loss: 1.948146104812622\n",
            "Epoch 3, Step 22489, Loss: 2.0459957122802734\n",
            "Epoch 3, Step 22490, Loss: 1.565185785293579\n",
            "Epoch 3, Step 22491, Loss: 1.9240986108779907\n",
            "Epoch 3, Step 22492, Loss: 1.029197096824646\n",
            "Epoch 3, Step 22493, Loss: 1.173660159111023\n",
            "Epoch 3, Step 22494, Loss: 0.7828186750411987\n",
            "Epoch 3, Step 22495, Loss: 1.2031407356262207\n",
            "Epoch 3, Step 22496, Loss: 1.77157723903656\n",
            "Epoch 3, Step 22497, Loss: 1.2836283445358276\n",
            "Epoch 3, Step 22498, Loss: 1.3911874294281006\n",
            "Epoch 3, Step 22499, Loss: 1.3154494762420654\n",
            "Epoch 3, Step 22500, Loss: 1.6587446928024292\n",
            "Epoch 3, Step 22501, Loss: 1.9111560583114624\n",
            "Epoch 3, Step 22502, Loss: 0.7454836964607239\n",
            "Epoch 3, Step 22503, Loss: 1.3144612312316895\n",
            "Epoch 3, Step 22504, Loss: 0.844804048538208\n",
            "Epoch 3, Step 22505, Loss: 1.3189268112182617\n",
            "Epoch 3, Step 22506, Loss: 0.7477000951766968\n",
            "Epoch 3, Step 22507, Loss: 2.443114995956421\n",
            "Epoch 3, Step 22508, Loss: 2.186736822128296\n",
            "Epoch 3, Step 22509, Loss: 1.6453027725219727\n",
            "Epoch 3, Step 22510, Loss: 1.8297832012176514\n",
            "Epoch 3, Step 22511, Loss: 2.0511696338653564\n",
            "Epoch 3, Step 22512, Loss: 2.154541492462158\n",
            "Epoch 3, Step 22513, Loss: 2.246335744857788\n",
            "Epoch 3, Step 22514, Loss: 2.010591983795166\n",
            "Epoch 3, Step 22515, Loss: 1.8106412887573242\n",
            "Epoch 3, Step 22516, Loss: 1.244080662727356\n",
            "Epoch 3, Step 22517, Loss: 2.209821939468384\n",
            "Epoch 3, Step 22518, Loss: 0.5195695161819458\n",
            "Epoch 3, Step 22519, Loss: 1.2129868268966675\n",
            "Epoch 3, Step 22520, Loss: 0.908167839050293\n",
            "Epoch 3, Step 22521, Loss: 0.8593524098396301\n",
            "Epoch 3, Step 22522, Loss: 1.7999396324157715\n",
            "Epoch 3, Step 22523, Loss: 1.1666169166564941\n",
            "Epoch 3, Step 22524, Loss: 1.7624353170394897\n",
            "Epoch 3, Step 22525, Loss: 2.8189051151275635\n",
            "Epoch 3, Step 22526, Loss: 1.369983434677124\n",
            "Epoch 3, Step 22527, Loss: 1.618058681488037\n",
            "Epoch 3, Step 22528, Loss: 2.2013096809387207\n",
            "Epoch 3, Step 22529, Loss: 1.6211495399475098\n",
            "Epoch 3, Step 22530, Loss: 1.3285136222839355\n",
            "Epoch 3, Step 22531, Loss: 1.560150146484375\n",
            "Epoch 3, Step 22532, Loss: 0.8472452163696289\n",
            "Epoch 3, Step 22533, Loss: 2.1545560359954834\n",
            "Epoch 3, Step 22534, Loss: 1.8235735893249512\n",
            "Epoch 3, Step 22535, Loss: 1.9196003675460815\n",
            "Epoch 3, Step 22536, Loss: 3.0236196517944336\n",
            "Epoch 3, Step 22537, Loss: 2.0868098735809326\n",
            "Epoch 3, Step 22538, Loss: 1.4480524063110352\n",
            "Epoch 3, Step 22539, Loss: 1.2999619245529175\n",
            "Epoch 3, Step 22540, Loss: 1.1151933670043945\n",
            "Epoch 3, Step 22541, Loss: 2.4847300052642822\n",
            "Epoch 3, Step 22542, Loss: 1.29136061668396\n",
            "Epoch 3, Step 22543, Loss: 1.2247889041900635\n",
            "Epoch 3, Step 22544, Loss: 2.4983251094818115\n",
            "Epoch 3, Step 22545, Loss: 0.6532171368598938\n",
            "Epoch 3, Step 22546, Loss: 1.9953029155731201\n",
            "Epoch 3, Step 22547, Loss: 1.9616433382034302\n",
            "Epoch 3, Step 22548, Loss: 0.9858548045158386\n",
            "Epoch 3, Step 22549, Loss: 1.5316015481948853\n",
            "Epoch 3, Step 22550, Loss: 1.8181471824645996\n",
            "Epoch 3, Step 22551, Loss: 2.074342966079712\n",
            "Epoch 3, Step 22552, Loss: 2.610569715499878\n",
            "Epoch 3, Step 22553, Loss: 2.080284595489502\n",
            "Epoch 3, Step 22554, Loss: 1.2960798740386963\n",
            "Epoch 3, Step 22555, Loss: 1.2271968126296997\n",
            "Epoch 3, Step 22556, Loss: 2.3446199893951416\n",
            "Epoch 3, Step 22557, Loss: 1.5146585702896118\n",
            "Epoch 3, Step 22558, Loss: 1.4067270755767822\n",
            "Epoch 3, Step 22559, Loss: 0.9508266448974609\n",
            "Epoch 3, Step 22560, Loss: 0.6006373167037964\n",
            "Epoch 3, Step 22561, Loss: 1.6503366231918335\n",
            "Epoch 3, Step 22562, Loss: 3.2251076698303223\n",
            "Epoch 3, Step 22563, Loss: 1.584805965423584\n",
            "Epoch 3, Step 22564, Loss: 1.9823071956634521\n",
            "Epoch 3, Step 22565, Loss: 1.7949638366699219\n",
            "Epoch 3, Step 22566, Loss: 0.8799703121185303\n",
            "Epoch 3, Step 22567, Loss: 2.035000801086426\n",
            "Epoch 3, Step 22568, Loss: 1.624202847480774\n",
            "Epoch 3, Step 22569, Loss: 1.2982938289642334\n",
            "Epoch 3, Step 22570, Loss: 2.0142698287963867\n",
            "Epoch 3, Step 22571, Loss: 2.02726149559021\n",
            "Epoch 3, Step 22572, Loss: 1.4085534811019897\n",
            "Epoch 3, Step 22573, Loss: 1.1542733907699585\n",
            "Epoch 3, Step 22574, Loss: 1.0957027673721313\n",
            "Epoch 3, Step 22575, Loss: 0.776237428188324\n",
            "Epoch 3, Step 22576, Loss: 0.6467171907424927\n",
            "Epoch 3, Step 22577, Loss: 0.3635976314544678\n",
            "Epoch 3, Step 22578, Loss: 2.0455245971679688\n",
            "Epoch 3, Step 22579, Loss: 2.4695138931274414\n",
            "Epoch 3, Step 22580, Loss: 2.1336143016815186\n",
            "Epoch 3, Step 22581, Loss: 1.547396183013916\n",
            "Epoch 3, Step 22582, Loss: 1.0448111295700073\n",
            "Epoch 3, Step 22583, Loss: 2.6571574211120605\n",
            "Epoch 3, Step 22584, Loss: 0.9942213296890259\n",
            "Epoch 3, Step 22585, Loss: 1.0058692693710327\n",
            "Epoch 3, Step 22586, Loss: 1.0039843320846558\n",
            "Epoch 3, Step 22587, Loss: 2.3793435096740723\n",
            "Epoch 3, Step 22588, Loss: 0.7073543667793274\n",
            "Epoch 3, Step 22589, Loss: 1.3400763273239136\n",
            "Epoch 3, Step 22590, Loss: 1.9216477870941162\n",
            "Epoch 3, Step 22591, Loss: 1.3723915815353394\n",
            "Epoch 3, Step 22592, Loss: 1.1212732791900635\n",
            "Epoch 3, Step 22593, Loss: 0.9598902463912964\n",
            "Epoch 3, Step 22594, Loss: 1.1038063764572144\n",
            "Epoch 3, Step 22595, Loss: 1.5164631605148315\n",
            "Epoch 3, Step 22596, Loss: 1.8099548816680908\n",
            "Epoch 3, Step 22597, Loss: 1.9832675457000732\n",
            "Epoch 3, Step 22598, Loss: 1.1770238876342773\n",
            "Epoch 3, Step 22599, Loss: 1.4114060401916504\n",
            "Epoch 3, Step 22600, Loss: 1.8705891370773315\n",
            "Epoch 3, Step 22601, Loss: 1.3672990798950195\n",
            "Epoch 3, Step 22602, Loss: 1.3728379011154175\n",
            "Epoch 3, Step 22603, Loss: 0.7142841219902039\n",
            "Epoch 3, Step 22604, Loss: 2.169252395629883\n",
            "Epoch 3, Step 22605, Loss: 1.5413191318511963\n",
            "Epoch 3, Step 22606, Loss: 1.2319458723068237\n",
            "Epoch 3, Step 22607, Loss: 2.1708953380584717\n",
            "Epoch 3, Step 22608, Loss: 2.1476967334747314\n",
            "Epoch 3, Step 22609, Loss: 2.102557897567749\n",
            "Epoch 3, Step 22610, Loss: 1.1804100275039673\n",
            "Epoch 3, Step 22611, Loss: 1.4254525899887085\n",
            "Epoch 3, Step 22612, Loss: 1.556850552558899\n",
            "Epoch 3, Step 22613, Loss: 1.5760010480880737\n",
            "Epoch 3, Step 22614, Loss: 0.8046097159385681\n",
            "Epoch 3, Step 22615, Loss: 1.614832878112793\n",
            "Epoch 3, Step 22616, Loss: 2.119676113128662\n",
            "Epoch 3, Step 22617, Loss: 2.7950222492218018\n",
            "Epoch 3, Step 22618, Loss: 2.3774220943450928\n",
            "Epoch 3, Step 22619, Loss: 1.6591830253601074\n",
            "Epoch 3, Step 22620, Loss: 1.645208716392517\n",
            "Epoch 3, Step 22621, Loss: 1.154412865638733\n",
            "Epoch 3, Step 22622, Loss: 2.2504589557647705\n",
            "Epoch 3, Step 22623, Loss: 1.4679275751113892\n",
            "Epoch 3, Step 22624, Loss: 1.3631770610809326\n",
            "Epoch 3, Step 22625, Loss: 1.776436686515808\n",
            "Epoch 3, Step 22626, Loss: 1.7723652124404907\n",
            "Epoch 3, Step 22627, Loss: 2.0713863372802734\n",
            "Epoch 3, Step 22628, Loss: 1.403782844543457\n",
            "Epoch 3, Step 22629, Loss: 1.4518613815307617\n",
            "Epoch 3, Step 22630, Loss: 1.8004539012908936\n",
            "Epoch 3, Step 22631, Loss: 2.365809440612793\n",
            "Epoch 3, Step 22632, Loss: 1.3789275884628296\n",
            "Epoch 3, Step 22633, Loss: 1.7164722681045532\n",
            "Epoch 3, Step 22634, Loss: 1.6822699308395386\n",
            "Epoch 3, Step 22635, Loss: 1.8186227083206177\n",
            "Epoch 3, Step 22636, Loss: 0.742414653301239\n",
            "Epoch 3, Step 22637, Loss: 1.3403786420822144\n",
            "Epoch 3, Step 22638, Loss: 0.879611074924469\n",
            "Epoch 3, Step 22639, Loss: 1.099919080734253\n",
            "Epoch 3, Step 22640, Loss: 1.996158242225647\n",
            "Epoch 3, Step 22641, Loss: 1.571170449256897\n",
            "Epoch 3, Step 22642, Loss: 0.5916361212730408\n",
            "Epoch 3, Step 22643, Loss: 1.8939191102981567\n",
            "Epoch 3, Step 22644, Loss: 1.411473274230957\n",
            "Epoch 3, Step 22645, Loss: 1.6097288131713867\n",
            "Epoch 3, Step 22646, Loss: 1.740267276763916\n",
            "Epoch 3, Step 22647, Loss: 1.2046374082565308\n",
            "Epoch 3, Step 22648, Loss: 1.1671664714813232\n",
            "Epoch 3, Step 22649, Loss: 1.5100845098495483\n",
            "Epoch 3, Step 22650, Loss: 1.9160102605819702\n",
            "Epoch 3, Step 22651, Loss: 1.0801602602005005\n",
            "Epoch 3, Step 22652, Loss: 2.4803483486175537\n",
            "Epoch 3, Step 22653, Loss: 2.296619415283203\n",
            "Epoch 3, Step 22654, Loss: 1.555681824684143\n",
            "Epoch 3, Step 22655, Loss: 1.6465740203857422\n",
            "Epoch 3, Step 22656, Loss: 1.2986868619918823\n",
            "Epoch 3, Step 22657, Loss: 0.6702579855918884\n",
            "Epoch 3, Step 22658, Loss: 0.6095905303955078\n",
            "Epoch 3, Step 22659, Loss: 1.5530723333358765\n",
            "Epoch 3, Step 22660, Loss: 1.4055746793746948\n",
            "Epoch 3, Step 22661, Loss: 2.0183494091033936\n",
            "Epoch 3, Step 22662, Loss: 1.351667046546936\n",
            "Epoch 3, Step 22663, Loss: 2.2313284873962402\n",
            "Epoch 3, Step 22664, Loss: 1.534251093864441\n",
            "Epoch 3, Step 22665, Loss: 1.1860532760620117\n",
            "Epoch 3, Step 22666, Loss: 1.1993778944015503\n",
            "Epoch 3, Step 22667, Loss: 1.8441476821899414\n",
            "Epoch 3, Step 22668, Loss: 1.6174330711364746\n",
            "Epoch 3, Step 22669, Loss: 1.8688504695892334\n",
            "Epoch 3, Step 22670, Loss: 1.4498063325881958\n",
            "Epoch 3, Step 22671, Loss: 1.5925463438034058\n",
            "Epoch 3, Step 22672, Loss: 2.6823549270629883\n",
            "Epoch 3, Step 22673, Loss: 0.7599620819091797\n",
            "Epoch 3, Step 22674, Loss: 2.020533561706543\n",
            "Epoch 3, Step 22675, Loss: 1.2151520252227783\n",
            "Epoch 3, Step 22676, Loss: 1.141263723373413\n",
            "Epoch 3, Step 22677, Loss: 2.3001770973205566\n",
            "Epoch 3, Step 22678, Loss: 0.9722230434417725\n",
            "Epoch 3, Step 22679, Loss: 1.1314332485198975\n",
            "Epoch 3, Step 22680, Loss: 1.7779862880706787\n",
            "Epoch 3, Step 22681, Loss: 1.7367937564849854\n",
            "Epoch 3, Step 22682, Loss: 1.815697193145752\n",
            "Epoch 3, Step 22683, Loss: 1.663867473602295\n",
            "Epoch 3, Step 22684, Loss: 1.2461811304092407\n",
            "Epoch 3, Step 22685, Loss: 1.0978951454162598\n",
            "Epoch 3, Step 22686, Loss: 1.8469957113265991\n",
            "Epoch 3, Step 22687, Loss: 1.522212266921997\n",
            "Epoch 3, Step 22688, Loss: 2.1919174194335938\n",
            "Epoch 3, Step 22689, Loss: 1.8491283655166626\n",
            "Epoch 3, Step 22690, Loss: 1.667831301689148\n",
            "Epoch 3, Step 22691, Loss: 0.7400233149528503\n",
            "Epoch 3, Step 22692, Loss: 2.8696017265319824\n",
            "Epoch 3, Step 22693, Loss: 2.355518341064453\n",
            "Epoch 3, Step 22694, Loss: 1.6738390922546387\n",
            "Epoch 3, Step 22695, Loss: 1.80515456199646\n",
            "Epoch 3, Step 22696, Loss: 1.9002883434295654\n",
            "Epoch 3, Step 22697, Loss: 1.3470982313156128\n",
            "Epoch 3, Step 22698, Loss: 1.4775669574737549\n",
            "Epoch 3, Step 22699, Loss: 1.4574764966964722\n",
            "Epoch 3, Step 22700, Loss: 1.5161601305007935\n",
            "Epoch 3, Step 22701, Loss: 1.7463274002075195\n",
            "Epoch 3, Step 22702, Loss: 1.8192012310028076\n",
            "Epoch 3, Step 22703, Loss: 2.2233898639678955\n",
            "Epoch 3, Step 22704, Loss: 2.258517026901245\n",
            "Epoch 3, Step 22705, Loss: 0.7578332424163818\n",
            "Epoch 3, Step 22706, Loss: 1.3582546710968018\n",
            "Epoch 3, Step 22707, Loss: 1.485293984413147\n",
            "Epoch 3, Step 22708, Loss: 1.1935020685195923\n",
            "Epoch 3, Step 22709, Loss: 0.6244378089904785\n",
            "Epoch 3, Step 22710, Loss: 0.8949397206306458\n",
            "Epoch 3, Step 22711, Loss: 1.6151798963546753\n",
            "Epoch 3, Step 22712, Loss: 2.2451765537261963\n",
            "Epoch 3, Step 22713, Loss: 1.2192142009735107\n",
            "Epoch 3, Step 22714, Loss: 0.7518948316574097\n",
            "Epoch 3, Step 22715, Loss: 2.2071621417999268\n",
            "Epoch 3, Step 22716, Loss: 1.1432240009307861\n",
            "Epoch 3, Step 22717, Loss: 0.9707716107368469\n",
            "Epoch 3, Step 22718, Loss: 1.0272793769836426\n",
            "Epoch 3, Step 22719, Loss: 2.008327007293701\n",
            "Epoch 3, Step 22720, Loss: 0.9331640005111694\n",
            "Epoch 3, Step 22721, Loss: 1.911751389503479\n",
            "Epoch 3, Step 22722, Loss: 0.6438753604888916\n",
            "Epoch 3, Step 22723, Loss: 1.7250486612319946\n",
            "Epoch 3, Step 22724, Loss: 1.6515225172042847\n",
            "Epoch 3, Step 22725, Loss: 0.9332906007766724\n",
            "Epoch 3, Step 22726, Loss: 1.4904321432113647\n",
            "Epoch 3, Step 22727, Loss: 1.6623347997665405\n",
            "Epoch 3, Step 22728, Loss: 0.6958277225494385\n",
            "Epoch 3, Step 22729, Loss: 1.5523964166641235\n",
            "Epoch 3, Step 22730, Loss: 1.3656322956085205\n",
            "Epoch 3, Step 22731, Loss: 2.1980557441711426\n",
            "Epoch 3, Step 22732, Loss: 1.8164572715759277\n",
            "Epoch 3, Step 22733, Loss: 1.5819177627563477\n",
            "Epoch 3, Step 22734, Loss: 1.5344170331954956\n",
            "Epoch 3, Step 22735, Loss: 2.659289598464966\n",
            "Epoch 3, Step 22736, Loss: 1.0894889831542969\n",
            "Epoch 3, Step 22737, Loss: 1.4290956258773804\n",
            "Epoch 3, Step 22738, Loss: 1.2856578826904297\n",
            "Epoch 3, Step 22739, Loss: 1.3603267669677734\n",
            "Epoch 3, Step 22740, Loss: 1.144607663154602\n",
            "Epoch 3, Step 22741, Loss: 1.8159759044647217\n",
            "Epoch 3, Step 22742, Loss: 1.918514609336853\n",
            "Epoch 3, Step 22743, Loss: 1.353804349899292\n",
            "Epoch 3, Step 22744, Loss: 2.4675865173339844\n",
            "Epoch 3, Step 22745, Loss: 0.911082923412323\n",
            "Epoch 3, Step 22746, Loss: 1.9840885400772095\n",
            "Epoch 3, Step 22747, Loss: 0.8222031593322754\n",
            "Epoch 3, Step 22748, Loss: 1.3423359394073486\n",
            "Epoch 3, Step 22749, Loss: 2.377260684967041\n",
            "Epoch 3, Step 22750, Loss: 0.7833021879196167\n",
            "Epoch 3, Step 22751, Loss: 1.8383073806762695\n",
            "Epoch 3, Step 22752, Loss: 1.3558512926101685\n",
            "Epoch 3, Step 22753, Loss: 1.7917561531066895\n",
            "Epoch 3, Step 22754, Loss: 1.0774235725402832\n",
            "Epoch 3, Step 22755, Loss: 1.0781258344650269\n",
            "Epoch 3, Step 22756, Loss: 1.4481017589569092\n",
            "Epoch 3, Step 22757, Loss: 2.5521657466888428\n",
            "Epoch 3, Step 22758, Loss: 0.5379982590675354\n",
            "Epoch 3, Step 22759, Loss: 1.3546806573867798\n",
            "Epoch 3, Step 22760, Loss: 2.0614748001098633\n",
            "Epoch 3, Step 22761, Loss: 1.9430675506591797\n",
            "Epoch 3, Step 22762, Loss: 1.374490737915039\n",
            "Epoch 3, Step 22763, Loss: 2.3629040718078613\n",
            "Epoch 3, Step 22764, Loss: 0.9179458022117615\n",
            "Epoch 3, Step 22765, Loss: 1.5260108709335327\n",
            "Epoch 3, Step 22766, Loss: 1.5079799890518188\n",
            "Epoch 3, Step 22767, Loss: 0.72137451171875\n",
            "Epoch 3, Step 22768, Loss: 1.2552295923233032\n",
            "Epoch 3, Step 22769, Loss: 1.9080936908721924\n",
            "Epoch 3, Step 22770, Loss: 2.0570366382598877\n",
            "Epoch 3, Step 22771, Loss: 0.48187246918678284\n",
            "Epoch 3, Step 22772, Loss: 2.241711139678955\n",
            "Epoch 3, Step 22773, Loss: 1.3771171569824219\n",
            "Epoch 3, Step 22774, Loss: 1.28426992893219\n",
            "Epoch 3, Step 22775, Loss: 1.9510656595230103\n",
            "Epoch 3, Step 22776, Loss: 2.098881244659424\n",
            "Epoch 3, Step 22777, Loss: 0.7325454354286194\n",
            "Epoch 3, Step 22778, Loss: 2.195580244064331\n",
            "Epoch 3, Step 22779, Loss: 0.9957185387611389\n",
            "Epoch 3, Step 22780, Loss: 2.1307036876678467\n",
            "Epoch 3, Step 22781, Loss: 1.451444149017334\n",
            "Epoch 3, Step 22782, Loss: 1.1355085372924805\n",
            "Epoch 3, Step 22783, Loss: 1.581407070159912\n",
            "Epoch 3, Step 22784, Loss: 1.9469290971755981\n",
            "Epoch 3, Step 22785, Loss: 2.198474884033203\n",
            "Epoch 3, Step 22786, Loss: 1.7286685705184937\n",
            "Epoch 3, Step 22787, Loss: 1.325238823890686\n",
            "Epoch 3, Step 22788, Loss: 2.0662155151367188\n",
            "Epoch 3, Step 22789, Loss: 1.27228844165802\n",
            "Epoch 3, Step 22790, Loss: 2.137164354324341\n",
            "Epoch 3, Step 22791, Loss: 1.4913055896759033\n",
            "Epoch 3, Step 22792, Loss: 2.154569149017334\n",
            "Epoch 3, Step 22793, Loss: 1.1456799507141113\n",
            "Epoch 3, Step 22794, Loss: 1.3575584888458252\n",
            "Epoch 3, Step 22795, Loss: 1.7572870254516602\n",
            "Epoch 3, Step 22796, Loss: 0.9171698093414307\n",
            "Epoch 3, Step 22797, Loss: 1.331084966659546\n",
            "Epoch 3, Step 22798, Loss: 1.0519723892211914\n",
            "Epoch 3, Step 22799, Loss: 1.3623340129852295\n",
            "Epoch 3, Step 22800, Loss: 1.4689055681228638\n",
            "Epoch 3, Step 22801, Loss: 1.2775733470916748\n",
            "Epoch 3, Step 22802, Loss: 2.8034937381744385\n",
            "Epoch 3, Step 22803, Loss: 1.3237606287002563\n",
            "Epoch 3, Step 22804, Loss: 2.1048331260681152\n",
            "Epoch 3, Step 22805, Loss: 2.0636889934539795\n",
            "Epoch 3, Step 22806, Loss: 1.7711542844772339\n",
            "Epoch 3, Step 22807, Loss: 2.376732110977173\n",
            "Epoch 3, Step 22808, Loss: 2.443910598754883\n",
            "Epoch 3, Step 22809, Loss: 0.8478622436523438\n",
            "Epoch 3, Step 22810, Loss: 2.7261693477630615\n",
            "Epoch 3, Step 22811, Loss: 2.5224802494049072\n",
            "Epoch 3, Step 22812, Loss: 1.306079626083374\n",
            "Epoch 3, Step 22813, Loss: 0.8182165026664734\n",
            "Epoch 3, Step 22814, Loss: 1.474393606185913\n",
            "Epoch 3, Step 22815, Loss: 1.8570516109466553\n",
            "Epoch 3, Step 22816, Loss: 1.65443754196167\n",
            "Epoch 3, Step 22817, Loss: 1.2665008306503296\n",
            "Epoch 3, Step 22818, Loss: 1.165907621383667\n",
            "Epoch 3, Step 22819, Loss: 0.7735714912414551\n",
            "Epoch 3, Step 22820, Loss: 0.880336344242096\n",
            "Epoch 3, Step 22821, Loss: 1.1631090641021729\n",
            "Epoch 3, Step 22822, Loss: 0.612108588218689\n",
            "Epoch 3, Step 22823, Loss: 0.7193702459335327\n",
            "Epoch 3, Step 22824, Loss: 1.447741985321045\n",
            "Epoch 3, Step 22825, Loss: 1.9735419750213623\n",
            "Epoch 3, Step 22826, Loss: 2.262805700302124\n",
            "Epoch 3, Step 22827, Loss: 1.1927369832992554\n",
            "Epoch 3, Step 22828, Loss: 1.3064686059951782\n",
            "Epoch 3, Step 22829, Loss: 1.5875402688980103\n",
            "Epoch 3, Step 22830, Loss: 1.9218820333480835\n",
            "Epoch 3, Step 22831, Loss: 1.3116004467010498\n",
            "Epoch 3, Step 22832, Loss: 1.8447730541229248\n",
            "Epoch 3, Step 22833, Loss: 2.1672823429107666\n",
            "Epoch 3, Step 22834, Loss: 1.7046352624893188\n",
            "Epoch 3, Step 22835, Loss: 1.5464208126068115\n",
            "Epoch 3, Step 22836, Loss: 0.7823009490966797\n",
            "Epoch 3, Step 22837, Loss: 0.8930541276931763\n",
            "Epoch 3, Step 22838, Loss: 0.34199029207229614\n",
            "Epoch 3, Step 22839, Loss: 1.0572309494018555\n",
            "Epoch 3, Step 22840, Loss: 1.905792236328125\n",
            "Epoch 3, Step 22841, Loss: 1.6631970405578613\n",
            "Epoch 3, Step 22842, Loss: 1.8170963525772095\n",
            "Epoch 3, Step 22843, Loss: 1.569823980331421\n",
            "Epoch 3, Step 22844, Loss: 0.6392906904220581\n",
            "Epoch 3, Step 22845, Loss: 2.4401798248291016\n",
            "Epoch 3, Step 22846, Loss: 2.0276682376861572\n",
            "Epoch 3, Step 22847, Loss: 2.3332133293151855\n",
            "Epoch 3, Step 22848, Loss: 1.7074105739593506\n",
            "Epoch 3, Step 22849, Loss: 1.613424301147461\n",
            "Epoch 3, Step 22850, Loss: 1.1410452127456665\n",
            "Epoch 3, Step 22851, Loss: 1.9373998641967773\n",
            "Epoch 3, Step 22852, Loss: 1.6018483638763428\n",
            "Epoch 3, Step 22853, Loss: 1.0889073610305786\n",
            "Epoch 3, Step 22854, Loss: 1.3104798793792725\n",
            "Epoch 3, Step 22855, Loss: 0.6460239291191101\n",
            "Epoch 3, Step 22856, Loss: 1.7016422748565674\n",
            "Epoch 3, Step 22857, Loss: 1.540661096572876\n",
            "Epoch 3, Step 22858, Loss: 1.5595569610595703\n",
            "Epoch 3, Step 22859, Loss: 1.4811949729919434\n",
            "Epoch 3, Step 22860, Loss: 1.576238751411438\n",
            "Epoch 3, Step 22861, Loss: 2.3563613891601562\n",
            "Epoch 3, Step 22862, Loss: 1.1458467245101929\n",
            "Epoch 3, Step 22863, Loss: 1.470186471939087\n",
            "Epoch 3, Step 22864, Loss: 1.490012526512146\n",
            "Epoch 3, Step 22865, Loss: 2.203821897506714\n",
            "Epoch 3, Step 22866, Loss: 1.5438729524612427\n",
            "Epoch 3, Step 22867, Loss: 0.5799895524978638\n",
            "Epoch 3, Step 22868, Loss: 1.1337007284164429\n",
            "Epoch 3, Step 22869, Loss: 1.9577463865280151\n",
            "Epoch 3, Step 22870, Loss: 2.788914918899536\n",
            "Epoch 3, Step 22871, Loss: 1.024822473526001\n",
            "Epoch 3, Step 22872, Loss: 1.270051121711731\n",
            "Epoch 3, Step 22873, Loss: 0.6534667015075684\n",
            "Epoch 3, Step 22874, Loss: 0.9376885890960693\n",
            "Epoch 3, Step 22875, Loss: 1.6448023319244385\n",
            "Epoch 3, Step 22876, Loss: 1.7963452339172363\n",
            "Epoch 3, Step 22877, Loss: 0.5850370526313782\n",
            "Epoch 3, Step 22878, Loss: 1.5699963569641113\n",
            "Epoch 3, Step 22879, Loss: 1.871358871459961\n",
            "Epoch 3, Step 22880, Loss: 2.6465530395507812\n",
            "Epoch 3, Step 22881, Loss: 1.16634202003479\n",
            "Epoch 3, Step 22882, Loss: 2.450113296508789\n",
            "Epoch 3, Step 22883, Loss: 2.572603225708008\n",
            "Epoch 3, Step 22884, Loss: 1.2127958536148071\n",
            "Epoch 3, Step 22885, Loss: 1.3818241357803345\n",
            "Epoch 3, Step 22886, Loss: 2.2192397117614746\n",
            "Epoch 3, Step 22887, Loss: 1.6308269500732422\n",
            "Epoch 3, Step 22888, Loss: 1.5814204216003418\n",
            "Epoch 3, Step 22889, Loss: 1.883622407913208\n",
            "Epoch 3, Step 22890, Loss: 1.544441819190979\n",
            "Epoch 3, Step 22891, Loss: 1.8427430391311646\n",
            "Epoch 3, Step 22892, Loss: 2.15444016456604\n",
            "Epoch 3, Step 22893, Loss: 1.8041667938232422\n",
            "Epoch 3, Step 22894, Loss: 2.191591501235962\n",
            "Epoch 3, Step 22895, Loss: 1.8863435983657837\n",
            "Epoch 3, Step 22896, Loss: 1.4961087703704834\n",
            "Epoch 3, Step 22897, Loss: 1.5745899677276611\n",
            "Epoch 3, Step 22898, Loss: 1.0953506231307983\n",
            "Epoch 3, Step 22899, Loss: 1.8917955160140991\n",
            "Epoch 3, Step 22900, Loss: 2.149933338165283\n",
            "Epoch 3, Step 22901, Loss: 1.4305669069290161\n",
            "Epoch 3, Step 22902, Loss: 1.9620482921600342\n",
            "Epoch 3, Step 22903, Loss: 1.8007850646972656\n",
            "Epoch 3, Step 22904, Loss: 0.8256251811981201\n",
            "Epoch 3, Step 22905, Loss: 2.6823596954345703\n",
            "Epoch 3, Step 22906, Loss: 1.6611497402191162\n",
            "Epoch 3, Step 22907, Loss: 2.014174699783325\n",
            "Epoch 3, Step 22908, Loss: 1.611689805984497\n",
            "Epoch 3, Step 22909, Loss: 1.4070992469787598\n",
            "Epoch 3, Step 22910, Loss: 2.034538745880127\n",
            "Epoch 3, Step 22911, Loss: 2.314645290374756\n",
            "Epoch 3, Step 22912, Loss: 1.4643462896347046\n",
            "Epoch 3, Step 22913, Loss: 2.043505907058716\n",
            "Epoch 3, Step 22914, Loss: 2.1127214431762695\n",
            "Epoch 3, Step 22915, Loss: 1.3056222200393677\n",
            "Epoch 3, Step 22916, Loss: 1.0356909036636353\n",
            "Epoch 3, Step 22917, Loss: 1.2587049007415771\n",
            "Epoch 3, Step 22918, Loss: 1.7254143953323364\n",
            "Epoch 3, Step 22919, Loss: 1.9814459085464478\n",
            "Epoch 3, Step 22920, Loss: 1.8869398832321167\n",
            "Epoch 3, Step 22921, Loss: 1.8776710033416748\n",
            "Epoch 3, Step 22922, Loss: 1.598731279373169\n",
            "Epoch 3, Step 22923, Loss: 1.6334242820739746\n",
            "Epoch 3, Step 22924, Loss: 1.7652676105499268\n",
            "Epoch 3, Step 22925, Loss: 1.4905833005905151\n",
            "Epoch 3, Step 22926, Loss: 1.6382725238800049\n",
            "Epoch 3, Step 22927, Loss: 1.9275295734405518\n",
            "Epoch 3, Step 22928, Loss: 1.4553806781768799\n",
            "Epoch 3, Step 22929, Loss: 1.6777478456497192\n",
            "Epoch 3, Step 22930, Loss: 0.5287730097770691\n",
            "Epoch 3, Step 22931, Loss: 1.6705642938613892\n",
            "Epoch 3, Step 22932, Loss: 1.7592734098434448\n",
            "Epoch 3, Step 22933, Loss: 0.8216223120689392\n",
            "Epoch 3, Step 22934, Loss: 1.6641862392425537\n",
            "Epoch 3, Step 22935, Loss: 2.1030759811401367\n",
            "Epoch 3, Step 22936, Loss: 1.4694565534591675\n",
            "Epoch 3, Step 22937, Loss: 1.4028596878051758\n",
            "Epoch 3, Step 22938, Loss: 1.3284518718719482\n",
            "Epoch 3, Step 22939, Loss: 1.5248675346374512\n",
            "Epoch 3, Step 22940, Loss: 1.1234313249588013\n",
            "Epoch 3, Step 22941, Loss: 1.489747405052185\n",
            "Epoch 3, Step 22942, Loss: 2.849484920501709\n",
            "Epoch 3, Step 22943, Loss: 2.7182984352111816\n",
            "Epoch 3, Step 22944, Loss: 1.477423906326294\n",
            "Epoch 3, Step 22945, Loss: 1.50002920627594\n",
            "Epoch 3, Step 22946, Loss: 1.218407154083252\n",
            "Epoch 3, Step 22947, Loss: 1.802934169769287\n",
            "Epoch 3, Step 22948, Loss: 1.780331015586853\n",
            "Epoch 3, Step 22949, Loss: 2.009939432144165\n",
            "Epoch 3, Step 22950, Loss: 0.8281320929527283\n",
            "Epoch 3, Step 22951, Loss: 0.8006200194358826\n",
            "Epoch 3, Step 22952, Loss: 0.7949968576431274\n",
            "Epoch 3, Step 22953, Loss: 1.146819829940796\n",
            "Epoch 3, Step 22954, Loss: 1.5923197269439697\n",
            "Epoch 3, Step 22955, Loss: 1.0716776847839355\n",
            "Epoch 3, Step 22956, Loss: 2.1596462726593018\n",
            "Epoch 3, Step 22957, Loss: 1.344147801399231\n",
            "Epoch 3, Step 22958, Loss: 1.2315627336502075\n",
            "Epoch 3, Step 22959, Loss: 1.6820052862167358\n",
            "Epoch 3, Step 22960, Loss: 0.5029813647270203\n",
            "Epoch 3, Step 22961, Loss: 0.5208137631416321\n",
            "Epoch 3, Step 22962, Loss: 1.2050201892852783\n",
            "Epoch 3, Step 22963, Loss: 1.2650233507156372\n",
            "Epoch 3, Step 22964, Loss: 0.7856114506721497\n",
            "Epoch 3, Step 22965, Loss: 0.3808418810367584\n",
            "Epoch 3, Step 22966, Loss: 2.4339354038238525\n",
            "Epoch 3, Step 22967, Loss: 2.9288406372070312\n",
            "Epoch 3, Step 22968, Loss: 1.4466609954833984\n",
            "Epoch 3, Step 22969, Loss: 0.9631260633468628\n",
            "Epoch 3, Step 22970, Loss: 2.141749143600464\n",
            "Epoch 3, Step 22971, Loss: 1.829136848449707\n",
            "Epoch 3, Step 22972, Loss: 1.6239782571792603\n",
            "Epoch 3, Step 22973, Loss: 2.4832308292388916\n",
            "Epoch 3, Step 22974, Loss: 2.835334300994873\n",
            "Epoch 3, Step 22975, Loss: 1.3568533658981323\n",
            "Epoch 3, Step 22976, Loss: 2.124351739883423\n",
            "Epoch 3, Step 22977, Loss: 1.0214054584503174\n",
            "Epoch 3, Step 22978, Loss: 1.9854528903961182\n",
            "Epoch 3, Step 22979, Loss: 0.8250341415405273\n",
            "Epoch 3, Step 22980, Loss: 1.418210506439209\n",
            "Epoch 3, Step 22981, Loss: 1.31174898147583\n",
            "Epoch 3, Step 22982, Loss: 1.384857177734375\n",
            "Epoch 3, Step 22983, Loss: 2.414213180541992\n",
            "Epoch 3, Step 22984, Loss: 0.755976140499115\n",
            "Epoch 3, Step 22985, Loss: 1.4767496585845947\n",
            "Epoch 3, Step 22986, Loss: 0.7634512186050415\n",
            "Epoch 3, Step 22987, Loss: 1.3861198425292969\n",
            "Epoch 3, Step 22988, Loss: 1.6329699754714966\n",
            "Epoch 3, Step 22989, Loss: 0.8535867929458618\n",
            "Epoch 3, Step 22990, Loss: 1.6783783435821533\n",
            "Epoch 3, Step 22991, Loss: 0.8304630517959595\n",
            "Epoch 3, Step 22992, Loss: 1.1353141069412231\n",
            "Epoch 3, Step 22993, Loss: 1.5310933589935303\n",
            "Epoch 3, Step 22994, Loss: 0.9097970128059387\n",
            "Epoch 3, Step 22995, Loss: 1.9325021505355835\n",
            "Epoch 3, Step 22996, Loss: 0.8054779171943665\n",
            "Epoch 3, Step 22997, Loss: 1.330834150314331\n",
            "Epoch 3, Step 22998, Loss: 1.4357514381408691\n",
            "Epoch 3, Step 22999, Loss: 0.9468936920166016\n",
            "Epoch 3, Step 23000, Loss: 0.7312837243080139\n",
            "Epoch 3, Step 23001, Loss: 0.5637383460998535\n",
            "Epoch 3, Step 23002, Loss: 1.3557474613189697\n",
            "Epoch 3, Step 23003, Loss: 1.526412844657898\n",
            "Epoch 3, Step 23004, Loss: 0.4862203598022461\n",
            "Epoch 3, Step 23005, Loss: 2.172508716583252\n",
            "Epoch 3, Step 23006, Loss: 0.8673312067985535\n",
            "Epoch 3, Step 23007, Loss: 1.8481078147888184\n",
            "Epoch 3, Step 23008, Loss: 1.3289456367492676\n",
            "Epoch 3, Step 23009, Loss: 2.10953426361084\n",
            "Epoch 3, Step 23010, Loss: 2.0508129596710205\n",
            "Epoch 3, Step 23011, Loss: 2.2812232971191406\n",
            "Epoch 3, Step 23012, Loss: 1.5470367670059204\n",
            "Epoch 3, Step 23013, Loss: 1.0380712747573853\n",
            "Epoch 3, Step 23014, Loss: 0.9138737916946411\n",
            "Epoch 3, Step 23015, Loss: 2.4823689460754395\n",
            "Epoch 3, Step 23016, Loss: 1.3609023094177246\n",
            "Epoch 3, Step 23017, Loss: 1.8699578046798706\n",
            "Epoch 3, Step 23018, Loss: 1.7998403310775757\n",
            "Epoch 3, Step 23019, Loss: 1.7226735353469849\n",
            "Epoch 3, Step 23020, Loss: 2.0544137954711914\n",
            "Epoch 3, Step 23021, Loss: 0.7185505628585815\n",
            "Epoch 3, Step 23022, Loss: 0.8245792388916016\n",
            "Epoch 3, Step 23023, Loss: 1.424808382987976\n",
            "Epoch 3, Step 23024, Loss: 1.9103912115097046\n",
            "Epoch 3, Step 23025, Loss: 1.6604629755020142\n",
            "Epoch 3, Step 23026, Loss: 1.7228087186813354\n",
            "Epoch 3, Step 23027, Loss: 1.0557093620300293\n",
            "Epoch 3, Step 23028, Loss: 1.193363070487976\n",
            "Epoch 3, Step 23029, Loss: 2.098167657852173\n",
            "Epoch 3, Step 23030, Loss: 1.9580167531967163\n",
            "Epoch 3, Step 23031, Loss: 2.5362863540649414\n",
            "Epoch 3, Step 23032, Loss: 0.5945280194282532\n",
            "Epoch 3, Step 23033, Loss: 1.683245301246643\n",
            "Epoch 3, Step 23034, Loss: 0.4311075210571289\n",
            "Epoch 3, Step 23035, Loss: 0.9114770889282227\n",
            "Epoch 3, Step 23036, Loss: 0.7026214599609375\n",
            "Epoch 3, Step 23037, Loss: 1.5131794214248657\n",
            "Epoch 3, Step 23038, Loss: 0.9522847533226013\n",
            "Epoch 3, Step 23039, Loss: 1.702057957649231\n",
            "Epoch 3, Step 23040, Loss: 0.6088937520980835\n",
            "Epoch 3, Step 23041, Loss: 1.84245765209198\n",
            "Epoch 3, Step 23042, Loss: 1.341172695159912\n",
            "Epoch 3, Step 23043, Loss: 1.2671997547149658\n",
            "Epoch 3, Step 23044, Loss: 1.785841941833496\n",
            "Epoch 3, Step 23045, Loss: 1.7927289009094238\n",
            "Epoch 3, Step 23046, Loss: 1.3817638158798218\n",
            "Epoch 3, Step 23047, Loss: 2.2599124908447266\n",
            "Epoch 3, Step 23048, Loss: 0.621355414390564\n",
            "Epoch 3, Step 23049, Loss: 1.150170922279358\n",
            "Epoch 3, Step 23050, Loss: 1.4336638450622559\n",
            "Epoch 3, Step 23051, Loss: 1.918925404548645\n",
            "Epoch 3, Step 23052, Loss: 1.3224049806594849\n",
            "Epoch 3, Step 23053, Loss: 2.312009572982788\n",
            "Epoch 3, Step 23054, Loss: 1.0129549503326416\n",
            "Epoch 3, Step 23055, Loss: 1.426873803138733\n",
            "Epoch 3, Step 23056, Loss: 2.066211700439453\n",
            "Epoch 3, Step 23057, Loss: 1.4231239557266235\n",
            "Epoch 3, Step 23058, Loss: 1.7249958515167236\n",
            "Epoch 3, Step 23059, Loss: 0.6040551066398621\n",
            "Epoch 3, Step 23060, Loss: 1.5510573387145996\n",
            "Epoch 3, Step 23061, Loss: 1.1103554964065552\n",
            "Epoch 3, Step 23062, Loss: 1.6059260368347168\n",
            "Epoch 3, Step 23063, Loss: 1.1115458011627197\n",
            "Epoch 3, Step 23064, Loss: 0.9685519337654114\n",
            "Epoch 3, Step 23065, Loss: 1.7125266790390015\n",
            "Epoch 3, Step 23066, Loss: 2.6435701847076416\n",
            "Epoch 3, Step 23067, Loss: 1.5150413513183594\n",
            "Epoch 3, Step 23068, Loss: 1.5707876682281494\n",
            "Epoch 3, Step 23069, Loss: 1.2970768213272095\n",
            "Epoch 3, Step 23070, Loss: 1.8808505535125732\n",
            "Epoch 3, Step 23071, Loss: 2.043921947479248\n",
            "Epoch 3, Step 23072, Loss: 2.090088367462158\n",
            "Epoch 3, Step 23073, Loss: 1.2268542051315308\n",
            "Epoch 3, Step 23074, Loss: 2.1329619884490967\n",
            "Epoch 3, Step 23075, Loss: 1.7496373653411865\n",
            "Epoch 3, Step 23076, Loss: 1.4433125257492065\n",
            "Epoch 3, Step 23077, Loss: 0.9318607449531555\n",
            "Epoch 3, Step 23078, Loss: 0.6505978107452393\n",
            "Epoch 3, Step 23079, Loss: 1.31535804271698\n",
            "Epoch 3, Step 23080, Loss: 2.688349485397339\n",
            "Epoch 3, Step 23081, Loss: 2.090590476989746\n",
            "Epoch 3, Step 23082, Loss: 2.4819846153259277\n",
            "Epoch 3, Step 23083, Loss: 2.184570074081421\n",
            "Epoch 3, Step 23084, Loss: 1.4778321981430054\n",
            "Epoch 3, Step 23085, Loss: 2.4347074031829834\n",
            "Epoch 3, Step 23086, Loss: 1.6546846628189087\n",
            "Epoch 3, Step 23087, Loss: 1.5035723447799683\n",
            "Epoch 3, Step 23088, Loss: 1.7351325750350952\n",
            "Epoch 3, Step 23089, Loss: 2.3342113494873047\n",
            "Epoch 3, Step 23090, Loss: 1.1279897689819336\n",
            "Epoch 3, Step 23091, Loss: 1.062312126159668\n",
            "Epoch 3, Step 23092, Loss: 1.1360204219818115\n",
            "Epoch 3, Step 23093, Loss: 1.2483892440795898\n",
            "Epoch 3, Step 23094, Loss: 1.2901532649993896\n",
            "Epoch 3, Step 23095, Loss: 1.260705590248108\n",
            "Epoch 3, Step 23096, Loss: 2.2742865085601807\n",
            "Epoch 3, Step 23097, Loss: 2.3049614429473877\n",
            "Epoch 3, Step 23098, Loss: 1.6382488012313843\n",
            "Epoch 3, Step 23099, Loss: 1.1984392404556274\n",
            "Epoch 3, Step 23100, Loss: 2.1605470180511475\n",
            "Epoch 3, Step 23101, Loss: 1.4733086824417114\n",
            "Epoch 3, Step 23102, Loss: 0.51682049036026\n",
            "Epoch 3, Step 23103, Loss: 2.4720942974090576\n",
            "Epoch 3, Step 23104, Loss: 0.9353533387184143\n",
            "Epoch 3, Step 23105, Loss: 1.8971186876296997\n",
            "Epoch 3, Step 23106, Loss: 1.1982970237731934\n",
            "Epoch 3, Step 23107, Loss: 1.8782707452774048\n",
            "Epoch 3, Step 23108, Loss: 3.0248336791992188\n",
            "Epoch 3, Step 23109, Loss: 2.0724761486053467\n",
            "Epoch 3, Step 23110, Loss: 1.885514736175537\n",
            "Epoch 3, Step 23111, Loss: 1.5850259065628052\n",
            "Epoch 3, Step 23112, Loss: 1.5880299806594849\n",
            "Epoch 3, Step 23113, Loss: 1.331747055053711\n",
            "Epoch 3, Step 23114, Loss: 1.315953016281128\n",
            "Epoch 3, Step 23115, Loss: 2.6809089183807373\n",
            "Epoch 3, Step 23116, Loss: 1.0955883264541626\n",
            "Epoch 3, Step 23117, Loss: 1.463623046875\n",
            "Epoch 3, Step 23118, Loss: 2.096271514892578\n",
            "Epoch 3, Step 23119, Loss: 1.047202467918396\n",
            "Epoch 3, Step 23120, Loss: 0.8608661890029907\n",
            "Epoch 3, Step 23121, Loss: 0.8589473962783813\n",
            "Epoch 3, Step 23122, Loss: 1.436126470565796\n",
            "Epoch 3, Step 23123, Loss: 0.8841794729232788\n",
            "Epoch 3, Step 23124, Loss: 1.459223747253418\n",
            "Epoch 3, Step 23125, Loss: 0.8650341033935547\n",
            "Epoch 3, Step 23126, Loss: 2.099733829498291\n",
            "Epoch 3, Step 23127, Loss: 1.59352445602417\n",
            "Epoch 3, Step 23128, Loss: 1.851007103919983\n",
            "Epoch 3, Step 23129, Loss: 0.5639870166778564\n",
            "Epoch 3, Step 23130, Loss: 1.8282700777053833\n",
            "Epoch 3, Step 23131, Loss: 1.2597042322158813\n",
            "Epoch 3, Step 23132, Loss: 1.9819703102111816\n",
            "Epoch 3, Step 23133, Loss: 2.4247310161590576\n",
            "Epoch 3, Step 23134, Loss: 2.076352834701538\n",
            "Epoch 3, Step 23135, Loss: 1.0420951843261719\n",
            "Epoch 3, Step 23136, Loss: 1.1386908292770386\n",
            "Epoch 3, Step 23137, Loss: 0.447500079870224\n",
            "Epoch 3, Step 23138, Loss: 2.292865753173828\n",
            "Epoch 3, Step 23139, Loss: 1.643751621246338\n",
            "Epoch 3, Step 23140, Loss: 2.0557312965393066\n",
            "Epoch 3, Step 23141, Loss: 2.032627582550049\n",
            "Epoch 3, Step 23142, Loss: 1.2273621559143066\n",
            "Epoch 3, Step 23143, Loss: 1.5725246667861938\n",
            "Epoch 3, Step 23144, Loss: 1.1883647441864014\n",
            "Epoch 3, Step 23145, Loss: 2.3976316452026367\n",
            "Epoch 3, Step 23146, Loss: 1.3701130151748657\n",
            "Epoch 3, Step 23147, Loss: 2.044562339782715\n",
            "Epoch 3, Step 23148, Loss: 1.361322283744812\n",
            "Epoch 3, Step 23149, Loss: 1.2838224172592163\n",
            "Epoch 3, Step 23150, Loss: 2.0674121379852295\n",
            "Epoch 3, Step 23151, Loss: 2.009618043899536\n",
            "Epoch 3, Step 23152, Loss: 0.9141959547996521\n",
            "Epoch 3, Step 23153, Loss: 0.7945771217346191\n",
            "Epoch 3, Step 23154, Loss: 1.080458402633667\n",
            "Epoch 3, Step 23155, Loss: 0.4823780357837677\n",
            "Epoch 3, Step 23156, Loss: 1.4194436073303223\n",
            "Epoch 3, Step 23157, Loss: 2.266420364379883\n",
            "Epoch 3, Step 23158, Loss: 1.7946711778640747\n",
            "Epoch 3, Step 23159, Loss: 1.7965431213378906\n",
            "Epoch 3, Step 23160, Loss: 1.2215090990066528\n",
            "Epoch 3, Step 23161, Loss: 1.3121031522750854\n",
            "Epoch 3, Step 23162, Loss: 2.4004967212677\n",
            "Epoch 3, Step 23163, Loss: 1.4602348804473877\n",
            "Epoch 3, Step 23164, Loss: 1.207372784614563\n",
            "Epoch 3, Step 23165, Loss: 2.102965831756592\n",
            "Epoch 3, Step 23166, Loss: 0.9333973526954651\n",
            "Epoch 3, Step 23167, Loss: 1.3999972343444824\n",
            "Epoch 3, Step 23168, Loss: 2.276381731033325\n",
            "Epoch 3, Step 23169, Loss: 0.788359522819519\n",
            "Epoch 3, Step 23170, Loss: 0.7484288811683655\n",
            "Epoch 3, Step 23171, Loss: 1.9990206956863403\n",
            "Epoch 3, Step 23172, Loss: 1.561306357383728\n",
            "Epoch 3, Step 23173, Loss: 1.5822153091430664\n",
            "Epoch 3, Step 23174, Loss: 0.9432727098464966\n",
            "Epoch 3, Step 23175, Loss: 2.0376856327056885\n",
            "Epoch 3, Step 23176, Loss: 1.762397050857544\n",
            "Epoch 3, Step 23177, Loss: 1.7433208227157593\n",
            "Epoch 3, Step 23178, Loss: 1.1732336282730103\n",
            "Epoch 3, Step 23179, Loss: 1.4610261917114258\n",
            "Epoch 3, Step 23180, Loss: 2.1528542041778564\n",
            "Epoch 3, Step 23181, Loss: 1.6731752157211304\n",
            "Epoch 3, Step 23182, Loss: 1.5742822885513306\n",
            "Epoch 3, Step 23183, Loss: 2.6753029823303223\n",
            "Epoch 3, Step 23184, Loss: 0.9249105453491211\n",
            "Epoch 3, Step 23185, Loss: 1.7165749073028564\n",
            "Epoch 3, Step 23186, Loss: 0.627856969833374\n",
            "Epoch 3, Step 23187, Loss: 1.9127203226089478\n",
            "Epoch 3, Step 23188, Loss: 1.0279836654663086\n",
            "Epoch 3, Step 23189, Loss: 2.8422341346740723\n",
            "Epoch 3, Step 23190, Loss: 2.056934118270874\n",
            "Epoch 3, Step 23191, Loss: 1.70586359500885\n",
            "Epoch 3, Step 23192, Loss: 1.9018311500549316\n",
            "Epoch 3, Step 23193, Loss: 0.8620890378952026\n",
            "Epoch 3, Step 23194, Loss: 0.5287231206893921\n",
            "Epoch 3, Step 23195, Loss: 0.8362940549850464\n",
            "Epoch 3, Step 23196, Loss: 1.1132121086120605\n",
            "Epoch 3, Step 23197, Loss: 0.6363996267318726\n",
            "Epoch 3, Step 23198, Loss: 1.1375062465667725\n",
            "Epoch 3, Step 23199, Loss: 1.070729374885559\n",
            "Epoch 3, Step 23200, Loss: 2.599067449569702\n",
            "Epoch 3, Step 23201, Loss: 0.9161372184753418\n",
            "Epoch 3, Step 23202, Loss: 0.5756306052207947\n",
            "Epoch 3, Step 23203, Loss: 2.128741979598999\n",
            "Epoch 3, Step 23204, Loss: 2.2234156131744385\n",
            "Epoch 3, Step 23205, Loss: 0.6672559380531311\n",
            "Epoch 3, Step 23206, Loss: 1.1607362031936646\n",
            "Epoch 3, Step 23207, Loss: 1.9364070892333984\n",
            "Epoch 3, Step 23208, Loss: 2.1809895038604736\n",
            "Epoch 3, Step 23209, Loss: 1.4109549522399902\n",
            "Epoch 3, Step 23210, Loss: 1.1596488952636719\n",
            "Epoch 3, Step 23211, Loss: 1.147186517715454\n",
            "Epoch 3, Step 23212, Loss: 1.2791938781738281\n",
            "Epoch 3, Step 23213, Loss: 1.4222089052200317\n",
            "Epoch 3, Step 23214, Loss: 1.4130396842956543\n",
            "Epoch 3, Step 23215, Loss: 2.690380811691284\n",
            "Epoch 3, Step 23216, Loss: 1.7684768438339233\n",
            "Epoch 3, Step 23217, Loss: 2.0685603618621826\n",
            "Epoch 3, Step 23218, Loss: 1.9302966594696045\n",
            "Epoch 3, Step 23219, Loss: 2.1044511795043945\n",
            "Epoch 3, Step 23220, Loss: 1.8777680397033691\n",
            "Epoch 3, Step 23221, Loss: 1.0540802478790283\n",
            "Epoch 3, Step 23222, Loss: 0.9336556196212769\n",
            "Epoch 3, Step 23223, Loss: 0.45007044076919556\n",
            "Epoch 3, Step 23224, Loss: 2.543255567550659\n",
            "Epoch 3, Step 23225, Loss: 1.9500880241394043\n",
            "Epoch 3, Step 23226, Loss: 1.6420860290527344\n",
            "Epoch 3, Step 23227, Loss: 1.3315956592559814\n",
            "Epoch 3, Step 23228, Loss: 1.4980603456497192\n",
            "Epoch 3, Step 23229, Loss: 0.637519896030426\n",
            "Epoch 3, Step 23230, Loss: 2.8690686225891113\n",
            "Epoch 3, Step 23231, Loss: 0.6931564807891846\n",
            "Epoch 3, Step 23232, Loss: 0.49317020177841187\n",
            "Epoch 3, Step 23233, Loss: 0.9501326680183411\n",
            "Epoch 3, Step 23234, Loss: 0.8088771104812622\n",
            "Epoch 3, Step 23235, Loss: 1.5436830520629883\n",
            "Epoch 3, Step 23236, Loss: 1.124455213546753\n",
            "Epoch 3, Step 23237, Loss: 1.8263773918151855\n",
            "Epoch 3, Step 23238, Loss: 1.2816439867019653\n",
            "Epoch 3, Step 23239, Loss: 1.4946722984313965\n",
            "Epoch 3, Step 23240, Loss: 2.972203493118286\n",
            "Epoch 3, Step 23241, Loss: 0.5770173072814941\n",
            "Epoch 3, Step 23242, Loss: 0.7009701132774353\n",
            "Epoch 3, Step 23243, Loss: 1.476542353630066\n",
            "Epoch 3, Step 23244, Loss: 2.820117950439453\n",
            "Epoch 3, Step 23245, Loss: 1.2459275722503662\n",
            "Epoch 3, Step 23246, Loss: 2.2938759326934814\n",
            "Epoch 3, Step 23247, Loss: 1.365481972694397\n",
            "Epoch 3, Step 23248, Loss: 1.5580098628997803\n",
            "Epoch 3, Step 23249, Loss: 1.4326083660125732\n",
            "Epoch 3, Step 23250, Loss: 1.6425663232803345\n",
            "Epoch 3, Step 23251, Loss: 1.4176145792007446\n",
            "Epoch 3, Step 23252, Loss: 1.300521969795227\n",
            "Epoch 3, Step 23253, Loss: 2.183457374572754\n",
            "Epoch 3, Step 23254, Loss: 1.8524972200393677\n",
            "Epoch 3, Step 23255, Loss: 1.4793004989624023\n",
            "Epoch 3, Step 23256, Loss: 1.7858197689056396\n",
            "Epoch 3, Step 23257, Loss: 1.3553950786590576\n",
            "Epoch 3, Step 23258, Loss: 1.053547739982605\n",
            "Epoch 3, Step 23259, Loss: 1.4132659435272217\n",
            "Epoch 3, Step 23260, Loss: 1.619253396987915\n",
            "Epoch 3, Step 23261, Loss: 1.5659563541412354\n",
            "Epoch 3, Step 23262, Loss: 1.4288005828857422\n",
            "Epoch 3, Step 23263, Loss: 2.03303599357605\n",
            "Epoch 3, Step 23264, Loss: 1.5650756359100342\n",
            "Epoch 3, Step 23265, Loss: 1.6137897968292236\n",
            "Epoch 3, Step 23266, Loss: 1.5481557846069336\n",
            "Epoch 3, Step 23267, Loss: 1.398031234741211\n",
            "Epoch 3, Step 23268, Loss: 0.7374104261398315\n",
            "Epoch 3, Step 23269, Loss: 0.9550466537475586\n",
            "Epoch 3, Step 23270, Loss: 1.0942586660385132\n",
            "Epoch 3, Step 23271, Loss: 2.036095380783081\n",
            "Epoch 3, Step 23272, Loss: 1.4346431493759155\n",
            "Epoch 3, Step 23273, Loss: 2.438758134841919\n",
            "Epoch 3, Step 23274, Loss: 1.6572074890136719\n",
            "Epoch 3, Step 23275, Loss: 1.701287031173706\n",
            "Epoch 3, Step 23276, Loss: 1.9446849822998047\n",
            "Epoch 3, Step 23277, Loss: 1.1689635515213013\n",
            "Epoch 3, Step 23278, Loss: 2.467322587966919\n",
            "Epoch 3, Step 23279, Loss: 1.2013460397720337\n",
            "Epoch 3, Step 23280, Loss: 2.0800156593322754\n",
            "Epoch 3, Step 23281, Loss: 1.2396821975708008\n",
            "Epoch 3, Step 23282, Loss: 1.2918214797973633\n",
            "Epoch 3, Step 23283, Loss: 0.9557816386222839\n",
            "Epoch 3, Step 23284, Loss: 1.3505196571350098\n",
            "Epoch 3, Step 23285, Loss: 2.16880464553833\n",
            "Epoch 3, Step 23286, Loss: 1.5278925895690918\n",
            "Epoch 3, Step 23287, Loss: 1.9553085565567017\n",
            "Epoch 3, Step 23288, Loss: 1.3482728004455566\n",
            "Epoch 3, Step 23289, Loss: 2.1147212982177734\n",
            "Epoch 3, Step 23290, Loss: 1.0089815855026245\n",
            "Epoch 3, Step 23291, Loss: 2.343723773956299\n",
            "Epoch 3, Step 23292, Loss: 1.9161220788955688\n",
            "Epoch 3, Step 23293, Loss: 0.7046238780021667\n",
            "Epoch 3, Step 23294, Loss: 1.9982786178588867\n",
            "Epoch 3, Step 23295, Loss: 0.812039852142334\n",
            "Epoch 3, Step 23296, Loss: 1.906365990638733\n",
            "Epoch 3, Step 23297, Loss: 1.882767915725708\n",
            "Epoch 3, Step 23298, Loss: 0.754270613193512\n",
            "Epoch 3, Step 23299, Loss: 2.251784086227417\n",
            "Epoch 3, Step 23300, Loss: 1.445824146270752\n",
            "Epoch 3, Step 23301, Loss: 2.311138391494751\n",
            "Epoch 3, Step 23302, Loss: 2.1429800987243652\n",
            "Epoch 3, Step 23303, Loss: 1.5250487327575684\n",
            "Epoch 3, Step 23304, Loss: 1.841709017753601\n",
            "Epoch 3, Step 23305, Loss: 2.4139857292175293\n",
            "Epoch 3, Step 23306, Loss: 2.7120473384857178\n",
            "Epoch 3, Step 23307, Loss: 1.1762356758117676\n",
            "Epoch 3, Step 23308, Loss: 1.5438014268875122\n",
            "Epoch 3, Step 23309, Loss: 1.3832955360412598\n",
            "Epoch 3, Step 23310, Loss: 1.2074347734451294\n",
            "Epoch 3, Step 23311, Loss: 1.3032097816467285\n",
            "Epoch 3, Step 23312, Loss: 0.5165206789970398\n",
            "Epoch 3, Step 23313, Loss: 1.7501882314682007\n",
            "Epoch 3, Step 23314, Loss: 2.79148268699646\n",
            "Epoch 3, Step 23315, Loss: 1.293631672859192\n",
            "Epoch 3, Step 23316, Loss: 1.8858134746551514\n",
            "Epoch 3, Step 23317, Loss: 1.140194058418274\n",
            "Epoch 3, Step 23318, Loss: 1.4215749502182007\n",
            "Epoch 3, Step 23319, Loss: 1.953341007232666\n",
            "Epoch 3, Step 23320, Loss: 2.2257001399993896\n",
            "Epoch 3, Step 23321, Loss: 1.550918459892273\n",
            "Epoch 3, Step 23322, Loss: 3.033975124359131\n",
            "Epoch 3, Step 23323, Loss: 1.8776257038116455\n",
            "Epoch 3, Step 23324, Loss: 1.8113188743591309\n",
            "Epoch 3, Step 23325, Loss: 1.8165783882141113\n",
            "Epoch 3, Step 23326, Loss: 1.5646308660507202\n",
            "Epoch 3, Step 23327, Loss: 1.5695173740386963\n",
            "Epoch 3, Step 23328, Loss: 1.146521806716919\n",
            "Epoch 3, Step 23329, Loss: 0.9724752902984619\n",
            "Epoch 3, Step 23330, Loss: 0.7423247694969177\n",
            "Epoch 3, Step 23331, Loss: 1.6900343894958496\n",
            "Epoch 3, Step 23332, Loss: 1.3379127979278564\n",
            "Epoch 3, Step 23333, Loss: 0.9878020286560059\n",
            "Epoch 3, Step 23334, Loss: 0.647221565246582\n",
            "Epoch 3, Step 23335, Loss: 1.0365999937057495\n",
            "Epoch 3, Step 23336, Loss: 1.4699445962905884\n",
            "Epoch 3, Step 23337, Loss: 1.8274236917495728\n",
            "Epoch 3, Step 23338, Loss: 1.9150004386901855\n",
            "Epoch 3, Step 23339, Loss: 1.3452420234680176\n",
            "Epoch 3, Step 23340, Loss: 2.314523696899414\n",
            "Epoch 3, Step 23341, Loss: 0.5509462356567383\n",
            "Epoch 3, Step 23342, Loss: 1.7672488689422607\n",
            "Epoch 3, Step 23343, Loss: 1.4278144836425781\n",
            "Epoch 3, Step 23344, Loss: 0.544095516204834\n",
            "Epoch 3, Step 23345, Loss: 2.196709156036377\n",
            "Epoch 3, Step 23346, Loss: 0.6012265086174011\n",
            "Epoch 3, Step 23347, Loss: 1.1142810583114624\n",
            "Epoch 3, Step 23348, Loss: 1.046165943145752\n",
            "Epoch 3, Step 23349, Loss: 1.823102355003357\n",
            "Epoch 3, Step 23350, Loss: 1.7042731046676636\n",
            "Epoch 3, Step 23351, Loss: 1.6879202127456665\n",
            "Epoch 3, Step 23352, Loss: 1.6215895414352417\n",
            "Epoch 3, Step 23353, Loss: 1.4241445064544678\n",
            "Epoch 3, Step 23354, Loss: 1.2665778398513794\n",
            "Epoch 3, Step 23355, Loss: 1.83815598487854\n",
            "Epoch 3, Step 23356, Loss: 2.380856990814209\n",
            "Epoch 3, Step 23357, Loss: 1.1120409965515137\n",
            "Epoch 3, Step 23358, Loss: 0.5914055705070496\n",
            "Epoch 3, Step 23359, Loss: 2.1768555641174316\n",
            "Epoch 3, Step 23360, Loss: 1.2518051862716675\n",
            "Epoch 3, Step 23361, Loss: 0.8582598567008972\n",
            "Epoch 3, Step 23362, Loss: 0.9767624735832214\n",
            "Epoch 3, Step 23363, Loss: 0.45553427934646606\n",
            "Epoch 3, Step 23364, Loss: 1.4097665548324585\n",
            "Epoch 3, Step 23365, Loss: 1.8996256589889526\n",
            "Epoch 3, Step 23366, Loss: 1.1503384113311768\n",
            "Epoch 3, Step 23367, Loss: 1.518632411956787\n",
            "Epoch 3, Step 23368, Loss: 0.7639741897583008\n",
            "Epoch 3, Step 23369, Loss: 2.0081729888916016\n",
            "Epoch 3, Step 23370, Loss: 1.3337996006011963\n",
            "Epoch 3, Step 23371, Loss: 1.6277188062667847\n",
            "Epoch 3, Step 23372, Loss: 0.5424162745475769\n",
            "Epoch 3, Step 23373, Loss: 1.2440134286880493\n",
            "Epoch 3, Step 23374, Loss: 1.010264277458191\n",
            "Epoch 3, Step 23375, Loss: 2.3916397094726562\n",
            "Epoch 3, Step 23376, Loss: 2.1070003509521484\n",
            "Epoch 3, Step 23377, Loss: 2.712751865386963\n",
            "Epoch 3, Step 23378, Loss: 1.9534791707992554\n",
            "Epoch 3, Step 23379, Loss: 0.5824617147445679\n",
            "Epoch 3, Step 23380, Loss: 1.6743465662002563\n",
            "Epoch 3, Step 23381, Loss: 1.8973034620285034\n",
            "Epoch 3, Step 23382, Loss: 2.4860379695892334\n",
            "Epoch 3, Step 23383, Loss: 2.372223377227783\n",
            "Epoch 3, Step 23384, Loss: 1.1930187940597534\n",
            "Epoch 3, Step 23385, Loss: 1.0515516996383667\n",
            "Epoch 3, Step 23386, Loss: 0.6622538566589355\n",
            "Epoch 3, Step 23387, Loss: 1.2280739545822144\n",
            "Epoch 3, Step 23388, Loss: 2.10335373878479\n",
            "Epoch 3, Step 23389, Loss: 2.6689348220825195\n",
            "Epoch 3, Step 23390, Loss: 1.8977621793746948\n",
            "Epoch 3, Step 23391, Loss: 0.6745700240135193\n",
            "Epoch 3, Step 23392, Loss: 1.7702698707580566\n",
            "Epoch 3, Step 23393, Loss: 0.7436839938163757\n",
            "Epoch 3, Step 23394, Loss: 1.5777151584625244\n",
            "Epoch 3, Step 23395, Loss: 1.694498062133789\n",
            "Epoch 3, Step 23396, Loss: 1.0291200876235962\n",
            "Epoch 3, Step 23397, Loss: 0.8139143586158752\n",
            "Epoch 3, Step 23398, Loss: 1.5434930324554443\n",
            "Epoch 3, Step 23399, Loss: 1.8619745969772339\n",
            "Epoch 3, Step 23400, Loss: 1.4806809425354004\n",
            "Epoch 3, Step 23401, Loss: 1.608595609664917\n",
            "Epoch 3, Step 23402, Loss: 1.5225640535354614\n",
            "Epoch 3, Step 23403, Loss: 1.840886116027832\n",
            "Epoch 3, Step 23404, Loss: 1.593546748161316\n",
            "Epoch 3, Step 23405, Loss: 0.5950202345848083\n",
            "Epoch 3, Step 23406, Loss: 1.6349455118179321\n",
            "Epoch 3, Step 23407, Loss: 1.6311700344085693\n",
            "Epoch 3, Step 23408, Loss: 1.778348445892334\n",
            "Epoch 3, Step 23409, Loss: 1.6010277271270752\n",
            "Epoch 3, Step 23410, Loss: 0.697155773639679\n",
            "Epoch 3, Step 23411, Loss: 1.148231029510498\n",
            "Epoch 3, Step 23412, Loss: 1.3427600860595703\n",
            "Epoch 3, Step 23413, Loss: 1.092520833015442\n",
            "Epoch 3, Step 23414, Loss: 0.8697687983512878\n",
            "Epoch 3, Step 23415, Loss: 1.815255880355835\n",
            "Epoch 3, Step 23416, Loss: 0.5267658233642578\n",
            "Epoch 3, Step 23417, Loss: 1.2105106115341187\n",
            "Epoch 3, Step 23418, Loss: 1.7793928384780884\n",
            "Epoch 3, Step 23419, Loss: 0.917716920375824\n",
            "Epoch 3, Step 23420, Loss: 1.5362939834594727\n",
            "Epoch 3, Step 23421, Loss: 2.093297004699707\n",
            "Epoch 3, Step 23422, Loss: 1.6409392356872559\n",
            "Epoch 3, Step 23423, Loss: 1.468182921409607\n",
            "Epoch 3, Step 23424, Loss: 2.579458475112915\n",
            "Epoch 3, Step 23425, Loss: 0.7651402354240417\n",
            "Epoch 3, Step 23426, Loss: 1.5325736999511719\n",
            "Epoch 3, Step 23427, Loss: 1.4670661687850952\n",
            "Epoch 3, Step 23428, Loss: 1.2082418203353882\n",
            "Epoch 3, Step 23429, Loss: 1.8572055101394653\n",
            "Epoch 3, Step 23430, Loss: 1.4904966354370117\n",
            "Epoch 3, Step 23431, Loss: 1.3241455554962158\n",
            "Epoch 3, Step 23432, Loss: 1.409013271331787\n",
            "Epoch 3, Step 23433, Loss: 1.4172171354293823\n",
            "Epoch 3, Step 23434, Loss: 1.572054147720337\n",
            "Epoch 3, Step 23435, Loss: 1.0834892988204956\n",
            "Epoch 3, Step 23436, Loss: 1.1616960763931274\n",
            "Epoch 3, Step 23437, Loss: 1.575042486190796\n",
            "Epoch 3, Step 23438, Loss: 1.3400150537490845\n",
            "Epoch 3, Step 23439, Loss: 1.202399492263794\n",
            "Epoch 3, Step 23440, Loss: 1.68753182888031\n",
            "Epoch 3, Step 23441, Loss: 1.5621886253356934\n",
            "Epoch 3, Step 23442, Loss: 1.756495475769043\n",
            "Epoch 3, Step 23443, Loss: 1.5803861618041992\n",
            "Epoch 3, Step 23444, Loss: 1.1851571798324585\n",
            "Epoch 3, Step 23445, Loss: 1.1578991413116455\n",
            "Epoch 3, Step 23446, Loss: 1.7024208307266235\n",
            "Epoch 3, Step 23447, Loss: 1.4526269435882568\n",
            "Epoch 3, Step 23448, Loss: 1.8289215564727783\n",
            "Epoch 3, Step 23449, Loss: 1.1031131744384766\n",
            "Epoch 3, Step 23450, Loss: 1.176466941833496\n",
            "Epoch 3, Step 23451, Loss: 1.5587363243103027\n",
            "Epoch 3, Step 23452, Loss: 1.975016474723816\n",
            "Epoch 3, Step 23453, Loss: 2.045135259628296\n",
            "Epoch 3, Step 23454, Loss: 1.4522428512573242\n",
            "Epoch 3, Step 23455, Loss: 0.505962610244751\n",
            "Epoch 3, Step 23456, Loss: 1.969208836555481\n",
            "Epoch 3, Step 23457, Loss: 0.40371888875961304\n",
            "Epoch 3, Step 23458, Loss: 1.8432387113571167\n",
            "Epoch 3, Step 23459, Loss: 1.2668375968933105\n",
            "Epoch 3, Step 23460, Loss: 2.004077434539795\n",
            "Epoch 3, Step 23461, Loss: 1.4148355722427368\n",
            "Epoch 3, Step 23462, Loss: 1.3672666549682617\n",
            "Epoch 3, Step 23463, Loss: 1.887186050415039\n",
            "Epoch 3, Step 23464, Loss: 2.3529422283172607\n",
            "Epoch 3, Step 23465, Loss: 1.3916573524475098\n",
            "Epoch 3, Step 23466, Loss: 2.258380651473999\n",
            "Epoch 3, Step 23467, Loss: 1.8602783679962158\n",
            "Epoch 3, Step 23468, Loss: 1.7223869562149048\n",
            "Epoch 3, Step 23469, Loss: 1.5209425687789917\n",
            "Epoch 3, Step 23470, Loss: 1.1675747632980347\n",
            "Epoch 3, Step 23471, Loss: 1.500676155090332\n",
            "Epoch 3, Step 23472, Loss: 1.0752830505371094\n",
            "Epoch 3, Step 23473, Loss: 0.8058954477310181\n",
            "Epoch 3, Step 23474, Loss: 2.1226754188537598\n",
            "Epoch 3, Step 23475, Loss: 1.230789303779602\n",
            "Epoch 3, Step 23476, Loss: 1.513672947883606\n",
            "Epoch 3, Step 23477, Loss: 1.4469767808914185\n",
            "Epoch 3, Step 23478, Loss: 1.567463755607605\n",
            "Epoch 3, Step 23479, Loss: 2.2102925777435303\n",
            "Epoch 3, Step 23480, Loss: 1.4650872945785522\n",
            "Epoch 3, Step 23481, Loss: 1.5392073392868042\n",
            "Epoch 3, Step 23482, Loss: 1.4296890497207642\n",
            "Epoch 3, Step 23483, Loss: 1.8262263536453247\n",
            "Epoch 3, Step 23484, Loss: 1.5752613544464111\n",
            "Epoch 3, Step 23485, Loss: 2.502620220184326\n",
            "Epoch 3, Step 23486, Loss: 1.5872362852096558\n",
            "Epoch 3, Step 23487, Loss: 2.001624345779419\n",
            "Epoch 3, Step 23488, Loss: 2.7943007946014404\n",
            "Epoch 3, Step 23489, Loss: 2.1294727325439453\n",
            "Epoch 3, Step 23490, Loss: 0.4324466288089752\n",
            "Epoch 3, Step 23491, Loss: 0.5936026573181152\n",
            "Epoch 3, Step 23492, Loss: 1.4454160928726196\n",
            "Epoch 3, Step 23493, Loss: 2.1290202140808105\n",
            "Epoch 3, Step 23494, Loss: 1.271837830543518\n",
            "Epoch 3, Step 23495, Loss: 1.2053022384643555\n",
            "Epoch 3, Step 23496, Loss: 1.8931607007980347\n",
            "Epoch 3, Step 23497, Loss: 2.2394185066223145\n",
            "Epoch 3, Step 23498, Loss: 1.4320306777954102\n",
            "Epoch 3, Step 23499, Loss: 1.4001212120056152\n",
            "Epoch 3, Step 23500, Loss: 0.5161615610122681\n",
            "Epoch 3, Step 23501, Loss: 1.400659441947937\n",
            "Epoch 3, Step 23502, Loss: 1.8103054761886597\n",
            "Epoch 3, Step 23503, Loss: 1.851394534111023\n",
            "Epoch 3, Step 23504, Loss: 1.354381799697876\n",
            "Epoch 3, Step 23505, Loss: 1.027549386024475\n",
            "Epoch 3, Step 23506, Loss: 1.7455682754516602\n",
            "Epoch 3, Step 23507, Loss: 0.9918980002403259\n",
            "Epoch 3, Step 23508, Loss: 1.1341038942337036\n",
            "Epoch 3, Step 23509, Loss: 1.4975045919418335\n",
            "Epoch 3, Step 23510, Loss: 0.7564396262168884\n",
            "Epoch 3, Step 23511, Loss: 1.638377070426941\n",
            "Epoch 3, Step 23512, Loss: 1.2989503145217896\n",
            "Epoch 3, Step 23513, Loss: 1.3598651885986328\n",
            "Epoch 3, Step 23514, Loss: 2.1506357192993164\n",
            "Epoch 3, Step 23515, Loss: 2.051755666732788\n",
            "Epoch 3, Step 23516, Loss: 1.8195958137512207\n",
            "Epoch 3, Step 23517, Loss: 2.0067343711853027\n",
            "Epoch 3, Step 23518, Loss: 1.721927285194397\n",
            "Epoch 3, Step 23519, Loss: 0.9615016579627991\n",
            "Epoch 3, Step 23520, Loss: 1.372780680656433\n",
            "Epoch 3, Step 23521, Loss: 2.292992353439331\n",
            "Epoch 3, Step 23522, Loss: 2.220871686935425\n",
            "Epoch 3, Step 23523, Loss: 1.0626840591430664\n",
            "Epoch 3, Step 23524, Loss: 2.224156379699707\n",
            "Epoch 3, Step 23525, Loss: 1.2686833143234253\n",
            "Epoch 3, Step 23526, Loss: 1.105231761932373\n",
            "Epoch 3, Step 23527, Loss: 2.17972993850708\n",
            "Epoch 3, Step 23528, Loss: 0.9475992918014526\n",
            "Epoch 3, Step 23529, Loss: 2.313800573348999\n",
            "Epoch 3, Step 23530, Loss: 1.5064644813537598\n",
            "Epoch 3, Step 23531, Loss: 1.3330622911453247\n",
            "Epoch 3, Step 23532, Loss: 1.9100360870361328\n",
            "Epoch 3, Step 23533, Loss: 1.7838886976242065\n",
            "Epoch 3, Step 23534, Loss: 1.254169225692749\n",
            "Epoch 3, Step 23535, Loss: 1.0207860469818115\n",
            "Epoch 3, Step 23536, Loss: 1.22689688205719\n",
            "Epoch 3, Step 23537, Loss: 0.5902097821235657\n",
            "Epoch 3, Step 23538, Loss: 1.9810034036636353\n",
            "Epoch 3, Step 23539, Loss: 0.7314059734344482\n",
            "Epoch 3, Step 23540, Loss: 0.35686638951301575\n",
            "Epoch 3, Step 23541, Loss: 1.3411072492599487\n",
            "Epoch 3, Step 23542, Loss: 0.7072267532348633\n",
            "Epoch 3, Step 23543, Loss: 1.5825245380401611\n",
            "Epoch 3, Step 23544, Loss: 1.8701963424682617\n",
            "Epoch 3, Step 23545, Loss: 1.7939419746398926\n",
            "Epoch 3, Step 23546, Loss: 1.386616826057434\n",
            "Epoch 3, Step 23547, Loss: 1.2556259632110596\n",
            "Epoch 3, Step 23548, Loss: 1.9660495519638062\n",
            "Epoch 3, Step 23549, Loss: 0.9989873766899109\n",
            "Epoch 3, Step 23550, Loss: 0.5916983485221863\n",
            "Epoch 3, Step 23551, Loss: 1.3687052726745605\n",
            "Epoch 3, Step 23552, Loss: 2.140965461730957\n",
            "Epoch 3, Step 23553, Loss: 1.961072564125061\n",
            "Epoch 3, Step 23554, Loss: 1.89027738571167\n",
            "Epoch 3, Step 23555, Loss: 1.7200573682785034\n",
            "Epoch 3, Step 23556, Loss: 1.8003736734390259\n",
            "Epoch 3, Step 23557, Loss: 2.3806917667388916\n",
            "Epoch 3, Step 23558, Loss: 1.912276268005371\n",
            "Epoch 3, Step 23559, Loss: 1.8699172735214233\n",
            "Epoch 3, Step 23560, Loss: 2.3204400539398193\n",
            "Epoch 3, Step 23561, Loss: 2.130699634552002\n",
            "Epoch 3, Step 23562, Loss: 0.9927464127540588\n",
            "Epoch 3, Step 23563, Loss: 2.1675240993499756\n",
            "Epoch 3, Step 23564, Loss: 1.257286787033081\n",
            "Epoch 3, Step 23565, Loss: 1.6966612339019775\n",
            "Epoch 3, Step 23566, Loss: 0.9938896894454956\n",
            "Epoch 3, Step 23567, Loss: 1.895385503768921\n",
            "Epoch 3, Step 23568, Loss: 2.1423611640930176\n",
            "Epoch 3, Step 23569, Loss: 0.562131404876709\n",
            "Epoch 3, Step 23570, Loss: 1.9595831632614136\n",
            "Epoch 3, Step 23571, Loss: 1.9279848337173462\n",
            "Epoch 3, Step 23572, Loss: 0.7758182287216187\n",
            "Epoch 3, Step 23573, Loss: 1.4866267442703247\n",
            "Epoch 3, Step 23574, Loss: 1.5073997974395752\n",
            "Epoch 3, Step 23575, Loss: 1.9162068367004395\n",
            "Epoch 3, Step 23576, Loss: 2.4080026149749756\n",
            "Epoch 3, Step 23577, Loss: 0.3866281509399414\n",
            "Epoch 3, Step 23578, Loss: 2.2790184020996094\n",
            "Epoch 3, Step 23579, Loss: 2.4381933212280273\n",
            "Epoch 3, Step 23580, Loss: 1.527123212814331\n",
            "Epoch 3, Step 23581, Loss: 0.35736051201820374\n",
            "Epoch 3, Step 23582, Loss: 0.5792174339294434\n",
            "Epoch 3, Step 23583, Loss: 2.9883742332458496\n",
            "Epoch 3, Step 23584, Loss: 0.8384444117546082\n",
            "Epoch 3, Step 23585, Loss: 1.0698904991149902\n",
            "Epoch 3, Step 23586, Loss: 2.3696916103363037\n",
            "Epoch 3, Step 23587, Loss: 2.671349048614502\n",
            "Epoch 3, Step 23588, Loss: 1.8861325979232788\n",
            "Epoch 3, Step 23589, Loss: 1.3231494426727295\n",
            "Epoch 3, Step 23590, Loss: 0.7509244680404663\n",
            "Epoch 3, Step 23591, Loss: 1.2920550107955933\n",
            "Epoch 3, Step 23592, Loss: 1.5478386878967285\n",
            "Epoch 3, Step 23593, Loss: 2.4747695922851562\n",
            "Epoch 3, Step 23594, Loss: 0.8734521865844727\n",
            "Epoch 3, Step 23595, Loss: 1.8859273195266724\n",
            "Epoch 3, Step 23596, Loss: 2.18009614944458\n",
            "Epoch 3, Step 23597, Loss: 2.0226237773895264\n",
            "Epoch 3, Step 23598, Loss: 2.0689642429351807\n",
            "Epoch 3, Step 23599, Loss: 1.4546658992767334\n",
            "Epoch 3, Step 23600, Loss: 1.8707586526870728\n",
            "Epoch 3, Step 23601, Loss: 0.6957272291183472\n",
            "Epoch 3, Step 23602, Loss: 1.3483092784881592\n",
            "Epoch 3, Step 23603, Loss: 1.7065370082855225\n",
            "Epoch 3, Step 23604, Loss: 1.4991377592086792\n",
            "Epoch 3, Step 23605, Loss: 1.5492295026779175\n",
            "Epoch 3, Step 23606, Loss: 1.3183749914169312\n",
            "Epoch 3, Step 23607, Loss: 2.369957447052002\n",
            "Epoch 3, Step 23608, Loss: 2.0999419689178467\n",
            "Epoch 3, Step 23609, Loss: 1.6739381551742554\n",
            "Epoch 3, Step 23610, Loss: 1.1500670909881592\n",
            "Epoch 3, Step 23611, Loss: 2.111286163330078\n",
            "Epoch 3, Step 23612, Loss: 1.2214746475219727\n",
            "Epoch 3, Step 23613, Loss: 1.814934253692627\n",
            "Epoch 3, Step 23614, Loss: 1.6516207456588745\n",
            "Epoch 3, Step 23615, Loss: 1.6797547340393066\n",
            "Epoch 3, Step 23616, Loss: 1.9607439041137695\n",
            "Epoch 3, Step 23617, Loss: 2.352134943008423\n",
            "Epoch 3, Step 23618, Loss: 1.2469383478164673\n",
            "Epoch 3, Step 23619, Loss: 1.9112070798873901\n",
            "Epoch 3, Step 23620, Loss: 1.2112441062927246\n",
            "Epoch 3, Step 23621, Loss: 0.710317850112915\n",
            "Epoch 3, Step 23622, Loss: 1.2224689722061157\n",
            "Epoch 3, Step 23623, Loss: 1.2916098833084106\n",
            "Epoch 3, Step 23624, Loss: 1.122989535331726\n",
            "Epoch 3, Step 23625, Loss: 2.2214839458465576\n",
            "Epoch 3, Step 23626, Loss: 1.8609392642974854\n",
            "Epoch 3, Step 23627, Loss: 1.6856919527053833\n",
            "Epoch 3, Step 23628, Loss: 1.4087207317352295\n",
            "Epoch 3, Step 23629, Loss: 1.054811716079712\n",
            "Epoch 3, Step 23630, Loss: 1.725894570350647\n",
            "Epoch 3, Step 23631, Loss: 1.803542137145996\n",
            "Epoch 3, Step 23632, Loss: 2.1805455684661865\n",
            "Epoch 3, Step 23633, Loss: 1.7776246070861816\n",
            "Epoch 3, Step 23634, Loss: 0.7502068877220154\n",
            "Epoch 3, Step 23635, Loss: 1.4245049953460693\n",
            "Epoch 3, Step 23636, Loss: 1.2899080514907837\n",
            "Epoch 3, Step 23637, Loss: 0.9297892451286316\n",
            "Epoch 3, Step 23638, Loss: 1.2551097869873047\n",
            "Epoch 3, Step 23639, Loss: 1.5475809574127197\n",
            "Epoch 3, Step 23640, Loss: 2.2158203125\n",
            "Epoch 3, Step 23641, Loss: 0.7118144631385803\n",
            "Epoch 3, Step 23642, Loss: 1.3425005674362183\n",
            "Epoch 3, Step 23643, Loss: 0.9118669033050537\n",
            "Epoch 3, Step 23644, Loss: 1.566192388534546\n",
            "Epoch 3, Step 23645, Loss: 0.6495266556739807\n",
            "Epoch 3, Step 23646, Loss: 2.1843008995056152\n",
            "Epoch 3, Step 23647, Loss: 1.4674595594406128\n",
            "Epoch 3, Step 23648, Loss: 2.4803173542022705\n",
            "Epoch 3, Step 23649, Loss: 0.6542713046073914\n",
            "Epoch 3, Step 23650, Loss: 1.4159959554672241\n",
            "Epoch 3, Step 23651, Loss: 2.7982466220855713\n",
            "Epoch 3, Step 23652, Loss: 2.223426103591919\n",
            "Epoch 3, Step 23653, Loss: 1.3636454343795776\n",
            "Epoch 3, Step 23654, Loss: 1.1003165245056152\n",
            "Epoch 3, Step 23655, Loss: 1.239039421081543\n",
            "Epoch 3, Step 23656, Loss: 1.7713463306427002\n",
            "Epoch 3, Step 23657, Loss: 0.6392849087715149\n",
            "Epoch 3, Step 23658, Loss: 1.1663727760314941\n",
            "Epoch 3, Step 23659, Loss: 1.1672759056091309\n",
            "Epoch 3, Step 23660, Loss: 2.269595146179199\n",
            "Epoch 3, Step 23661, Loss: 1.110112190246582\n",
            "Epoch 3, Step 23662, Loss: 1.2172380685806274\n",
            "Epoch 3, Step 23663, Loss: 1.9313604831695557\n",
            "Epoch 3, Step 23664, Loss: 1.4170563220977783\n",
            "Epoch 3, Step 23665, Loss: 1.384809970855713\n",
            "Epoch 3, Step 23666, Loss: 2.2115092277526855\n",
            "Epoch 3, Step 23667, Loss: 2.0469870567321777\n",
            "Epoch 3, Step 23668, Loss: 1.1391559839248657\n",
            "Epoch 3, Step 23669, Loss: 1.781675100326538\n",
            "Epoch 3, Step 23670, Loss: 1.2895807027816772\n",
            "Epoch 3, Step 23671, Loss: 1.9734050035476685\n",
            "Epoch 3, Step 23672, Loss: 2.3003158569335938\n",
            "Epoch 3, Step 23673, Loss: 1.1269351243972778\n",
            "Epoch 3, Step 23674, Loss: 1.3172354698181152\n",
            "Epoch 3, Step 23675, Loss: 1.9492672681808472\n",
            "Epoch 3, Step 23676, Loss: 2.38557767868042\n",
            "Epoch 3, Step 23677, Loss: 1.3058562278747559\n",
            "Epoch 3, Step 23678, Loss: 1.6014350652694702\n",
            "Epoch 3, Step 23679, Loss: 0.9706193208694458\n",
            "Epoch 3, Step 23680, Loss: 1.1855531930923462\n",
            "Epoch 3, Step 23681, Loss: 1.88643217086792\n",
            "Epoch 3, Step 23682, Loss: 1.1171931028366089\n",
            "Epoch 3, Step 23683, Loss: 1.9513698816299438\n",
            "Epoch 3, Step 23684, Loss: 1.2793738842010498\n",
            "Epoch 3, Step 23685, Loss: 1.769079566001892\n",
            "Epoch 3, Step 23686, Loss: 1.4672884941101074\n",
            "Epoch 3, Step 23687, Loss: 2.1358377933502197\n",
            "Epoch 3, Step 23688, Loss: 1.801534652709961\n",
            "Epoch 3, Step 23689, Loss: 2.511369228363037\n",
            "Epoch 3, Step 23690, Loss: 0.7088267803192139\n",
            "Epoch 3, Step 23691, Loss: 1.6226369142532349\n",
            "Epoch 3, Step 23692, Loss: 1.5729012489318848\n",
            "Epoch 3, Step 23693, Loss: 2.1591172218322754\n",
            "Epoch 3, Step 23694, Loss: 2.235746145248413\n",
            "Epoch 3, Step 23695, Loss: 1.3160383701324463\n",
            "Epoch 3, Step 23696, Loss: 1.808518409729004\n",
            "Epoch 3, Step 23697, Loss: 2.0254878997802734\n",
            "Epoch 3, Step 23698, Loss: 1.6132620573043823\n",
            "Epoch 3, Step 23699, Loss: 2.167346477508545\n",
            "Epoch 3, Step 23700, Loss: 0.8462557792663574\n",
            "Epoch 3, Step 23701, Loss: 1.4331656694412231\n",
            "Epoch 3, Step 23702, Loss: 1.5205813646316528\n",
            "Epoch 3, Step 23703, Loss: 1.3050158023834229\n",
            "Epoch 3, Step 23704, Loss: 1.7901617288589478\n",
            "Epoch 3, Step 23705, Loss: 2.331590175628662\n",
            "Epoch 3, Step 23706, Loss: 2.0149900913238525\n",
            "Epoch 3, Step 23707, Loss: 1.4827018976211548\n",
            "Epoch 3, Step 23708, Loss: 1.3418793678283691\n",
            "Epoch 3, Step 23709, Loss: 1.5488795042037964\n",
            "Epoch 3, Step 23710, Loss: 1.9734082221984863\n",
            "Epoch 3, Step 23711, Loss: 1.3928364515304565\n",
            "Epoch 3, Step 23712, Loss: 1.304264783859253\n",
            "Epoch 3, Step 23713, Loss: 2.1555333137512207\n",
            "Epoch 3, Step 23714, Loss: 1.7717989683151245\n",
            "Epoch 3, Step 23715, Loss: 0.9584929347038269\n",
            "Epoch 3, Step 23716, Loss: 0.7240420579910278\n",
            "Epoch 3, Step 23717, Loss: 1.7005596160888672\n",
            "Epoch 3, Step 23718, Loss: 2.248100996017456\n",
            "Epoch 3, Step 23719, Loss: 0.9134615063667297\n",
            "Epoch 3, Step 23720, Loss: 1.709205985069275\n",
            "Epoch 3, Step 23721, Loss: 0.846824586391449\n",
            "Epoch 3, Step 23722, Loss: 1.1076335906982422\n",
            "Epoch 3, Step 23723, Loss: 2.6821799278259277\n",
            "Epoch 3, Step 23724, Loss: 1.1523321866989136\n",
            "Epoch 3, Step 23725, Loss: 1.8888139724731445\n",
            "Epoch 3, Step 23726, Loss: 2.2595508098602295\n",
            "Epoch 3, Step 23727, Loss: 0.6352757215499878\n",
            "Epoch 3, Step 23728, Loss: 2.469263792037964\n",
            "Epoch 3, Step 23729, Loss: 1.855090618133545\n",
            "Epoch 3, Step 23730, Loss: 1.7348854541778564\n",
            "Epoch 3, Step 23731, Loss: 1.0084222555160522\n",
            "Epoch 3, Step 23732, Loss: 1.1619470119476318\n",
            "Epoch 3, Step 23733, Loss: 0.9522135853767395\n",
            "Epoch 3, Step 23734, Loss: 3.4403395652770996\n",
            "Epoch 3, Step 23735, Loss: 1.4456181526184082\n",
            "Epoch 3, Step 23736, Loss: 0.8529345989227295\n",
            "Epoch 3, Step 23737, Loss: 1.9874004125595093\n",
            "Epoch 3, Step 23738, Loss: 2.070930242538452\n",
            "Epoch 3, Step 23739, Loss: 1.0255166292190552\n",
            "Epoch 3, Step 23740, Loss: 1.118192195892334\n",
            "Epoch 3, Step 23741, Loss: 1.952748417854309\n",
            "Epoch 3, Step 23742, Loss: 2.135000228881836\n",
            "Epoch 3, Step 23743, Loss: 1.2259749174118042\n",
            "Epoch 3, Step 23744, Loss: 0.9581440687179565\n",
            "Epoch 3, Step 23745, Loss: 2.452176332473755\n",
            "Epoch 3, Step 23746, Loss: 1.22707200050354\n",
            "Epoch 3, Step 23747, Loss: 1.819448471069336\n",
            "Epoch 3, Step 23748, Loss: 1.7426061630249023\n",
            "Epoch 3, Step 23749, Loss: 1.0579843521118164\n",
            "Epoch 3, Step 23750, Loss: 1.5577095746994019\n",
            "Epoch 3, Step 23751, Loss: 1.5626140832901\n",
            "Epoch 3, Step 23752, Loss: 1.2551405429840088\n",
            "Epoch 3, Step 23753, Loss: 2.2983529567718506\n",
            "Epoch 3, Step 23754, Loss: 1.7197867631912231\n",
            "Epoch 3, Step 23755, Loss: 0.5972762703895569\n",
            "Epoch 3, Step 23756, Loss: 2.351306438446045\n",
            "Epoch 3, Step 23757, Loss: 0.5420254468917847\n",
            "Epoch 3, Step 23758, Loss: 1.6171470880508423\n",
            "Epoch 3, Step 23759, Loss: 1.925596833229065\n",
            "Epoch 3, Step 23760, Loss: 2.575493335723877\n",
            "Epoch 3, Step 23761, Loss: 1.7246876955032349\n",
            "Epoch 3, Step 23762, Loss: 0.5169976949691772\n",
            "Epoch 3, Step 23763, Loss: 1.8718347549438477\n",
            "Epoch 3, Step 23764, Loss: 2.213902711868286\n",
            "Epoch 3, Step 23765, Loss: 1.1748985052108765\n",
            "Epoch 3, Step 23766, Loss: 1.385366439819336\n",
            "Epoch 3, Step 23767, Loss: 1.5825387239456177\n",
            "Epoch 3, Step 23768, Loss: 0.8936819434165955\n",
            "Epoch 3, Step 23769, Loss: 1.85530686378479\n",
            "Epoch 3, Step 23770, Loss: 1.265608549118042\n",
            "Epoch 3, Step 23771, Loss: 1.5137909650802612\n",
            "Epoch 3, Step 23772, Loss: 1.7412737607955933\n",
            "Epoch 3, Step 23773, Loss: 0.7892859578132629\n",
            "Epoch 3, Step 23774, Loss: 1.6810271739959717\n",
            "Epoch 3, Step 23775, Loss: 1.1813956499099731\n",
            "Epoch 3, Step 23776, Loss: 1.8570348024368286\n",
            "Epoch 3, Step 23777, Loss: 1.0708370208740234\n",
            "Epoch 3, Step 23778, Loss: 0.8341644406318665\n",
            "Epoch 3, Step 23779, Loss: 1.284883737564087\n",
            "Epoch 3, Step 23780, Loss: 1.2405329942703247\n",
            "Epoch 3, Step 23781, Loss: 0.6700544357299805\n",
            "Epoch 3, Step 23782, Loss: 2.242219924926758\n",
            "Epoch 3, Step 23783, Loss: 1.3601123094558716\n",
            "Epoch 3, Step 23784, Loss: 1.1960699558258057\n",
            "Epoch 3, Step 23785, Loss: 2.257227659225464\n",
            "Epoch 3, Step 23786, Loss: 2.340597152709961\n",
            "Epoch 3, Step 23787, Loss: 1.5142890214920044\n",
            "Epoch 3, Step 23788, Loss: 1.322341799736023\n",
            "Epoch 3, Step 23789, Loss: 1.4395705461502075\n",
            "Epoch 3, Step 23790, Loss: 1.074813961982727\n",
            "Epoch 3, Step 23791, Loss: 2.429182291030884\n",
            "Epoch 3, Step 23792, Loss: 1.5236960649490356\n",
            "Epoch 3, Step 23793, Loss: 1.6608887910842896\n",
            "Epoch 3, Step 23794, Loss: 0.8792782425880432\n",
            "Epoch 3, Step 23795, Loss: 2.15134596824646\n",
            "Epoch 3, Step 23796, Loss: 1.7093133926391602\n",
            "Epoch 3, Step 23797, Loss: 1.5973246097564697\n",
            "Epoch 3, Step 23798, Loss: 1.5812733173370361\n",
            "Epoch 3, Step 23799, Loss: 1.883217453956604\n",
            "Epoch 3, Step 23800, Loss: 2.4420464038848877\n",
            "Epoch 3, Step 23801, Loss: 1.151286005973816\n",
            "Epoch 3, Step 23802, Loss: 1.16217839717865\n",
            "Epoch 3, Step 23803, Loss: 1.919249415397644\n",
            "Epoch 3, Step 23804, Loss: 1.509609580039978\n",
            "Epoch 3, Step 23805, Loss: 2.050183057785034\n",
            "Epoch 3, Step 23806, Loss: 2.160616159439087\n",
            "Epoch 3, Step 23807, Loss: 1.360639214515686\n",
            "Epoch 3, Step 23808, Loss: 1.2597651481628418\n",
            "Epoch 3, Step 23809, Loss: 0.9906029105186462\n",
            "Epoch 3, Step 23810, Loss: 2.010631561279297\n",
            "Epoch 3, Step 23811, Loss: 1.4567099809646606\n",
            "Epoch 3, Step 23812, Loss: 2.1992757320404053\n",
            "Epoch 3, Step 23813, Loss: 1.024871826171875\n",
            "Epoch 3, Step 23814, Loss: 1.4476746320724487\n",
            "Epoch 3, Step 23815, Loss: 1.5487571954727173\n",
            "Epoch 3, Step 23816, Loss: 1.6612260341644287\n",
            "Epoch 3, Step 23817, Loss: 1.5923666954040527\n",
            "Epoch 3, Step 23818, Loss: 2.3183486461639404\n",
            "Epoch 3, Step 23819, Loss: 0.9348419308662415\n",
            "Epoch 3, Step 23820, Loss: 2.2027153968811035\n",
            "Epoch 3, Step 23821, Loss: 0.6357227563858032\n",
            "Epoch 3, Step 23822, Loss: 1.7504777908325195\n",
            "Epoch 3, Step 23823, Loss: 0.7444011569023132\n",
            "Epoch 3, Step 23824, Loss: 1.6816797256469727\n",
            "Epoch 3, Step 23825, Loss: 1.6178258657455444\n",
            "Epoch 3, Step 23826, Loss: 1.670459270477295\n",
            "Epoch 3, Step 23827, Loss: 1.1699743270874023\n",
            "Epoch 3, Step 23828, Loss: 1.5054949522018433\n",
            "Epoch 3, Step 23829, Loss: 0.750990629196167\n",
            "Epoch 3, Step 23830, Loss: 1.4273011684417725\n",
            "Epoch 3, Step 23831, Loss: 1.7216087579727173\n",
            "Epoch 3, Step 23832, Loss: 1.4584952592849731\n",
            "Epoch 3, Step 23833, Loss: 1.8172446489334106\n",
            "Epoch 3, Step 23834, Loss: 1.4050325155258179\n",
            "Epoch 3, Step 23835, Loss: 2.062546730041504\n",
            "Epoch 3, Step 23836, Loss: 1.9210617542266846\n",
            "Epoch 3, Step 23837, Loss: 1.4802136421203613\n",
            "Epoch 3, Step 23838, Loss: 2.0155365467071533\n",
            "Epoch 3, Step 23839, Loss: 1.9360525608062744\n",
            "Epoch 3, Step 23840, Loss: 2.279305934906006\n",
            "Epoch 3, Step 23841, Loss: 1.772751808166504\n",
            "Epoch 3, Step 23842, Loss: 1.6601557731628418\n",
            "Epoch 3, Step 23843, Loss: 1.913957953453064\n",
            "Epoch 3, Step 23844, Loss: 1.297582745552063\n",
            "Epoch 3, Step 23845, Loss: 2.045189619064331\n",
            "Epoch 3, Step 23846, Loss: 0.5867090821266174\n",
            "Epoch 3, Step 23847, Loss: 1.7510126829147339\n",
            "Epoch 3, Step 23848, Loss: 1.3397221565246582\n",
            "Epoch 3, Step 23849, Loss: 1.2474027872085571\n",
            "Epoch 3, Step 23850, Loss: 2.711576461791992\n",
            "Epoch 3, Step 23851, Loss: 1.0587329864501953\n",
            "Epoch 3, Step 23852, Loss: 1.3165948390960693\n",
            "Epoch 3, Step 23853, Loss: 1.4911125898361206\n",
            "Epoch 3, Step 23854, Loss: 1.1442266702651978\n",
            "Epoch 3, Step 23855, Loss: 1.23429536819458\n",
            "Epoch 3, Step 23856, Loss: 1.0866687297821045\n",
            "Epoch 3, Step 23857, Loss: 3.088089942932129\n",
            "Epoch 3, Step 23858, Loss: 1.5338900089263916\n",
            "Epoch 3, Step 23859, Loss: 1.8230254650115967\n",
            "Epoch 3, Step 23860, Loss: 1.4973516464233398\n",
            "Epoch 3, Step 23861, Loss: 0.579481303691864\n",
            "Epoch 3, Step 23862, Loss: 1.6054317951202393\n",
            "Epoch 3, Step 23863, Loss: 1.5749821662902832\n",
            "Epoch 3, Step 23864, Loss: 1.905809998512268\n",
            "Epoch 3, Step 23865, Loss: 1.3935414552688599\n",
            "Epoch 3, Step 23866, Loss: 0.8798559308052063\n",
            "Epoch 3, Step 23867, Loss: 0.7410529851913452\n",
            "Epoch 3, Step 23868, Loss: 1.7397818565368652\n",
            "Epoch 3, Step 23869, Loss: 1.4300291538238525\n",
            "Epoch 3, Step 23870, Loss: 2.326192855834961\n",
            "Epoch 3, Step 23871, Loss: 1.8572449684143066\n",
            "Epoch 3, Step 23872, Loss: 2.3600547313690186\n",
            "Epoch 3, Step 23873, Loss: 2.232628345489502\n",
            "Epoch 3, Step 23874, Loss: 2.2429120540618896\n",
            "Epoch 3, Step 23875, Loss: 1.5831130743026733\n",
            "Epoch 3, Step 23876, Loss: 1.6863698959350586\n",
            "Epoch 3, Step 23877, Loss: 1.3577004671096802\n",
            "Epoch 3, Step 23878, Loss: 1.56759512424469\n",
            "Epoch 3, Step 23879, Loss: 0.8327019810676575\n",
            "Epoch 3, Step 23880, Loss: 2.055220127105713\n",
            "Epoch 3, Step 23881, Loss: 2.051938056945801\n",
            "Epoch 3, Step 23882, Loss: 2.1118276119232178\n",
            "Epoch 3, Step 23883, Loss: 1.9664939641952515\n",
            "Epoch 3, Step 23884, Loss: 2.413331985473633\n",
            "Epoch 3, Step 23885, Loss: 1.7743330001831055\n",
            "Epoch 3, Step 23886, Loss: 2.319812059402466\n",
            "Epoch 3, Step 23887, Loss: 2.0227184295654297\n",
            "Epoch 3, Step 23888, Loss: 1.7694951295852661\n",
            "Epoch 3, Step 23889, Loss: 1.2896822690963745\n",
            "Epoch 3, Step 23890, Loss: 1.0188767910003662\n",
            "Epoch 3, Step 23891, Loss: 2.7102720737457275\n",
            "Epoch 3, Step 23892, Loss: 0.7929700613021851\n",
            "Epoch 3, Step 23893, Loss: 2.3250443935394287\n",
            "Epoch 3, Step 23894, Loss: 2.018897771835327\n",
            "Epoch 3, Step 23895, Loss: 1.9962732791900635\n",
            "Epoch 3, Step 23896, Loss: 1.9917055368423462\n",
            "Epoch 3, Step 23897, Loss: 1.2457482814788818\n",
            "Epoch 3, Step 23898, Loss: 1.3386502265930176\n",
            "Epoch 3, Step 23899, Loss: 1.2284694910049438\n",
            "Epoch 3, Step 23900, Loss: 1.3636561632156372\n",
            "Epoch 3, Step 23901, Loss: 0.6586499214172363\n",
            "Epoch 3, Step 23902, Loss: 0.706818699836731\n",
            "Epoch 3, Step 23903, Loss: 1.42538583278656\n",
            "Epoch 3, Step 23904, Loss: 1.7903125286102295\n",
            "Epoch 3, Step 23905, Loss: 0.6304134726524353\n",
            "Epoch 3, Step 23906, Loss: 1.2615877389907837\n",
            "Epoch 3, Step 23907, Loss: 1.5879149436950684\n",
            "Epoch 3, Step 23908, Loss: 1.452845811843872\n",
            "Epoch 3, Step 23909, Loss: 2.6428754329681396\n",
            "Epoch 3, Step 23910, Loss: 1.7694897651672363\n",
            "Epoch 3, Step 23911, Loss: 1.5844817161560059\n",
            "Epoch 3, Step 23912, Loss: 1.9471697807312012\n",
            "Epoch 3, Step 23913, Loss: 1.7773854732513428\n",
            "Epoch 3, Step 23914, Loss: 1.258594036102295\n",
            "Epoch 3, Step 23915, Loss: 0.700145423412323\n",
            "Epoch 3, Step 23916, Loss: 1.6593071222305298\n",
            "Epoch 3, Step 23917, Loss: 1.745301604270935\n",
            "Epoch 3, Step 23918, Loss: 1.5982474088668823\n",
            "Epoch 3, Step 23919, Loss: 1.159947156906128\n",
            "Epoch 3, Step 23920, Loss: 0.7484062910079956\n",
            "Epoch 3, Step 23921, Loss: 2.052487850189209\n",
            "Epoch 3, Step 23922, Loss: 0.6131024360656738\n",
            "Epoch 3, Step 23923, Loss: 0.915798008441925\n",
            "Epoch 3, Step 23924, Loss: 1.8889660835266113\n",
            "Epoch 3, Step 23925, Loss: 1.6374962329864502\n",
            "Epoch 3, Step 23926, Loss: 2.058668613433838\n",
            "Epoch 3, Step 23927, Loss: 2.0411038398742676\n",
            "Epoch 3, Step 23928, Loss: 1.2800925970077515\n",
            "Epoch 3, Step 23929, Loss: 1.9709266424179077\n",
            "Epoch 3, Step 23930, Loss: 0.775199294090271\n",
            "Epoch 3, Step 23931, Loss: 2.681781530380249\n",
            "Epoch 3, Step 23932, Loss: 1.1359882354736328\n",
            "Epoch 3, Step 23933, Loss: 1.5438995361328125\n",
            "Epoch 3, Step 23934, Loss: 2.442368268966675\n",
            "Epoch 3, Step 23935, Loss: 1.5595660209655762\n",
            "Epoch 3, Step 23936, Loss: 2.287693977355957\n",
            "Epoch 3, Step 23937, Loss: 0.9664356708526611\n",
            "Epoch 3, Step 23938, Loss: 1.4477214813232422\n",
            "Epoch 3, Step 23939, Loss: 1.0975903272628784\n",
            "Epoch 3, Step 23940, Loss: 2.9967575073242188\n",
            "Epoch 3, Step 23941, Loss: 1.7081950902938843\n",
            "Epoch 3, Step 23942, Loss: 2.1057958602905273\n",
            "Epoch 3, Step 23943, Loss: 1.877183198928833\n",
            "Epoch 3, Step 23944, Loss: 2.3695437908172607\n",
            "Epoch 3, Step 23945, Loss: 1.796902060508728\n",
            "Epoch 3, Step 23946, Loss: 1.4503493309020996\n",
            "Epoch 3, Step 23947, Loss: 0.4996515214443207\n",
            "Epoch 3, Step 23948, Loss: 0.606826663017273\n",
            "Epoch 3, Step 23949, Loss: 0.9957947134971619\n",
            "Epoch 3, Step 23950, Loss: 1.5209052562713623\n",
            "Epoch 3, Step 23951, Loss: 0.4688337445259094\n",
            "Epoch 3, Step 23952, Loss: 0.7392373085021973\n",
            "Epoch 3, Step 23953, Loss: 1.279387354850769\n",
            "Epoch 3, Step 23954, Loss: 2.025428056716919\n",
            "Epoch 3, Step 23955, Loss: 1.907462239265442\n",
            "Epoch 3, Step 23956, Loss: 1.6334214210510254\n",
            "Epoch 3, Step 23957, Loss: 2.2352101802825928\n",
            "Epoch 3, Step 23958, Loss: 2.3592731952667236\n",
            "Epoch 3, Step 23959, Loss: 1.1728638410568237\n",
            "Epoch 3, Step 23960, Loss: 2.283747911453247\n",
            "Epoch 3, Step 23961, Loss: 1.3660178184509277\n",
            "Epoch 3, Step 23962, Loss: 1.2407431602478027\n",
            "Epoch 3, Step 23963, Loss: 1.524583101272583\n",
            "Epoch 3, Step 23964, Loss: 2.2763562202453613\n",
            "Epoch 3, Step 23965, Loss: 1.6676762104034424\n",
            "Epoch 3, Step 23966, Loss: 0.6238644123077393\n",
            "Epoch 3, Step 23967, Loss: 0.990558922290802\n",
            "Epoch 3, Step 23968, Loss: 1.9002496004104614\n",
            "Epoch 3, Step 23969, Loss: 1.9574211835861206\n",
            "Epoch 3, Step 23970, Loss: 1.3261018991470337\n",
            "Epoch 3, Step 23971, Loss: 2.351809024810791\n",
            "Epoch 3, Step 23972, Loss: 1.4598196744918823\n",
            "Epoch 3, Step 23973, Loss: 1.3969745635986328\n",
            "Epoch 3, Step 23974, Loss: 2.0766420364379883\n",
            "Epoch 3, Step 23975, Loss: 2.040186643600464\n",
            "Epoch 3, Step 23976, Loss: 1.3290852308273315\n",
            "Epoch 3, Step 23977, Loss: 1.5900665521621704\n",
            "Epoch 3, Step 23978, Loss: 1.3487056493759155\n",
            "Epoch 3, Step 23979, Loss: 1.5624973773956299\n",
            "Epoch 3, Step 23980, Loss: 1.7312768697738647\n",
            "Epoch 3, Step 23981, Loss: 1.9000822305679321\n",
            "Epoch 3, Step 23982, Loss: 0.8413385152816772\n",
            "Epoch 3, Step 23983, Loss: 1.7085318565368652\n",
            "Epoch 3, Step 23984, Loss: 1.1434983015060425\n",
            "Epoch 3, Step 23985, Loss: 2.198526620864868\n",
            "Epoch 3, Step 23986, Loss: 2.0848495960235596\n",
            "Epoch 3, Step 23987, Loss: 2.4121885299682617\n",
            "Epoch 3, Step 23988, Loss: 1.311318039894104\n",
            "Epoch 3, Step 23989, Loss: 1.4036989212036133\n",
            "Epoch 3, Step 23990, Loss: 1.7071229219436646\n",
            "Epoch 3, Step 23991, Loss: 1.5139182806015015\n",
            "Epoch 3, Step 23992, Loss: 1.8073103427886963\n",
            "Epoch 3, Step 23993, Loss: 0.7999731302261353\n",
            "Epoch 3, Step 23994, Loss: 2.308702230453491\n",
            "Epoch 3, Step 23995, Loss: 1.8369044065475464\n",
            "Epoch 3, Step 23996, Loss: 1.5512858629226685\n",
            "Epoch 3, Step 23997, Loss: 1.4516433477401733\n",
            "Epoch 3, Step 23998, Loss: 1.091822624206543\n",
            "Epoch 3, Step 23999, Loss: 1.6526329517364502\n",
            "Epoch 3, Step 24000, Loss: 1.513642430305481\n",
            "Epoch 3, Step 24001, Loss: 0.914390504360199\n",
            "Epoch 3, Step 24002, Loss: 1.3819653987884521\n",
            "Epoch 3, Step 24003, Loss: 2.0507595539093018\n",
            "Epoch 3, Step 24004, Loss: 0.9653393626213074\n",
            "Epoch 3, Step 24005, Loss: 2.4210352897644043\n",
            "Epoch 3, Step 24006, Loss: 1.0301870107650757\n",
            "Epoch 3, Step 24007, Loss: 1.7920968532562256\n",
            "Epoch 3, Step 24008, Loss: 1.081372618675232\n",
            "Epoch 3, Step 24009, Loss: 1.727043628692627\n",
            "Epoch 3, Step 24010, Loss: 2.2577929496765137\n",
            "Epoch 3, Step 24011, Loss: 0.9903563261032104\n",
            "Epoch 3, Step 24012, Loss: 0.8176024556159973\n",
            "Epoch 3, Step 24013, Loss: 0.9502983689308167\n",
            "Epoch 3, Step 24014, Loss: 1.538415551185608\n",
            "Epoch 3, Step 24015, Loss: 1.3392032384872437\n",
            "Epoch 3, Step 24016, Loss: 2.0092499256134033\n",
            "Epoch 3, Step 24017, Loss: 1.6036889553070068\n",
            "Epoch 3, Step 24018, Loss: 1.4732818603515625\n",
            "Epoch 3, Step 24019, Loss: 0.7110916972160339\n",
            "Epoch 3, Step 24020, Loss: 0.848057746887207\n",
            "Epoch 3, Step 24021, Loss: 2.3385329246520996\n",
            "Epoch 3, Step 24022, Loss: 1.9612107276916504\n",
            "Epoch 3, Step 24023, Loss: 1.1326322555541992\n",
            "Epoch 3, Step 24024, Loss: 0.6679633259773254\n",
            "Epoch 3, Step 24025, Loss: 2.2642569541931152\n",
            "Epoch 3, Step 24026, Loss: 0.4430639445781708\n",
            "Epoch 3, Step 24027, Loss: 1.1059225797653198\n",
            "Epoch 3, Step 24028, Loss: 2.135317802429199\n",
            "Epoch 3, Step 24029, Loss: 1.3622398376464844\n",
            "Epoch 3, Step 24030, Loss: 1.7876323461532593\n",
            "Epoch 3, Step 24031, Loss: 0.970889687538147\n",
            "Epoch 3, Step 24032, Loss: 1.3368926048278809\n",
            "Epoch 3, Step 24033, Loss: 1.1952265501022339\n",
            "Epoch 3, Step 24034, Loss: 1.26387619972229\n",
            "Epoch 3, Step 24035, Loss: 2.437051773071289\n",
            "Epoch 3, Step 24036, Loss: 1.0860493183135986\n",
            "Epoch 3, Step 24037, Loss: 1.5475815534591675\n",
            "Epoch 3, Step 24038, Loss: 1.7037097215652466\n",
            "Epoch 3, Step 24039, Loss: 0.8786121010780334\n",
            "Epoch 3, Step 24040, Loss: 1.7983070611953735\n",
            "Epoch 3, Step 24041, Loss: 1.0653737783432007\n",
            "Epoch 3, Step 24042, Loss: 1.66881263256073\n",
            "Epoch 3, Step 24043, Loss: 2.5309998989105225\n",
            "Epoch 3, Step 24044, Loss: 0.9687520861625671\n",
            "Epoch 3, Step 24045, Loss: 1.2585861682891846\n",
            "Epoch 3, Step 24046, Loss: 1.0981478691101074\n",
            "Epoch 3, Step 24047, Loss: 1.1439836025238037\n",
            "Epoch 3, Step 24048, Loss: 2.4500892162323\n",
            "Epoch 3, Step 24049, Loss: 2.2737255096435547\n",
            "Epoch 3, Step 24050, Loss: 1.7942898273468018\n",
            "Epoch 3, Step 24051, Loss: 1.2271114587783813\n",
            "Epoch 3, Step 24052, Loss: 0.6946232318878174\n",
            "Epoch 3, Step 24053, Loss: 2.500823497772217\n",
            "Epoch 3, Step 24054, Loss: 1.7756874561309814\n",
            "Epoch 3, Step 24055, Loss: 1.2645907402038574\n",
            "Epoch 3, Step 24056, Loss: 1.2765916585922241\n",
            "Epoch 3, Step 24057, Loss: 2.036708116531372\n",
            "Epoch 3, Step 24058, Loss: 1.511046290397644\n",
            "Epoch 3, Step 24059, Loss: 2.2538187503814697\n",
            "Epoch 3, Step 24060, Loss: 2.0161995887756348\n",
            "Epoch 3, Step 24061, Loss: 2.439727544784546\n",
            "Epoch 3, Step 24062, Loss: 1.766013741493225\n",
            "Epoch 3, Step 24063, Loss: 2.5406761169433594\n",
            "Epoch 3, Step 24064, Loss: 2.239095687866211\n",
            "Epoch 3, Step 24065, Loss: 1.2412488460540771\n",
            "Epoch 3, Step 24066, Loss: 2.048065185546875\n",
            "Epoch 3, Step 24067, Loss: 0.45788824558258057\n",
            "Epoch 3, Step 24068, Loss: 1.4463380575180054\n",
            "Epoch 3, Step 24069, Loss: 1.6853898763656616\n",
            "Epoch 3, Step 24070, Loss: 2.059539318084717\n",
            "Epoch 3, Step 24071, Loss: 2.419011116027832\n",
            "Epoch 3, Step 24072, Loss: 1.515903115272522\n",
            "Epoch 3, Step 24073, Loss: 1.904837727546692\n",
            "Epoch 3, Step 24074, Loss: 1.9359333515167236\n",
            "Epoch 3, Step 24075, Loss: 1.4026011228561401\n",
            "Epoch 3, Step 24076, Loss: 1.62636399269104\n",
            "Epoch 3, Step 24077, Loss: 1.0312613248825073\n",
            "Epoch 3, Step 24078, Loss: 0.5815799236297607\n",
            "Epoch 3, Step 24079, Loss: 2.1116833686828613\n",
            "Epoch 3, Step 24080, Loss: 0.5861334800720215\n",
            "Epoch 3, Step 24081, Loss: 1.8840663433074951\n",
            "Epoch 3, Step 24082, Loss: 1.6254551410675049\n",
            "Epoch 3, Step 24083, Loss: 0.7251925468444824\n",
            "Epoch 3, Step 24084, Loss: 1.7910629510879517\n",
            "Epoch 3, Step 24085, Loss: 1.3528279066085815\n",
            "Epoch 3, Step 24086, Loss: 2.406123638153076\n",
            "Epoch 3, Step 24087, Loss: 1.7119252681732178\n",
            "Epoch 3, Step 24088, Loss: 1.2132829427719116\n",
            "Epoch 3, Step 24089, Loss: 1.880797028541565\n",
            "Epoch 3, Step 24090, Loss: 1.915255069732666\n",
            "Epoch 3, Step 24091, Loss: 1.71048903465271\n",
            "Epoch 3, Step 24092, Loss: 1.5895768404006958\n",
            "Epoch 3, Step 24093, Loss: 0.6479609608650208\n",
            "Epoch 3, Step 24094, Loss: 0.6311990022659302\n",
            "Epoch 3, Step 24095, Loss: 1.332275629043579\n",
            "Epoch 3, Step 24096, Loss: 1.7271555662155151\n",
            "Epoch 3, Step 24097, Loss: 1.1311590671539307\n",
            "Epoch 3, Step 24098, Loss: 1.5743412971496582\n",
            "Epoch 3, Step 24099, Loss: 2.589919328689575\n",
            "Epoch 3, Step 24100, Loss: 0.8037168383598328\n",
            "Epoch 3, Step 24101, Loss: 1.2061508893966675\n",
            "Epoch 3, Step 24102, Loss: 0.9359429478645325\n",
            "Epoch 3, Step 24103, Loss: 1.4643914699554443\n",
            "Epoch 3, Step 24104, Loss: 1.5895235538482666\n",
            "Epoch 3, Step 24105, Loss: 1.7599388360977173\n",
            "Epoch 3, Step 24106, Loss: 1.3116375207901\n",
            "Epoch 3, Step 24107, Loss: 1.1377685070037842\n",
            "Epoch 3, Step 24108, Loss: 2.0046637058258057\n",
            "Epoch 3, Step 24109, Loss: 1.3005106449127197\n",
            "Epoch 3, Step 24110, Loss: 0.5396841764450073\n",
            "Epoch 3, Step 24111, Loss: 1.5950958728790283\n",
            "Epoch 3, Step 24112, Loss: 1.5129956007003784\n",
            "Epoch 3, Step 24113, Loss: 2.2819371223449707\n",
            "Epoch 3, Step 24114, Loss: 1.96421480178833\n",
            "Epoch 3, Step 24115, Loss: 2.381683111190796\n",
            "Epoch 3, Step 24116, Loss: 0.9783090949058533\n",
            "Epoch 3, Step 24117, Loss: 1.8127225637435913\n",
            "Epoch 3, Step 24118, Loss: 1.749201774597168\n",
            "Epoch 3, Step 24119, Loss: 1.9514422416687012\n",
            "Epoch 3, Step 24120, Loss: 2.3961260318756104\n",
            "Epoch 3, Step 24121, Loss: 1.0364532470703125\n",
            "Epoch 3, Step 24122, Loss: 2.3996548652648926\n",
            "Epoch 3, Step 24123, Loss: 1.395514726638794\n",
            "Epoch 3, Step 24124, Loss: 1.1311196088790894\n",
            "Epoch 3, Step 24125, Loss: 1.3718739748001099\n",
            "Epoch 3, Step 24126, Loss: 2.8096578121185303\n",
            "Epoch 3, Step 24127, Loss: 1.7926949262619019\n",
            "Epoch 3, Step 24128, Loss: 1.6194391250610352\n",
            "Epoch 3, Step 24129, Loss: 2.002326726913452\n",
            "Epoch 3, Step 24130, Loss: 2.1409952640533447\n",
            "Epoch 3, Step 24131, Loss: 2.026562452316284\n",
            "Epoch 3, Step 24132, Loss: 2.271571159362793\n",
            "Epoch 3, Step 24133, Loss: 2.220824956893921\n",
            "Epoch 3, Step 24134, Loss: 1.4368106126785278\n",
            "Epoch 3, Step 24135, Loss: 1.7175910472869873\n",
            "Epoch 3, Step 24136, Loss: 0.768587589263916\n",
            "Epoch 3, Step 24137, Loss: 0.8590732216835022\n",
            "Epoch 3, Step 24138, Loss: 1.825812578201294\n",
            "Epoch 3, Step 24139, Loss: 1.256872296333313\n",
            "Epoch 3, Step 24140, Loss: 1.121034860610962\n",
            "Epoch 3, Step 24141, Loss: 1.857721209526062\n",
            "Epoch 3, Step 24142, Loss: 2.4431886672973633\n",
            "Epoch 3, Step 24143, Loss: 1.5417611598968506\n",
            "Epoch 3, Step 24144, Loss: 1.7055838108062744\n",
            "Epoch 3, Step 24145, Loss: 2.2405152320861816\n",
            "Epoch 3, Step 24146, Loss: 0.6589425802230835\n",
            "Epoch 3, Step 24147, Loss: 1.7285538911819458\n",
            "Epoch 3, Step 24148, Loss: 1.7648284435272217\n",
            "Epoch 3, Step 24149, Loss: 1.0197380781173706\n",
            "Epoch 3, Step 24150, Loss: 1.4704865217208862\n",
            "Epoch 3, Step 24151, Loss: 1.2219053506851196\n",
            "Epoch 3, Step 24152, Loss: 0.29030120372772217\n",
            "Epoch 3, Step 24153, Loss: 1.3233309984207153\n",
            "Epoch 3, Step 24154, Loss: 1.4375993013381958\n",
            "Epoch 3, Step 24155, Loss: 2.4476919174194336\n",
            "Epoch 3, Step 24156, Loss: 1.1755881309509277\n",
            "Epoch 3, Step 24157, Loss: 2.255826234817505\n",
            "Epoch 3, Step 24158, Loss: 0.7570225596427917\n",
            "Epoch 3, Step 24159, Loss: 1.093358039855957\n",
            "Epoch 3, Step 24160, Loss: 1.6216161251068115\n",
            "Epoch 3, Step 24161, Loss: 2.1257386207580566\n",
            "Epoch 3, Step 24162, Loss: 1.9130383729934692\n",
            "Epoch 3, Step 24163, Loss: 1.4859834909439087\n",
            "Epoch 3, Step 24164, Loss: 1.2381058931350708\n",
            "Epoch 3, Step 24165, Loss: 1.1789233684539795\n",
            "Epoch 3, Step 24166, Loss: 1.5640252828598022\n",
            "Epoch 3, Step 24167, Loss: 2.3958537578582764\n",
            "Epoch 3, Step 24168, Loss: 2.831907272338867\n",
            "Epoch 3, Step 24169, Loss: 1.1228870153427124\n",
            "Epoch 3, Step 24170, Loss: 1.0150094032287598\n",
            "Epoch 3, Step 24171, Loss: 1.867492437362671\n",
            "Epoch 3, Step 24172, Loss: 0.6555119752883911\n",
            "Epoch 3, Step 24173, Loss: 1.5683735609054565\n",
            "Epoch 3, Step 24174, Loss: 2.035271167755127\n",
            "Epoch 3, Step 24175, Loss: 1.0129910707473755\n",
            "Epoch 3, Step 24176, Loss: 1.1927597522735596\n",
            "Epoch 3, Step 24177, Loss: 0.9388222694396973\n",
            "Epoch 3, Step 24178, Loss: 1.0063503980636597\n",
            "Epoch 3, Step 24179, Loss: 1.4770562648773193\n",
            "Epoch 3, Step 24180, Loss: 0.7050411701202393\n",
            "Epoch 3, Step 24181, Loss: 1.763197898864746\n",
            "Epoch 3, Step 24182, Loss: 2.280421733856201\n",
            "Epoch 3, Step 24183, Loss: 0.9932949542999268\n",
            "Epoch 3, Step 24184, Loss: 1.7564204931259155\n",
            "Epoch 3, Step 24185, Loss: 2.305178642272949\n",
            "Epoch 3, Step 24186, Loss: 2.2779242992401123\n",
            "Epoch 3, Step 24187, Loss: 1.3096580505371094\n",
            "Epoch 3, Step 24188, Loss: 1.2573795318603516\n",
            "Epoch 3, Step 24189, Loss: 1.31490159034729\n",
            "Epoch 3, Step 24190, Loss: 2.1585352420806885\n",
            "Epoch 3, Step 24191, Loss: 1.8935770988464355\n",
            "Epoch 3, Step 24192, Loss: 1.6611695289611816\n",
            "Epoch 3, Step 24193, Loss: 1.1165790557861328\n",
            "Epoch 3, Step 24194, Loss: 0.5613840222358704\n",
            "Epoch 3, Step 24195, Loss: 0.5737314224243164\n",
            "Epoch 3, Step 24196, Loss: 1.075951099395752\n",
            "Epoch 3, Step 24197, Loss: 2.0929479598999023\n",
            "Epoch 3, Step 24198, Loss: 1.7049024105072021\n",
            "Epoch 3, Step 24199, Loss: 1.6355596780776978\n",
            "Epoch 3, Step 24200, Loss: 1.9579228162765503\n",
            "Epoch 3, Step 24201, Loss: 1.845676064491272\n",
            "Epoch 3, Step 24202, Loss: 1.1014043092727661\n",
            "Epoch 3, Step 24203, Loss: 1.2853319644927979\n",
            "Epoch 3, Step 24204, Loss: 0.996811032295227\n",
            "Epoch 3, Step 24205, Loss: 1.7226399183273315\n",
            "Epoch 3, Step 24206, Loss: 2.3784232139587402\n",
            "Epoch 3, Step 24207, Loss: 2.0050837993621826\n",
            "Epoch 3, Step 24208, Loss: 0.8017805218696594\n",
            "Epoch 3, Step 24209, Loss: 1.2839561700820923\n",
            "Epoch 3, Step 24210, Loss: 1.161556363105774\n",
            "Epoch 3, Step 24211, Loss: 0.8775933980941772\n",
            "Epoch 3, Step 24212, Loss: 1.061640977859497\n",
            "Epoch 3, Step 24213, Loss: 1.7181200981140137\n",
            "Epoch 3, Step 24214, Loss: 1.377680778503418\n",
            "Epoch 3, Step 24215, Loss: 1.1294715404510498\n",
            "Epoch 3, Step 24216, Loss: 1.4015215635299683\n",
            "Epoch 3, Step 24217, Loss: 1.8644754886627197\n",
            "Epoch 3, Step 24218, Loss: 1.4991339445114136\n",
            "Epoch 3, Step 24219, Loss: 1.715481162071228\n",
            "Epoch 3, Step 24220, Loss: 0.9141111373901367\n",
            "Epoch 3, Step 24221, Loss: 1.2241387367248535\n",
            "Epoch 3, Step 24222, Loss: 1.833630084991455\n",
            "Epoch 3, Step 24223, Loss: 1.851464033126831\n",
            "Epoch 3, Step 24224, Loss: 0.6053515672683716\n",
            "Epoch 3, Step 24225, Loss: 1.3469460010528564\n",
            "Epoch 3, Step 24226, Loss: 1.0923868417739868\n",
            "Epoch 3, Step 24227, Loss: 1.1881567239761353\n",
            "Epoch 3, Step 24228, Loss: 1.250391960144043\n",
            "Epoch 3, Step 24229, Loss: 1.2591207027435303\n",
            "Epoch 3, Step 24230, Loss: 1.026872992515564\n",
            "Epoch 3, Step 24231, Loss: 1.28795325756073\n",
            "Epoch 3, Step 24232, Loss: 1.756337285041809\n",
            "Epoch 3, Step 24233, Loss: 2.023979425430298\n",
            "Epoch 3, Step 24234, Loss: 1.3628305196762085\n",
            "Epoch 3, Step 24235, Loss: 1.844368815422058\n",
            "Epoch 3, Step 24236, Loss: 1.54045832157135\n",
            "Epoch 3, Step 24237, Loss: 2.136040687561035\n",
            "Epoch 3, Step 24238, Loss: 2.055595636367798\n",
            "Epoch 3, Step 24239, Loss: 1.4521608352661133\n",
            "Epoch 3, Step 24240, Loss: 0.5611624717712402\n",
            "Epoch 3, Step 24241, Loss: 1.3312031030654907\n",
            "Epoch 3, Step 24242, Loss: 0.5892221331596375\n",
            "Epoch 3, Step 24243, Loss: 1.497849702835083\n",
            "Epoch 3, Step 24244, Loss: 1.3505632877349854\n",
            "Epoch 3, Step 24245, Loss: 1.3189548254013062\n",
            "Epoch 3, Step 24246, Loss: 1.8441981077194214\n",
            "Epoch 3, Step 24247, Loss: 1.1136552095413208\n",
            "Epoch 3, Step 24248, Loss: 1.8624809980392456\n",
            "Epoch 3, Step 24249, Loss: 0.9025613069534302\n",
            "Epoch 3, Step 24250, Loss: 2.239494800567627\n",
            "Epoch 3, Step 24251, Loss: 1.8490142822265625\n",
            "Epoch 3, Step 24252, Loss: 0.7846320271492004\n",
            "Epoch 3, Step 24253, Loss: 1.4576607942581177\n",
            "Epoch 3, Step 24254, Loss: 1.2707730531692505\n",
            "Epoch 3, Step 24255, Loss: 0.8424831032752991\n",
            "Epoch 3, Step 24256, Loss: 1.298433542251587\n",
            "Epoch 3, Step 24257, Loss: 1.2815675735473633\n",
            "Epoch 3, Step 24258, Loss: 1.0468175411224365\n",
            "Epoch 3, Step 24259, Loss: 0.6704990267753601\n",
            "Epoch 3, Step 24260, Loss: 2.216499090194702\n",
            "Epoch 3, Step 24261, Loss: 0.7875176668167114\n",
            "Epoch 3, Step 24262, Loss: 1.0004559755325317\n",
            "Epoch 3, Step 24263, Loss: 2.1096270084381104\n",
            "Epoch 3, Step 24264, Loss: 1.4982819557189941\n",
            "Epoch 3, Step 24265, Loss: 1.0915452241897583\n",
            "Epoch 3, Step 24266, Loss: 1.1481480598449707\n",
            "Epoch 3, Step 24267, Loss: 1.6244206428527832\n",
            "Epoch 3, Step 24268, Loss: 0.4863792657852173\n",
            "Epoch 3, Step 24269, Loss: 0.6091269850730896\n",
            "Epoch 3, Step 24270, Loss: 2.0913312435150146\n",
            "Epoch 3, Step 24271, Loss: 2.021244764328003\n",
            "Epoch 3, Step 24272, Loss: 1.4366955757141113\n",
            "Epoch 3, Step 24273, Loss: 2.085914134979248\n",
            "Epoch 3, Step 24274, Loss: 1.4289356470108032\n",
            "Epoch 3, Step 24275, Loss: 1.9526467323303223\n",
            "Epoch 3, Step 24276, Loss: 1.4025338888168335\n",
            "Epoch 3, Step 24277, Loss: 1.5057929754257202\n",
            "Epoch 3, Step 24278, Loss: 1.2928239107131958\n",
            "Epoch 3, Step 24279, Loss: 1.5504677295684814\n",
            "Epoch 3, Step 24280, Loss: 1.1551820039749146\n",
            "Epoch 3, Step 24281, Loss: 1.1262743473052979\n",
            "Epoch 3, Step 24282, Loss: 1.1341716051101685\n",
            "Epoch 3, Step 24283, Loss: 0.5958214998245239\n",
            "Epoch 3, Step 24284, Loss: 1.37545645236969\n",
            "Epoch 3, Step 24285, Loss: 1.1800600290298462\n",
            "Epoch 3, Step 24286, Loss: 2.467806100845337\n",
            "Epoch 3, Step 24287, Loss: 0.7667364478111267\n",
            "Epoch 3, Step 24288, Loss: 1.2443690299987793\n",
            "Epoch 3, Step 24289, Loss: 0.6067067384719849\n",
            "Epoch 3, Step 24290, Loss: 1.6054069995880127\n",
            "Epoch 3, Step 24291, Loss: 2.0077097415924072\n",
            "Epoch 3, Step 24292, Loss: 0.75553297996521\n",
            "Epoch 3, Step 24293, Loss: 1.8584293127059937\n",
            "Epoch 3, Step 24294, Loss: 0.6006821393966675\n",
            "Epoch 3, Step 24295, Loss: 1.6742258071899414\n",
            "Epoch 3, Step 24296, Loss: 2.1479485034942627\n",
            "Epoch 3, Step 24297, Loss: 1.353797435760498\n",
            "Epoch 3, Step 24298, Loss: 1.784921407699585\n",
            "Epoch 3, Step 24299, Loss: 0.9493342041969299\n",
            "Epoch 3, Step 24300, Loss: 1.0399694442749023\n",
            "Epoch 3, Step 24301, Loss: 2.0901291370391846\n",
            "Epoch 3, Step 24302, Loss: 1.3630160093307495\n",
            "Epoch 3, Step 24303, Loss: 0.4918311834335327\n",
            "Epoch 3, Step 24304, Loss: 1.0587278604507446\n",
            "Epoch 3, Step 24305, Loss: 1.626399278640747\n",
            "Epoch 3, Step 24306, Loss: 2.1718575954437256\n",
            "Epoch 3, Step 24307, Loss: 2.549740791320801\n",
            "Epoch 3, Step 24308, Loss: 1.3376424312591553\n",
            "Epoch 3, Step 24309, Loss: 1.3429865837097168\n",
            "Epoch 3, Step 24310, Loss: 3.0412514209747314\n",
            "Epoch 3, Step 24311, Loss: 1.3096362352371216\n",
            "Epoch 3, Step 24312, Loss: 2.3222544193267822\n",
            "Epoch 3, Step 24313, Loss: 2.509068250656128\n",
            "Epoch 3, Step 24314, Loss: 1.6031228303909302\n",
            "Epoch 3, Step 24315, Loss: 1.8028507232666016\n",
            "Epoch 3, Step 24316, Loss: 0.9853589534759521\n",
            "Epoch 3, Step 24317, Loss: 1.600246548652649\n",
            "Epoch 3, Step 24318, Loss: 1.661139726638794\n",
            "Epoch 3, Step 24319, Loss: 1.5924034118652344\n",
            "Epoch 3, Step 24320, Loss: 1.7954447269439697\n",
            "Epoch 3, Step 24321, Loss: 1.8372526168823242\n",
            "Epoch 3, Step 24322, Loss: 1.3802086114883423\n",
            "Epoch 3, Step 24323, Loss: 1.2239423990249634\n",
            "Epoch 3, Step 24324, Loss: 1.726438283920288\n",
            "Epoch 3, Step 24325, Loss: 2.04963755607605\n",
            "Epoch 3, Step 24326, Loss: 1.553853154182434\n",
            "Epoch 3, Step 24327, Loss: 2.000950813293457\n",
            "Epoch 3, Step 24328, Loss: 1.5141106843948364\n",
            "Epoch 3, Step 24329, Loss: 1.723164439201355\n",
            "Epoch 3, Step 24330, Loss: 1.333376169204712\n",
            "Epoch 3, Step 24331, Loss: 1.7580171823501587\n",
            "Epoch 3, Step 24332, Loss: 1.1666817665100098\n",
            "Epoch 3, Step 24333, Loss: 2.2292990684509277\n",
            "Epoch 3, Step 24334, Loss: 1.751922845840454\n",
            "Epoch 3, Step 24335, Loss: 2.317204475402832\n",
            "Epoch 3, Step 24336, Loss: 1.2137656211853027\n",
            "Epoch 3, Step 24337, Loss: 1.662930965423584\n",
            "Epoch 3, Step 24338, Loss: 1.8678427934646606\n",
            "Epoch 3, Step 24339, Loss: 1.1444298028945923\n",
            "Epoch 3, Step 24340, Loss: 2.679327964782715\n",
            "Epoch 3, Step 24341, Loss: 2.109647274017334\n",
            "Epoch 3, Step 24342, Loss: 1.6744650602340698\n",
            "Epoch 3, Step 24343, Loss: 0.8854901790618896\n",
            "Epoch 3, Step 24344, Loss: 2.0344126224517822\n",
            "Epoch 3, Step 24345, Loss: 1.9604486227035522\n",
            "Epoch 3, Step 24346, Loss: 1.6968222856521606\n",
            "Epoch 3, Step 24347, Loss: 1.8238461017608643\n",
            "Epoch 3, Step 24348, Loss: 1.8592349290847778\n",
            "Epoch 3, Step 24349, Loss: 0.7300468683242798\n",
            "Epoch 3, Step 24350, Loss: 1.4333120584487915\n",
            "Epoch 3, Step 24351, Loss: 1.265798807144165\n",
            "Epoch 3, Step 24352, Loss: 1.4365051984786987\n",
            "Epoch 3, Step 24353, Loss: 1.3552199602127075\n",
            "Epoch 3, Step 24354, Loss: 1.4523913860321045\n",
            "Epoch 3, Step 24355, Loss: 1.0782912969589233\n",
            "Epoch 3, Step 24356, Loss: 1.4821436405181885\n",
            "Epoch 3, Step 24357, Loss: 1.6915568113327026\n",
            "Epoch 3, Step 24358, Loss: 1.966642141342163\n",
            "Epoch 3, Step 24359, Loss: 0.32473090291023254\n",
            "Epoch 3, Step 24360, Loss: 1.5010086297988892\n",
            "Epoch 3, Step 24361, Loss: 0.4716719090938568\n",
            "Epoch 3, Step 24362, Loss: 1.8598204851150513\n",
            "Epoch 3, Step 24363, Loss: 1.3175201416015625\n",
            "Epoch 3, Step 24364, Loss: 0.8885489702224731\n",
            "Epoch 3, Step 24365, Loss: 1.518462061882019\n",
            "Epoch 3, Step 24366, Loss: 1.958478331565857\n",
            "Epoch 3, Step 24367, Loss: 0.9639808535575867\n",
            "Epoch 3, Step 24368, Loss: 2.4468765258789062\n",
            "Epoch 3, Step 24369, Loss: 2.4485254287719727\n",
            "Epoch 3, Step 24370, Loss: 2.201011896133423\n",
            "Epoch 3, Step 24371, Loss: 1.4188368320465088\n",
            "Epoch 3, Step 24372, Loss: 0.4057639539241791\n",
            "Epoch 3, Step 24373, Loss: 1.0627241134643555\n",
            "Epoch 3, Step 24374, Loss: 2.164227247238159\n",
            "Epoch 3, Step 24375, Loss: 2.039530038833618\n",
            "Epoch 3, Step 24376, Loss: 2.0568315982818604\n",
            "Epoch 3, Step 24377, Loss: 0.7960736751556396\n",
            "Epoch 3, Step 24378, Loss: 1.4728442430496216\n",
            "Epoch 3, Step 24379, Loss: 1.7910196781158447\n",
            "Epoch 3, Step 24380, Loss: 1.6918858289718628\n",
            "Epoch 3, Step 24381, Loss: 1.3479280471801758\n",
            "Epoch 3, Step 24382, Loss: 1.2143467664718628\n",
            "Epoch 3, Step 24383, Loss: 2.5923914909362793\n",
            "Epoch 3, Step 24384, Loss: 1.1427866220474243\n",
            "Epoch 3, Step 24385, Loss: 1.5847309827804565\n",
            "Epoch 3, Step 24386, Loss: 0.8920074701309204\n",
            "Epoch 3, Step 24387, Loss: 1.2469749450683594\n",
            "Epoch 3, Step 24388, Loss: 0.7037807703018188\n",
            "Epoch 3, Step 24389, Loss: 1.459855556488037\n",
            "Epoch 3, Step 24390, Loss: 1.3800740242004395\n",
            "Epoch 3, Step 24391, Loss: 1.8366954326629639\n",
            "Epoch 3, Step 24392, Loss: 1.2600350379943848\n",
            "Epoch 3, Step 24393, Loss: 1.270782709121704\n",
            "Epoch 3, Step 24394, Loss: 1.761080265045166\n",
            "Epoch 3, Step 24395, Loss: 1.083897352218628\n",
            "Epoch 3, Step 24396, Loss: 0.8963820934295654\n",
            "Epoch 3, Step 24397, Loss: 1.34127938747406\n",
            "Epoch 3, Step 24398, Loss: 1.383914828300476\n",
            "Epoch 3, Step 24399, Loss: 0.5369818806648254\n",
            "Epoch 3, Step 24400, Loss: 1.4670554399490356\n",
            "Epoch 3, Step 24401, Loss: 1.7631616592407227\n",
            "Epoch 3, Step 24402, Loss: 1.4570287466049194\n",
            "Epoch 3, Step 24403, Loss: 1.560567855834961\n",
            "Epoch 3, Step 24404, Loss: 1.6163172721862793\n",
            "Epoch 3, Step 24405, Loss: 1.2973549365997314\n",
            "Epoch 3, Step 24406, Loss: 1.1756839752197266\n",
            "Epoch 3, Step 24407, Loss: 2.460975170135498\n",
            "Epoch 3, Step 24408, Loss: 1.805863380432129\n",
            "Epoch 3, Step 24409, Loss: 1.9496972560882568\n",
            "Epoch 3, Step 24410, Loss: 0.7773603200912476\n",
            "Epoch 3, Step 24411, Loss: 1.8207204341888428\n",
            "Epoch 3, Step 24412, Loss: 1.047964334487915\n",
            "Epoch 3, Step 24413, Loss: 1.0529091358184814\n",
            "Epoch 3, Step 24414, Loss: 1.9104392528533936\n",
            "Epoch 3, Step 24415, Loss: 0.41634100675582886\n",
            "Epoch 3, Step 24416, Loss: 1.2058024406433105\n",
            "Epoch 3, Step 24417, Loss: 1.8782551288604736\n",
            "Epoch 3, Step 24418, Loss: 2.1065754890441895\n",
            "Epoch 3, Step 24419, Loss: 0.7120940685272217\n",
            "Epoch 3, Step 24420, Loss: 1.3077119588851929\n",
            "Epoch 3, Step 24421, Loss: 1.7797914743423462\n",
            "Epoch 3, Step 24422, Loss: 1.5577425956726074\n",
            "Epoch 3, Step 24423, Loss: 1.803525686264038\n",
            "Epoch 3, Step 24424, Loss: 0.9672425985336304\n",
            "Epoch 3, Step 24425, Loss: 1.578590989112854\n",
            "Epoch 3, Step 24426, Loss: 1.494598388671875\n",
            "Epoch 3, Step 24427, Loss: 1.5345748662948608\n",
            "Epoch 3, Step 24428, Loss: 1.4579869508743286\n",
            "Epoch 3, Step 24429, Loss: 1.6397429704666138\n",
            "Epoch 3, Step 24430, Loss: 0.8433727622032166\n",
            "Epoch 3, Step 24431, Loss: 1.2166335582733154\n",
            "Epoch 3, Step 24432, Loss: 1.8168764114379883\n",
            "Epoch 3, Step 24433, Loss: 1.6798495054244995\n",
            "Epoch 3, Step 24434, Loss: 2.299076557159424\n",
            "Epoch 3, Step 24435, Loss: 0.5347641110420227\n",
            "Epoch 3, Step 24436, Loss: 1.9593952894210815\n",
            "Epoch 3, Step 24437, Loss: 1.4715913534164429\n",
            "Epoch 3, Step 24438, Loss: 2.478496789932251\n",
            "Epoch 3, Step 24439, Loss: 2.415351390838623\n",
            "Epoch 3, Step 24440, Loss: 1.3959921598434448\n",
            "Epoch 3, Step 24441, Loss: 2.379783868789673\n",
            "Epoch 3, Step 24442, Loss: 1.334904432296753\n",
            "Epoch 3, Step 24443, Loss: 0.7604401707649231\n",
            "Epoch 3, Step 24444, Loss: 1.3457155227661133\n",
            "Epoch 3, Step 24445, Loss: 1.3933666944503784\n",
            "Epoch 3, Step 24446, Loss: 2.2899973392486572\n",
            "Epoch 3, Step 24447, Loss: 2.133612632751465\n",
            "Epoch 3, Step 24448, Loss: 1.2327340841293335\n",
            "Epoch 3, Step 24449, Loss: 2.047365188598633\n",
            "Epoch 3, Step 24450, Loss: 2.0581560134887695\n",
            "Epoch 3, Step 24451, Loss: 0.5871849656105042\n",
            "Epoch 3, Step 24452, Loss: 1.827061653137207\n",
            "Epoch 3, Step 24453, Loss: 1.9077926874160767\n",
            "Epoch 3, Step 24454, Loss: 1.0462582111358643\n",
            "Epoch 3, Step 24455, Loss: 1.3660978078842163\n",
            "Epoch 3, Step 24456, Loss: 0.6414676308631897\n",
            "Epoch 3, Step 24457, Loss: 1.649334192276001\n",
            "Epoch 3, Step 24458, Loss: 1.6179330348968506\n",
            "Epoch 3, Step 24459, Loss: 0.825813889503479\n",
            "Epoch 3, Step 24460, Loss: 0.3994455933570862\n",
            "Epoch 3, Step 24461, Loss: 2.2549519538879395\n",
            "Epoch 3, Step 24462, Loss: 2.0826191902160645\n",
            "Epoch 3, Step 24463, Loss: 1.5120049715042114\n",
            "Epoch 3, Step 24464, Loss: 2.174386739730835\n",
            "Epoch 3, Step 24465, Loss: 2.2747881412506104\n",
            "Epoch 3, Step 24466, Loss: 1.4425071477890015\n",
            "Epoch 3, Step 24467, Loss: 1.6840569972991943\n",
            "Epoch 3, Step 24468, Loss: 1.9137639999389648\n",
            "Epoch 3, Step 24469, Loss: 2.84344744682312\n",
            "Epoch 3, Step 24470, Loss: 1.404190182685852\n",
            "Epoch 3, Step 24471, Loss: 1.829434871673584\n",
            "Epoch 3, Step 24472, Loss: 0.9034878015518188\n",
            "Epoch 3, Step 24473, Loss: 1.6364634037017822\n",
            "Epoch 3, Step 24474, Loss: 1.730873465538025\n",
            "Epoch 3, Step 24475, Loss: 1.0085241794586182\n",
            "Epoch 3, Step 24476, Loss: 1.0378385782241821\n",
            "Epoch 3, Step 24477, Loss: 1.439182162284851\n",
            "Epoch 3, Step 24478, Loss: 1.0796139240264893\n",
            "Epoch 3, Step 24479, Loss: 0.8794667720794678\n",
            "Epoch 3, Step 24480, Loss: 1.3755719661712646\n",
            "Epoch 3, Step 24481, Loss: 1.60629141330719\n",
            "Epoch 3, Step 24482, Loss: 0.8787878751754761\n",
            "Epoch 3, Step 24483, Loss: 1.3181802034378052\n",
            "Epoch 3, Step 24484, Loss: 0.5026598572731018\n",
            "Epoch 3, Step 24485, Loss: 1.7951219081878662\n",
            "Epoch 3, Step 24486, Loss: 1.213844895362854\n",
            "Epoch 3, Step 24487, Loss: 1.569848895072937\n",
            "Epoch 3, Step 24488, Loss: 1.576568841934204\n",
            "Epoch 3, Step 24489, Loss: 1.4092943668365479\n",
            "Epoch 3, Step 24490, Loss: 1.5123939514160156\n",
            "Epoch 3, Step 24491, Loss: 0.7994844317436218\n",
            "Epoch 3, Step 24492, Loss: 1.3966403007507324\n",
            "Epoch 3, Step 24493, Loss: 1.7338981628417969\n",
            "Epoch 3, Step 24494, Loss: 1.941721796989441\n",
            "Epoch 3, Step 24495, Loss: 1.7437376976013184\n",
            "Epoch 3, Step 24496, Loss: 0.8621122241020203\n",
            "Epoch 3, Step 24497, Loss: 0.7706604599952698\n",
            "Epoch 3, Step 24498, Loss: 1.5571166276931763\n",
            "Epoch 3, Step 24499, Loss: 1.5650471448898315\n",
            "Epoch 3, Step 24500, Loss: 0.5551437735557556\n",
            "Epoch 3, Step 24501, Loss: 1.7925230264663696\n",
            "Epoch 3, Step 24502, Loss: 1.3059675693511963\n",
            "Epoch 3, Step 24503, Loss: 0.6850866079330444\n",
            "Epoch 3, Step 24504, Loss: 1.4135690927505493\n",
            "Epoch 3, Step 24505, Loss: 1.0382356643676758\n",
            "Epoch 3, Step 24506, Loss: 1.149131417274475\n",
            "Epoch 3, Step 24507, Loss: 1.2417666912078857\n",
            "Epoch 3, Step 24508, Loss: 0.8810073137283325\n",
            "Epoch 3, Step 24509, Loss: 1.1342108249664307\n",
            "Epoch 3, Step 24510, Loss: 1.788299560546875\n",
            "Epoch 3, Step 24511, Loss: 2.426203489303589\n",
            "Epoch 3, Step 24512, Loss: 2.282208204269409\n",
            "Epoch 3, Step 24513, Loss: 2.007763385772705\n",
            "Epoch 3, Step 24514, Loss: 1.4832788705825806\n",
            "Epoch 3, Step 24515, Loss: 1.3664923906326294\n",
            "Epoch 3, Step 24516, Loss: 0.613837718963623\n",
            "Epoch 3, Step 24517, Loss: 0.6045979261398315\n",
            "Epoch 3, Step 24518, Loss: 1.2620272636413574\n",
            "Epoch 3, Step 24519, Loss: 1.384684681892395\n",
            "Epoch 3, Step 24520, Loss: 1.6788135766983032\n",
            "Epoch 3, Step 24521, Loss: 1.7401150465011597\n",
            "Epoch 3, Step 24522, Loss: 2.1363091468811035\n",
            "Epoch 3, Step 24523, Loss: 1.0073020458221436\n",
            "Epoch 3, Step 24524, Loss: 0.8147737383842468\n",
            "Epoch 3, Step 24525, Loss: 0.6427273154258728\n",
            "Epoch 3, Step 24526, Loss: 2.6425046920776367\n",
            "Epoch 3, Step 24527, Loss: 1.8079187870025635\n",
            "Epoch 3, Step 24528, Loss: 1.4682039022445679\n",
            "Epoch 3, Step 24529, Loss: 1.3429780006408691\n",
            "Epoch 3, Step 24530, Loss: 2.1835222244262695\n",
            "Epoch 3, Step 24531, Loss: 1.5536390542984009\n",
            "Epoch 3, Step 24532, Loss: 2.4963910579681396\n",
            "Epoch 3, Step 24533, Loss: 1.6579983234405518\n",
            "Epoch 3, Step 24534, Loss: 1.5638271570205688\n",
            "Epoch 3, Step 24535, Loss: 1.0663729906082153\n",
            "Epoch 3, Step 24536, Loss: 1.613612174987793\n",
            "Epoch 3, Step 24537, Loss: 0.7273200750350952\n",
            "Epoch 3, Step 24538, Loss: 1.6438015699386597\n",
            "Epoch 3, Step 24539, Loss: 2.232750654220581\n",
            "Epoch 3, Step 24540, Loss: 0.6367993354797363\n",
            "Epoch 3, Step 24541, Loss: 1.515443205833435\n",
            "Epoch 3, Step 24542, Loss: 1.8288185596466064\n",
            "Epoch 3, Step 24543, Loss: 1.3844996690750122\n",
            "Epoch 3, Step 24544, Loss: 2.649564743041992\n",
            "Epoch 3, Step 24545, Loss: 1.8058056831359863\n",
            "Epoch 3, Step 24546, Loss: 0.5428184270858765\n",
            "Epoch 3, Step 24547, Loss: 1.7144416570663452\n",
            "Epoch 3, Step 24548, Loss: 2.2734053134918213\n",
            "Epoch 3, Step 24549, Loss: 1.4927847385406494\n",
            "Epoch 3, Step 24550, Loss: 1.5022591352462769\n",
            "Epoch 3, Step 24551, Loss: 0.9820957779884338\n",
            "Epoch 3, Step 24552, Loss: 1.4530656337738037\n",
            "Epoch 3, Step 24553, Loss: 1.25200355052948\n",
            "Epoch 3, Step 24554, Loss: 1.344534158706665\n",
            "Epoch 3, Step 24555, Loss: 2.337144613265991\n",
            "Epoch 3, Step 24556, Loss: 1.6913989782333374\n",
            "Epoch 3, Step 24557, Loss: 2.671074867248535\n",
            "Epoch 3, Step 24558, Loss: 2.355250835418701\n",
            "Epoch 3, Step 24559, Loss: 2.0203113555908203\n",
            "Epoch 3, Step 24560, Loss: 1.0489649772644043\n",
            "Epoch 3, Step 24561, Loss: 2.2011959552764893\n",
            "Epoch 3, Step 24562, Loss: 1.7227848768234253\n",
            "Epoch 3, Step 24563, Loss: 0.8641325235366821\n",
            "Epoch 3, Step 24564, Loss: 1.168387532234192\n",
            "Epoch 3, Step 24565, Loss: 2.2486279010772705\n",
            "Epoch 3, Step 24566, Loss: 0.6351907849311829\n",
            "Epoch 3, Step 24567, Loss: 1.81990385055542\n",
            "Epoch 3, Step 24568, Loss: 2.0397820472717285\n",
            "Epoch 3, Step 24569, Loss: 1.4701061248779297\n",
            "Epoch 3, Step 24570, Loss: 2.1103920936584473\n",
            "Epoch 3, Step 24571, Loss: 3.321788787841797\n",
            "Epoch 3, Step 24572, Loss: 1.0505807399749756\n",
            "Epoch 3, Step 24573, Loss: 2.337921619415283\n",
            "Epoch 3, Step 24574, Loss: 1.0999948978424072\n",
            "Epoch 3, Step 24575, Loss: 1.4807230234146118\n",
            "Epoch 3, Step 24576, Loss: 0.5812721848487854\n",
            "Epoch 3, Step 24577, Loss: 2.3400866985321045\n",
            "Epoch 3, Step 24578, Loss: 1.0685408115386963\n",
            "Epoch 3, Step 24579, Loss: 1.214703917503357\n",
            "Epoch 3, Step 24580, Loss: 1.8643712997436523\n",
            "Epoch 3, Step 24581, Loss: 1.5235342979431152\n",
            "Epoch 3, Step 24582, Loss: 0.6897994875907898\n",
            "Epoch 3, Step 24583, Loss: 1.0286271572113037\n",
            "Epoch 3, Step 24584, Loss: 1.0578359365463257\n",
            "Epoch 3, Step 24585, Loss: 1.1043481826782227\n",
            "Epoch 3, Step 24586, Loss: 2.0006277561187744\n",
            "Epoch 3, Step 24587, Loss: 1.4630025625228882\n",
            "Epoch 3, Step 24588, Loss: 0.7694509625434875\n",
            "Epoch 3, Step 24589, Loss: 1.259680986404419\n",
            "Epoch 3, Step 24590, Loss: 1.7233805656433105\n",
            "Epoch 3, Step 24591, Loss: 1.581426739692688\n",
            "Epoch 3, Step 24592, Loss: 1.4087506532669067\n",
            "Epoch 3, Step 24593, Loss: 1.7052925825119019\n",
            "Epoch 3, Step 24594, Loss: 2.3313655853271484\n",
            "Epoch 3, Step 24595, Loss: 2.2787115573883057\n",
            "Epoch 3, Step 24596, Loss: 1.5786082744598389\n",
            "Epoch 3, Step 24597, Loss: 0.9696593284606934\n",
            "Epoch 3, Step 24598, Loss: 2.318563461303711\n",
            "Epoch 3, Step 24599, Loss: 2.028529644012451\n",
            "Epoch 3, Step 24600, Loss: 1.6091489791870117\n",
            "Epoch 3, Step 24601, Loss: 1.8672879934310913\n",
            "Epoch 3, Step 24602, Loss: 1.977429747581482\n",
            "Epoch 3, Step 24603, Loss: 1.1446274518966675\n",
            "Epoch 3, Step 24604, Loss: 1.683624267578125\n",
            "Epoch 3, Step 24605, Loss: 1.486189842224121\n",
            "Epoch 3, Step 24606, Loss: 0.5215210914611816\n",
            "Epoch 3, Step 24607, Loss: 1.9574888944625854\n",
            "Epoch 3, Step 24608, Loss: 2.46565580368042\n",
            "Epoch 3, Step 24609, Loss: 1.9788838624954224\n",
            "Epoch 3, Step 24610, Loss: 1.0603647232055664\n",
            "Epoch 3, Step 24611, Loss: 1.3867615461349487\n",
            "Epoch 3, Step 24612, Loss: 2.1163275241851807\n",
            "Epoch 3, Step 24613, Loss: 1.698388934135437\n",
            "Epoch 3, Step 24614, Loss: 1.3019404411315918\n",
            "Epoch 3, Step 24615, Loss: 1.3918813467025757\n",
            "Epoch 3, Step 24616, Loss: 1.548838496208191\n",
            "Epoch 3, Step 24617, Loss: 2.2465415000915527\n",
            "Epoch 3, Step 24618, Loss: 1.727795958518982\n",
            "Epoch 3, Step 24619, Loss: 0.9350352883338928\n",
            "Epoch 3, Step 24620, Loss: 1.4495394229888916\n",
            "Epoch 3, Step 24621, Loss: 1.6946746110916138\n",
            "Epoch 3, Step 24622, Loss: 2.033524990081787\n",
            "Epoch 3, Step 24623, Loss: 1.3461068868637085\n",
            "Epoch 3, Step 24624, Loss: 1.8177204132080078\n",
            "Epoch 3, Step 24625, Loss: 1.8531111478805542\n",
            "Epoch 3, Step 24626, Loss: 1.1816997528076172\n",
            "Epoch 3, Step 24627, Loss: 1.5094577074050903\n",
            "Epoch 3, Step 24628, Loss: 2.5102133750915527\n",
            "Epoch 3, Step 24629, Loss: 1.250610113143921\n",
            "Epoch 3, Step 24630, Loss: 1.0976461172103882\n",
            "Epoch 3, Step 24631, Loss: 1.306707739830017\n",
            "Epoch 3, Step 24632, Loss: 1.4544239044189453\n",
            "Epoch 3, Step 24633, Loss: 1.4321320056915283\n",
            "Epoch 3, Step 24634, Loss: 1.042787790298462\n",
            "Epoch 3, Step 24635, Loss: 0.8080672025680542\n",
            "Epoch 3, Step 24636, Loss: 0.4197021424770355\n",
            "Epoch 3, Step 24637, Loss: 1.1037322282791138\n",
            "Epoch 3, Step 24638, Loss: 1.481353759765625\n",
            "Epoch 3, Step 24639, Loss: 1.8170068264007568\n",
            "Epoch 3, Step 24640, Loss: 1.401995062828064\n",
            "Epoch 3, Step 24641, Loss: 1.533007264137268\n",
            "Epoch 3, Step 24642, Loss: 0.7669371962547302\n",
            "Epoch 3, Step 24643, Loss: 0.7591179013252258\n",
            "Epoch 3, Step 24644, Loss: 1.54863703250885\n",
            "Epoch 3, Step 24645, Loss: 1.6696274280548096\n",
            "Epoch 3, Step 24646, Loss: 1.1387872695922852\n",
            "Epoch 3, Step 24647, Loss: 1.2564719915390015\n",
            "Epoch 3, Step 24648, Loss: 1.1566112041473389\n",
            "Epoch 3, Step 24649, Loss: 0.833041787147522\n",
            "Epoch 3, Step 24650, Loss: 1.9010106325149536\n",
            "Epoch 3, Step 24651, Loss: 1.221561312675476\n",
            "Epoch 3, Step 24652, Loss: 0.9120548367500305\n",
            "Epoch 3, Step 24653, Loss: 1.1360242366790771\n",
            "Epoch 3, Step 24654, Loss: 1.1829849481582642\n",
            "Epoch 3, Step 24655, Loss: 1.5132099390029907\n",
            "Epoch 3, Step 24656, Loss: 1.341417908668518\n",
            "Epoch 3, Step 24657, Loss: 1.324448823928833\n",
            "Epoch 3, Step 24658, Loss: 1.9140220880508423\n",
            "Epoch 3, Step 24659, Loss: 2.410510540008545\n",
            "Epoch 3, Step 24660, Loss: 2.4592275619506836\n",
            "Epoch 3, Step 24661, Loss: 1.0789752006530762\n",
            "Epoch 3, Step 24662, Loss: 1.5489606857299805\n",
            "Epoch 3, Step 24663, Loss: 2.100447416305542\n",
            "Epoch 3, Step 24664, Loss: 1.2891170978546143\n",
            "Epoch 3, Step 24665, Loss: 1.5201090574264526\n",
            "Epoch 3, Step 24666, Loss: 0.9350247383117676\n",
            "Epoch 3, Step 24667, Loss: 0.6562987565994263\n",
            "Epoch 3, Step 24668, Loss: 1.726625680923462\n",
            "Epoch 3, Step 24669, Loss: 1.1542787551879883\n",
            "Epoch 3, Step 24670, Loss: 1.8929078578948975\n",
            "Epoch 3, Step 24671, Loss: 1.8150984048843384\n",
            "Epoch 3, Step 24672, Loss: 1.4510282278060913\n",
            "Epoch 3, Step 24673, Loss: 1.3798726797103882\n",
            "Epoch 3, Step 24674, Loss: 2.23767352104187\n",
            "Epoch 3, Step 24675, Loss: 0.4571126699447632\n",
            "Epoch 3, Step 24676, Loss: 0.503749668598175\n",
            "Epoch 3, Step 24677, Loss: 1.4065537452697754\n",
            "Epoch 3, Step 24678, Loss: 2.373884677886963\n",
            "Epoch 3, Step 24679, Loss: 1.8358958959579468\n",
            "Epoch 3, Step 24680, Loss: 2.3116042613983154\n",
            "Epoch 3, Step 24681, Loss: 1.4216655492782593\n",
            "Epoch 3, Step 24682, Loss: 1.9957586526870728\n",
            "Epoch 3, Step 24683, Loss: 1.5318896770477295\n",
            "Epoch 3, Step 24684, Loss: 1.2264344692230225\n",
            "Epoch 3, Step 24685, Loss: 2.265124559402466\n",
            "Epoch 3, Step 24686, Loss: 1.0453269481658936\n",
            "Epoch 3, Step 24687, Loss: 1.7675784826278687\n",
            "Epoch 3, Step 24688, Loss: 1.6128292083740234\n",
            "Epoch 3, Step 24689, Loss: 0.8978480100631714\n",
            "Epoch 3, Step 24690, Loss: 1.8823472261428833\n",
            "Epoch 3, Step 24691, Loss: 1.5183379650115967\n",
            "Epoch 3, Step 24692, Loss: 0.5344130992889404\n",
            "Epoch 3, Step 24693, Loss: 1.315307378768921\n",
            "Epoch 3, Step 24694, Loss: 1.9722918272018433\n",
            "Epoch 3, Step 24695, Loss: 2.5519490242004395\n",
            "Epoch 3, Step 24696, Loss: 1.710952639579773\n",
            "Epoch 3, Step 24697, Loss: 1.558631181716919\n",
            "Epoch 3, Step 24698, Loss: 2.0978288650512695\n",
            "Epoch 3, Step 24699, Loss: 1.764062523841858\n",
            "Epoch 3, Step 24700, Loss: 1.2966816425323486\n",
            "Epoch 3, Step 24701, Loss: 1.5580979585647583\n",
            "Epoch 3, Step 24702, Loss: 1.2291288375854492\n",
            "Epoch 3, Step 24703, Loss: 1.7571380138397217\n",
            "Epoch 3, Step 24704, Loss: 2.294651746749878\n",
            "Epoch 3, Step 24705, Loss: 2.1472115516662598\n",
            "Epoch 3, Step 24706, Loss: 2.1099853515625\n",
            "Epoch 3, Step 24707, Loss: 2.1005959510803223\n",
            "Epoch 3, Step 24708, Loss: 1.3860565423965454\n",
            "Epoch 3, Step 24709, Loss: 1.572502851486206\n",
            "Epoch 3, Step 24710, Loss: 0.874650776386261\n",
            "Epoch 3, Step 24711, Loss: 1.0781636238098145\n",
            "Epoch 3, Step 24712, Loss: 1.9238243103027344\n",
            "Epoch 3, Step 24713, Loss: 2.053997755050659\n",
            "Epoch 3, Step 24714, Loss: 1.7757596969604492\n",
            "Epoch 3, Step 24715, Loss: 0.5626739263534546\n",
            "Epoch 3, Step 24716, Loss: 1.4479206800460815\n",
            "Epoch 3, Step 24717, Loss: 2.469238758087158\n",
            "Epoch 3, Step 24718, Loss: 1.6417597532272339\n",
            "Epoch 3, Step 24719, Loss: 1.7259559631347656\n",
            "Epoch 3, Step 24720, Loss: 0.5698508024215698\n",
            "Epoch 3, Step 24721, Loss: 0.7773240804672241\n",
            "Epoch 3, Step 24722, Loss: 0.5409848690032959\n",
            "Epoch 3, Step 24723, Loss: 2.18141508102417\n",
            "Epoch 3, Step 24724, Loss: 2.064683198928833\n",
            "Epoch 3, Step 24725, Loss: 1.4628714323043823\n",
            "Epoch 3, Step 24726, Loss: 1.3546500205993652\n",
            "Epoch 3, Step 24727, Loss: 1.4546575546264648\n",
            "Epoch 3, Step 24728, Loss: 1.5265600681304932\n",
            "Epoch 3, Step 24729, Loss: 1.1307430267333984\n",
            "Epoch 3, Step 24730, Loss: 0.5501761436462402\n",
            "Epoch 3, Step 24731, Loss: 2.2082645893096924\n",
            "Epoch 3, Step 24732, Loss: 2.0889108180999756\n",
            "Epoch 3, Step 24733, Loss: 0.6346653699874878\n",
            "Epoch 3, Step 24734, Loss: 1.9428932666778564\n",
            "Epoch 3, Step 24735, Loss: 2.288839817047119\n",
            "Epoch 3, Step 24736, Loss: 2.542853832244873\n",
            "Epoch 3, Step 24737, Loss: 1.290649175643921\n",
            "Epoch 3, Step 24738, Loss: 1.6706684827804565\n",
            "Epoch 3, Step 24739, Loss: 1.6213676929473877\n",
            "Epoch 3, Step 24740, Loss: 1.4361132383346558\n",
            "Epoch 3, Step 24741, Loss: 2.0620663166046143\n",
            "Epoch 3, Step 24742, Loss: 1.66500723361969\n",
            "Epoch 3, Step 24743, Loss: 1.037795901298523\n",
            "Epoch 3, Step 24744, Loss: 1.6993935108184814\n",
            "Epoch 3, Step 24745, Loss: 1.0713595151901245\n",
            "Epoch 3, Step 24746, Loss: 1.5242263078689575\n",
            "Epoch 3, Step 24747, Loss: 1.5778775215148926\n",
            "Epoch 3, Step 24748, Loss: 2.132850170135498\n",
            "Epoch 3, Step 24749, Loss: 1.1808645725250244\n",
            "Epoch 3, Step 24750, Loss: 1.473127007484436\n",
            "Epoch 3, Step 24751, Loss: 1.769942283630371\n",
            "Epoch 3, Step 24752, Loss: 0.6200803518295288\n",
            "Epoch 3, Step 24753, Loss: 0.45069795846939087\n",
            "Epoch 3, Step 24754, Loss: 1.7963459491729736\n",
            "Epoch 3, Step 24755, Loss: 1.3810638189315796\n",
            "Epoch 3, Step 24756, Loss: 1.7665934562683105\n",
            "Epoch 3, Step 24757, Loss: 1.8673076629638672\n",
            "Epoch 3, Step 24758, Loss: 2.0840766429901123\n",
            "Epoch 3, Step 24759, Loss: 0.9596888422966003\n",
            "Epoch 3, Step 24760, Loss: 1.4691376686096191\n",
            "Epoch 3, Step 24761, Loss: 2.1912806034088135\n",
            "Epoch 3, Step 24762, Loss: 2.120441198348999\n",
            "Epoch 3, Step 24763, Loss: 1.5602127313613892\n",
            "Epoch 3, Step 24764, Loss: 2.1512293815612793\n",
            "Epoch 3, Step 24765, Loss: 1.541344404220581\n",
            "Epoch 3, Step 24766, Loss: 1.9827126264572144\n",
            "Epoch 3, Step 24767, Loss: 2.627095937728882\n",
            "Epoch 3, Step 24768, Loss: 1.3375648260116577\n",
            "Epoch 3, Step 24769, Loss: 1.6989495754241943\n",
            "Epoch 3, Step 24770, Loss: 1.7655192613601685\n",
            "Epoch 3, Step 24771, Loss: 1.6194003820419312\n",
            "Epoch 3, Step 24772, Loss: 0.6808584928512573\n",
            "Epoch 3, Step 24773, Loss: 1.7187676429748535\n",
            "Epoch 3, Step 24774, Loss: 1.7151663303375244\n",
            "Epoch 3, Step 24775, Loss: 1.0326769351959229\n",
            "Epoch 3, Step 24776, Loss: 1.9867138862609863\n",
            "Epoch 3, Step 24777, Loss: 1.951968789100647\n",
            "Epoch 3, Step 24778, Loss: 1.8363289833068848\n",
            "Epoch 3, Step 24779, Loss: 0.7503289580345154\n",
            "Epoch 3, Step 24780, Loss: 1.709067463874817\n",
            "Epoch 3, Step 24781, Loss: 0.9170621037483215\n",
            "Epoch 3, Step 24782, Loss: 0.8072444796562195\n",
            "Epoch 3, Step 24783, Loss: 1.6911702156066895\n",
            "Epoch 3, Step 24784, Loss: 1.9638227224349976\n",
            "Epoch 3, Step 24785, Loss: 0.8415890336036682\n",
            "Epoch 3, Step 24786, Loss: 3.0046794414520264\n",
            "Epoch 3, Step 24787, Loss: 1.1117277145385742\n",
            "Epoch 3, Step 24788, Loss: 2.1041903495788574\n",
            "Epoch 3, Step 24789, Loss: 1.2860630750656128\n",
            "Epoch 3, Step 24790, Loss: 0.7978205680847168\n",
            "Epoch 3, Step 24791, Loss: 1.8565667867660522\n",
            "Epoch 3, Step 24792, Loss: 1.4945541620254517\n",
            "Epoch 3, Step 24793, Loss: 0.6260014176368713\n",
            "Epoch 3, Step 24794, Loss: 0.5111477971076965\n",
            "Epoch 3, Step 24795, Loss: 0.9584245681762695\n",
            "Epoch 3, Step 24796, Loss: 1.6246343851089478\n",
            "Epoch 3, Step 24797, Loss: 1.2343696355819702\n",
            "Epoch 3, Step 24798, Loss: 0.7506499886512756\n",
            "Epoch 3, Step 24799, Loss: 2.5308687686920166\n",
            "Epoch 3, Step 24800, Loss: 2.2038910388946533\n",
            "Epoch 3, Step 24801, Loss: 1.0580202341079712\n",
            "Epoch 3, Step 24802, Loss: 3.0136799812316895\n",
            "Epoch 3, Step 24803, Loss: 1.6713653802871704\n",
            "Epoch 3, Step 24804, Loss: 1.6795088052749634\n",
            "Epoch 3, Step 24805, Loss: 1.464120626449585\n",
            "Epoch 3, Step 24806, Loss: 1.666294813156128\n",
            "Epoch 3, Step 24807, Loss: 2.636716365814209\n",
            "Epoch 3, Step 24808, Loss: 2.195812463760376\n",
            "Epoch 3, Step 24809, Loss: 1.9054509401321411\n",
            "Epoch 3, Step 24810, Loss: 3.038135051727295\n",
            "Epoch 3, Step 24811, Loss: 0.6471116542816162\n",
            "Epoch 3, Step 24812, Loss: 0.6514100432395935\n",
            "Epoch 3, Step 24813, Loss: 2.4987270832061768\n",
            "Epoch 3, Step 24814, Loss: 1.659308910369873\n",
            "Epoch 3, Step 24815, Loss: 1.586571216583252\n",
            "Epoch 3, Step 24816, Loss: 1.4882242679595947\n",
            "Epoch 3, Step 24817, Loss: 1.1198307275772095\n",
            "Epoch 3, Step 24818, Loss: 1.5137883424758911\n",
            "Epoch 3, Step 24819, Loss: 1.2886834144592285\n",
            "Epoch 3, Step 24820, Loss: 2.0972206592559814\n",
            "Epoch 3, Step 24821, Loss: 1.2380679845809937\n",
            "Epoch 3, Step 24822, Loss: 2.4709091186523438\n",
            "Epoch 3, Step 24823, Loss: 1.618880033493042\n",
            "Epoch 3, Step 24824, Loss: 1.3926571607589722\n",
            "Epoch 3, Step 24825, Loss: 1.9998252391815186\n",
            "Epoch 3, Step 24826, Loss: 1.5959023237228394\n",
            "Epoch 3, Step 24827, Loss: 1.2468838691711426\n",
            "Epoch 3, Step 24828, Loss: 1.5797377824783325\n",
            "Epoch 3, Step 24829, Loss: 0.448499470949173\n",
            "Epoch 3, Step 24830, Loss: 1.2066082954406738\n",
            "Epoch 3, Step 24831, Loss: 1.326562523841858\n",
            "Epoch 3, Step 24832, Loss: 1.4227677583694458\n",
            "Epoch 3, Step 24833, Loss: 2.3307907581329346\n",
            "Epoch 3, Step 24834, Loss: 2.0582540035247803\n",
            "Epoch 3, Step 24835, Loss: 1.1399976015090942\n",
            "Epoch 3, Step 24836, Loss: 2.62804913520813\n",
            "Epoch 3, Step 24837, Loss: 0.8231321573257446\n",
            "Epoch 3, Step 24838, Loss: 1.4276221990585327\n",
            "Epoch 3, Step 24839, Loss: 1.0978275537490845\n",
            "Epoch 3, Step 24840, Loss: 1.3059581518173218\n",
            "Epoch 3, Step 24841, Loss: 0.7631385922431946\n",
            "Epoch 3, Step 24842, Loss: 1.0607597827911377\n",
            "Epoch 3, Step 24843, Loss: 1.3148131370544434\n",
            "Epoch 3, Step 24844, Loss: 1.1410058736801147\n",
            "Epoch 3, Step 24845, Loss: 2.236981153488159\n",
            "Epoch 3, Step 24846, Loss: 2.4463253021240234\n",
            "Epoch 3, Step 24847, Loss: 0.9472127556800842\n",
            "Epoch 3, Step 24848, Loss: 1.418918251991272\n",
            "Epoch 3, Step 24849, Loss: 1.9709113836288452\n",
            "Epoch 3, Step 24850, Loss: 1.1269968748092651\n",
            "Epoch 3, Step 24851, Loss: 1.1917362213134766\n",
            "Epoch 3, Step 24852, Loss: 1.4793437719345093\n",
            "Epoch 3, Step 24853, Loss: 1.7459378242492676\n",
            "Epoch 3, Step 24854, Loss: 1.808111310005188\n",
            "Epoch 3, Step 24855, Loss: 1.0552678108215332\n",
            "Epoch 3, Step 24856, Loss: 1.3093140125274658\n",
            "Epoch 3, Step 24857, Loss: 1.327452301979065\n",
            "Epoch 3, Step 24858, Loss: 0.6873173117637634\n",
            "Epoch 3, Step 24859, Loss: 1.2625441551208496\n",
            "Epoch 3, Step 24860, Loss: 1.9709510803222656\n",
            "Epoch 3, Step 24861, Loss: 1.2435506582260132\n",
            "Epoch 3, Step 24862, Loss: 0.7998756170272827\n",
            "Epoch 3, Step 24863, Loss: 1.8546777963638306\n",
            "Epoch 3, Step 24864, Loss: 0.9914826154708862\n",
            "Epoch 3, Step 24865, Loss: 0.815818190574646\n",
            "Epoch 3, Step 24866, Loss: 1.4505181312561035\n",
            "Epoch 3, Step 24867, Loss: 0.9355121850967407\n",
            "Epoch 3, Step 24868, Loss: 1.365777611732483\n",
            "Epoch 3, Step 24869, Loss: 1.6827925443649292\n",
            "Epoch 3, Step 24870, Loss: 2.715684413909912\n",
            "Epoch 3, Step 24871, Loss: 3.8735008239746094\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('./sinhala_fine_tuned_distilgpt2_2/tokenizer_config.json',\n",
              " './sinhala_fine_tuned_distilgpt2_2/special_tokens_map.json',\n",
              " './sinhala_fine_tuned_distilgpt2_2/sentencepiece.bpe.model',\n",
              " './sinhala_fine_tuned_distilgpt2_2/added_tokens.json')"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import shutil\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path in your Google Drive where you want to save the model\n",
        "drive_path = '/content/drive/MyDrive/sinhala_fine_tuned_distilgpt2_2'\n",
        "\n",
        "# Copy the directory to Google Drive\n",
        "shutil.copytree('./sinhala_fine_tuned_distilgpt2_2', drive_path)\n",
        "\n",
        "print(f\"Model saved to {drive_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "720RfNJSNiH8",
        "outputId": "2841f8d7-e862-4f73-8b6a-9c3b89deb530"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Model saved to /content/drive/MyDrive/sinhala_fine_tuned_distilgpt2_2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, XLMRobertaTokenizer\n",
        "import torch\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "model_path = \"./sinhala_fine_tuned_distilgpt2_2\"\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(model_path)\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Define the prompt\n",
        "prompt = \"අක්ෂර වින්‍යාසය සහ ව්‍යාකරණ වැරදි සඳහා මෙම වාක්‍යය ඇගයීමට ලක් කරන්න: ඔහු තම ආහාර වේල සකසා ආපනශාලාවෙන් පිටව ගියේය\"\n",
        "\n",
        "# Tokenize the input\n",
        "inputs = tokenizer(prompt, return_tensors='pt')\n",
        "\n",
        "# Ensure that model and inputs are on the same device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "# Generate text\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = model.generate(\n",
        "        inputs['input_ids'],\n",
        "        max_length=100,\n",
        "        num_return_sequences=1,\n",
        "        pad_token_id=tokenizer.pad_token_id,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        bos_token_id=tokenizer.bos_token_id\n",
        "    )\n",
        "\n",
        "# Decode the generated text\n",
        "generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(generated_text)\n",
        "cx"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "617Y_X5dNt8_",
        "outputId": "022e4d1a-9349-4167-a1af-8babd9aff586"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "අක්ෂර වින් යාසය සහ ව් යාකරණ වැරදි සඳහා මෙම වාක් යය ඇගයීමට ලක් කරන්න: ඔහු තම ආහාර වේල සකසා ආපනශාලාවෙන් පිටව ගියේය\n"
          ]
        }
      ]
    }
  ]
}